# Exisentials, Refinements, and Bidirectional Typing

Dunfield and Krishnaswami have a [Survey of Bidirectional Typing](https://www.cl.cam.ac.uk/~nk480/bidir-survey.pdf)
that's got me thinking about a bidirectional system that requires more
annotations than people usually like having. In particular, annotating
the variable introduced by lambda. That's it. This way, lambda would
synthesize a type without needing to introduce metavariables.

I really want to be able to refine integers. That is, I'd like to be
able to have types like this:

    (n : Nat) -> (m : Nat | m < n) -> Bool

This is a function where the second argument is less than the first
argument. Doing this is tricky. It requires dependent types, or at
least some simulation of them. Equivalently, we might have:

    forall (n : Nat) (m : Nat). {m < n}. SNat n -> SNat m -> Bool

Or maybe: 

    forall (n : Nat) (m : Nat | m < n). SNat n -> SNat m -> Bool

I'm not sure what the best way to approach this is. Also, consider
an array of refined integers:

    forall (k : Nat). Array (exists (n : Nat). {n < k}. SNat n) -> Bool

The above function consume an array of integers where all the integers
are less than some number `k`. But impredicative types are tough to deal
with. We desperately want to avoid an existential inside a type constructor.

    forall (k : Nat). Array (Nat | <k) -> Bool

But the existential is still there. It's just hidden. Expanding this with
Presburger arithmetic, we have:

    forall (k : Nat). Array (exists n p. SNat n | n + p = k) -> Bool

But maybe this is ok. Consider a syntax like this:

    forall (j : Nat) (k : Nat | j < _). Array (Nat | _ < k && j < _) -> Bool

Anything underscore refers to the natural number on the LHS of the bar. What
are the properties of this:

* When applied at the value-level, it is very much like an existentially bound
  variable, but it is an outermost existential binding.
* At the type level, it's different. Maybe we can avoid having this at the
  type level at all, although this makes certain relationships between numbers
  difficult to capture. In particular, you can end up with these weird
  `Nat | _ == x && _ < y` constructions where you have to repeat the information
  about being less than `y` over and over.

I think I'm getting close to identifying the tension. The advantage of having
something like `SNat`, where the annotation is a single variable, is that
information is not duplicated. But, the disadvantage is that you end up having
to put existentials in a place where you do not want them. I think that the
solution is to have two different existential quantifiers: one for natural
numbers and one for ground types. The one for ground types cannot be used
inside a type constructor, but the one for natural numbers can. Why can it?
Because it doesn't cause problems with impredicativity. With universal types
(just ground types), when trying to construct a value of type
`Maybe (forall a. a -> a)`, we run into the issue that `just` has type
`just : forall b. b -> Maybe b`, and so we would have to perform an impredicative
instantiation of `b` to `forall a. a -> a` to construct a value. These are bad
for the DK bidirectional system. Existentials are similarly bad. Ideally, we
would like to be able to look at the argument to `just` and decide, based on
that, what the type of the resulting `Maybe` is. With existentials in play,
the expression `just 5` could synthesize `Maybe Nat` or `Maybe (exists t. t)`.
Actually, this doesn't seem like that much of an issue because
`Maybe Nat :< Maybe (exists t. t)`. I think that it might be a worse
issue for universals than for existentials. In something like:

    length nil

We want this to typecheck. The `nil` does synthesize a type, but it's free in
the element type. This will bubble up in the context and get solved eventually
just like in the DK system. I think that's fine. Anyway, for arrays, we have
subtyping relationships like:

    Array Nat :< Array (exists t. t) 
    Array (exists n. {n < 55}. SNat n) :< Array (exists n. {n < 60}. SNat n) 

This makes since. Should users be allowed to pass an argument with an
existential in the outermost level of its type to a function? Consider:

    -- In Haskell, this is a function that modifies a number, preserving
    -- no properties in any obvious way:
    scramble : Nat -> Nat
    -- But in a language that tracks properties of integers:
    scramble : forall n. Nat n -> exists m. Nat m

Should we allow this:

    scramble (scramble 5)

Or should we instead require:

    -- explicitly opens up the existential
    let x = scramble 5
     in scramble x

The first construction is what programmers want to be able to use, but the
second one is easier on the machine. It gets trickier when the outer function
is supposed to propagate the length (e.g. `replicate`).

    replicate : forall n a. Nat n -> a -> Vec n a
    replicate
      (scramble 5 : exists m. Nat m) -- Requires: Nat n :< exists m. Nat m
      (true : Bool) -- Requires: a :< Bool

These constraints during typechecking are difficult to solve. We want the
results type to be `exists m. Vec m a`, but a type system will have a hard
time figuring this out. If we instead bound the result of `scramble` first,
this would work out more easily.

I'm beginning to warm up more to the idea of just requiring a lot of let
bindings. Let's think about refined type of some common functions:

    plus  : forall m n. Nat m -> Nat n -> Nat (m + n)
    minus : forall m n. Nat (m + n) -> Nat n -> Nat m
    zero  : Nat 0
    multiply : forall m n. Nat m -> Nat n -> exists k. Nat k -- Presburger cannot multiply
    divide   : forall m n. {n > 0}. Nat m -> Nat n -> exists k. Nat k

    -- Comparison brings existentials into scope. Note that minus can be
    -- used in any of the arms.
    case-compare m n of
      LT { exists k. k + m = n } -> ...
      EQ { m = n } -> ...
      GT { exists k. k + n = m } -> ...

    -- What happens when we subtract when the two numbers are equal?
    CTX {m0, n0, m0 = n0} (minus m n)
      INPUT CTX {m0, n0, m0 = n0, m1, n1}
        n : Nat n0 ==> Nat n0 :< Nat n1        ==> n1 = n0
        m : Nat m0 ==> Nat m0 :< Nat (m1 + n1) ==> m0 = m1 + n1 ==> n0 = m1 + n0 ==> m1 = 0
      OUTPUT CTX {m0, n0, m0 = n0, m1 = 0, n1 = n0}

For subtraction to typecheck, we had to consider the arguments in a specific
order. A typechecker couldn't guess the order in which the arguments would
need to be considered, so I think that the real algorithm would need to be
able to move between arguments when it got stuck. But, we are going for
synthesis here, not checking, so the inputs should always have sufficient
information for determining the output type, and if they don't, it should
fail. For example, it we just tried to subtract two numbers without knowing
anything else about them:

    CTX {m0, n0} (minus m n)
      INPUT CTX {m0, n0, m1, n1}
        n : Nat n0 ==> Nat n0 :< Nat n1        ==> n1 = n0
        m : Nat m0 ==> Nat m0 :< Nat (m1 + n1) ==> m0 = m1 + n1 ==> m0 = m1 + n0
      OUTPUT CTX {m0, n0, n1 = n0, m1 = ?}

Since we could not solve the metavariable `m1`, this will fail. What about
refinements that subsume other refinements? What if we have a function that
expects a number less than 9 and we give it a number less than 7 instead?

    eatSmall : forall n. {n < 9}. Nat n -> Bool
    CTX {n0} {n0 < 7} (eatSmall n)
      INPUT CTX {n0, n1} {n0 < 7, n1 < 9}
        n : Nat n0 ==> Nat n0 :< Nat n1 ==> (assert that x < 7 ==> x < 9) ==> n1 = n0 
      OUTPUT CTX {n0, n1 = n0}

So, this is different because it actually has to handle the refinements. In the
general case, we will have two variables and any number of constraints targeting
either of them, and then we must the question "does everything we know
about this one imply everything we know about this other one". If so, we can
perform the assignment. If not, we can try to make progress on other unsolved
variables and then come back to it.

So, actually, I think that in the earlier example where subtraction worked out,
I don't think that Presburger can really solve `m1` like that. I think that
what we would have to do is:

    CTX {m0, n0, m0 = n0} (minus m n)
      INPUT CTX {m0, n0, m1, n1} {m0 = n0}
        n : Nat n0 ==> Nat n0 :< Nat n1        ==> n1 = n0
        m : Nat m0 ==> Nat m0 :< Nat (m1 + n1) ==> m0 = m1 + n1
      OUTPUT CTX {m0, n0, m1, n1} {n1 = n0, m0 = m1 + n1}
      CONFIRM: forall m0 n0. exists m1 n1. (n1 = n0, m0 = m1 + n1)

So, we require that the existentials have at least one solution for any given
input, but we just leave them alone in the output context and types. This is
somewhat unsatisfying because it means that function application brings more
types into scope. It would be nice if there were a way to solve Presburger
for a single variable. In particular, in the above example, how can we figure
out that:

    forall m0 n0. exists m1 n1. (n1 = n0, m0 = m1 + n1)

Has unique solutions:

    n1 = n0
    m1 = 0
