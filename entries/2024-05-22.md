# Reference Counting Without Tracing

I'm thinking about a simple kind of reference counting that does not
involve tracing (and consequently cannot be deferred). This kind of
RC should be able to work in either a VM or with native code.

The heap is partitioned into per-type arenas. We store three pieces
of information for each object:

1. The payload (i.e. the values of the fields)
2. A reference count (32-bit integer)

These three pieces of information are not stored contiguously. The
payloads are contiguous with one another, but they are not near the
metadata. Similarly, the reference counts are contiguous with one
another, and the liveliness bits are the same way.

When performing an allocation, we scan the reference counts from the
beginning to the end. When we find one that is zero, we use that
object. There is another optimization that can be layered on top

1. The payload (i.e. the values of the fields)
2. A reference count (32-bit integer)
3. A liveliness bit for a block of 16 reference counts (1 bit).

This additional layer speeds up allocation since a 64-byte cache line
of liveliness bits tells of which one of the 512 blocks of 16 reference
counts has a zero count in it. That gives us information about 8192
possible slots for an object by touching only two cache lines.

But this has a cost. We have to maintain the liveliness bits, which
involves bookkeeping when the count changes to zero or from zero.

A smarter strategy is just have a stack of every free slot. Every time
an object is freed, push the slot's 32-bit position onto the top of
the stack and increment a variable that tracks the head. To allocate
an object, we pop the slot from the top of the stack (decrementing
the variable that tracks the head). If the stack is empty when we
allocate, we can either kill the program or request additional memory
from the kernel to replenish our supply.

# Design Decisions

Here are some problems to think about:

* Is it easy to support a variant where we only use 16 bits for everything?
  This impacts: counters, size of pointer fields, size of elements in stack
  of free slots. There are some programs where we do not expect large numbers
  of objects to be created.
* How does array slicing work? We could have a reference count for each
  element, but that's a ton of overhead. We could do the silly thing that
  everyone does and just keep track of the base of the array everywhere.
  Then slices wouldn't actually reduce memory pressure. It's really hard
  to shoehorn this into an RC scheme. If an array is unique and we take
  a slice, we can promptly return the unused part at the end (or beginning).
  This might be a clue about what this system has to look like. With arrays,
  we cannot have a stack of available slots. We need a big list of contiguous
  areas. The simplicity of vanilla RC schemes is that the metadata stays
  up to date. I don't know if there is a way to do this.
* Is there anything we can do to reduce the cost of RC updates when assigning
  a local variable to an object? One option is to store a "stack frame id"
  beside an object's counter. When a local variable is bound to an object,
  we could update the "stack frame id" instead of adjusting the counter.
  If a stack frame from higher up the chain was already there, we would leave
  the existing value alone. When a function returns, check all the local
  variables to see if any need to be freed. I don't like this. It's too
  complicated, and we are doing nearly the same thing that was already
  happening.

Maybe the scenario I'm thinking about is this:

    x = myObject.child
    ... possibilites: x outlives myObject, myObject outlives x

Let's assume that the counter for the object `child` started out at 1.
That is, the only reference to it was through `myObject`. If `myObject`
outlives `x`, then we are in an easy situation. We can just not update
the RC to reflect the existence of `x`. It would have been incremented
and decremented, so everything is fine. Even if we create other references
to the child by passing `x` around, we still do not need the increment
or decrement for `x`. But if `x` outlives `myObject`, this does not work
any more. Why? If we don't increment the counter, then when `myObject`
goes out of scope, its children's RCs will be decremented. This will
push the RC of `child` to zero, and it will be deallocated.

So we can only elide RC updates when we are certain that the ellision
does not cause the RC to hit zero when it was still live.

If we require that the caller is responsible for updating the RC
on function arguments after the function returns, we end up with a nice
optimization available to us. No local variable assigned to an
argument object or to anything reachable from that object needs
to worry about RC updates.

Compatibility with interpreter. How does light static analysis fit
in with an interpreter? Who performs the analysis? I'm not sure.
I need to think about this more.

# Reference Counting Strategy Array Slices

For array slices, here is an idea. This causes slicing operations to leak
memory, but it only requires two 32-bit words to represent arrays as they
are passed around in a program. The idea is put an indirection between
the array and the reference count. When an array is allocated, the allocation 
function chooses a slot in some runtime-managed array to store the RC.
Then, the index of this slot is broadcasted into an rc-slot-lookup array.
When a slice needs to increment or decrement its RC, we feed the offset
beginning of the payload into the rc-slot-lookup array, and that tells
us where the actuall RC is stored.

There is a problem with zero-length slices since the beginning of the slice
is likely the beginning of whatever happens to be layed out after it on the
heap. One solution is to not count zero-length arrays as live. Another
solution is to insert padding between arrays. I don't love either of these.

One neat trick about slicing arrays is that if we slice into an array, we
can often elide the RC update because the arrays that was sliced into
does not typically get used again.

# Interpretation and Compilation

It would be cool if an bytecode interpreter for a language used the same
memory layout for objects that compiled executables used. This way, it should
be possible to compile certain leaf functions and speed up the interpreted
program. You would also need to have them agree on the calling convention.
So probably passing everything on the stack.
