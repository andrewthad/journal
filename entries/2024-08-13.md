# Computing Field Positions Cheaply

On 2024-05-06, I explored how Sixten handles (or how I think it handles)
fields in polymorphic objects. I have been thinking about other ways to
accomplish this. Here are some questions:

* Alignment. Should we have any alignment guarantees? Should we allow
  scalar values to cross cache lines?

I had developed a blind spot in my view of the solution space. Regions
do not require tracing, so we do not need to treat pointers and integers
differently from one another. So the runtime representation of a type can
just be its size, and it's reasonable to assume that sizes are less than
256 bytes.

The problem is dealing with types like this:

    data Foo a b c d e = MkFoo a b b b c d e

How can we cheaply find the position of the field of type `e`? I believe
that the system that Sixten uses would just add the sizes of types `[a-d]`,
multiplying the size of `b` by 3 first. We might try storing all of the
type variables (i.e. the sizes of them) in a vector register. The data
constructor `MkFoo` would be associated with vector registers that told
us how to scale the sizes to get the positions we cared about:

1. `[0,0,0,0,0]`
2. `[1,0,0,0,0]`
3. `[1,1,0,0,0]`
4. `[1,2,0,0,0]`
5. `[1,3,0,0,0]`
6. `[1,3,1,0,0]`
7. `[1,3,1,1,0]`

To find the position of a field, we vertically multiply the vector of sizes
with the vector of offsets. Then we horizontally add the scaled sizes.

This works, but it is expensive. The performance of horizontal addition
on x86 is terrible. We could try packing this into a 64-bit word instead
(limiting us to 8 type variables per function), but then we do not have
any vector operations. People have been figuring out ways to simulate vector
arithmetic on 64-bit registers since forever, but I don't like this path.
Instead, we could change how sizes work. What if we only allowed sizes
that were powers of 2? We would represent sizes as an exponent instead
of as the size itself. Even with just 4 bits, we could represent sizes
as large as 2^15. This does not help us though. We still need the vector
of scales, and we cannot multiply these by the exponents without converting
the exponents to numbers, and even if we were able to convert the exponents
to numbers, we still have no cheap way to perform a vector multiply.

What if the scales also had to be base-2 numbers? Then they would be
represented as exponents, and we could simulate vertical multiplication
by simply adding the 64-bit words and assuming none of the element additions
overflowed. There's no way to get the total size from this. I thought `popcnt`
would work, but it does not.

For `popcnt` to work, we need each size to be represented by setting a number
of bits equal to the size. But we cannot apply scales to these elements.

Let's go back to the exponent representation. That seemed to have some
amount of promise. There are two obstacles:

1. Cannot extract the offset from the final value
2. Cannot represent certain scalars (e.g. 3, 5, 7)

Problem 1 is the more serious one. We need to be able to horizontally add
the elements in the vector. How do we do that? We could shift them one-by-one
into the low position, mask, and add. But that's iterative. Maybe instead,
we could reserve the high bits. Then we could multiply by a constant that
shifted everything to the top. I think that works. Let's pretend that we
are working with a 32-bit number, and we are using it as a vector of
three 8-bit numbers (the top 8 bits are reserved):

      0x00, Alph, Beta, Gamm
    x 0x01, 0x01, 0x01, 0x00
    ========================
      0x0000100 * (Gamm) +
      0x0010000 * (Beta + Gamm) +
      0x1000000 * (Beta + Gamm + Alph)

We do not need to go any higher because multiplication by `0x100000000` modulo
2^32 is zero. The part of the result that we are interested in is the third
line, the one that adds everything together. For this to work, we need for
`Beta + Gamm` to not overflow (and spill into the high bits). If they were
to overflow, the result would be doubly corrupted because the result of
`Beta + Gamm + Alph` would also be wrong. Preventing overflow does not seem
difficult since overflow corresponds to a size of `2^(2^8)`, which is a big
number.

Let's go back to the problem of scaling the sizes. We can scale with addition,
but this only lets us scale by powers of two. I've noticed that simulating
horizontal addition with multiplication works fine even if we are using
raw sizes instead of exponents. We run into overflow problems at a much
lower number (256), but I don't think it is a serious issue. Now I am wondering
if a vertical multiply is possible. What if we went with this arrangement
instead: scale, size, scale, size, etc. We can get this trivially with
bitwise disjunction as long as we pad the original values with zeros
in the right places. Although, even this might not be needed. What about
amending the previous approach:

      0x00    , AlphSize, BetaSize, GammSize
    x GammScal, BetaScal, AlphScal, 0x00
    ========================================
      ... elided ... +
      0x1000000 * (BetaSize * BetaScal + GammSize * GammScal + AlphSize * AlphScal)

Now, we can do the whole thing in a single multiply. This also opens the door
for having multiple VLAs in a data constructor. We need to store the array
sizes and the scalar field sizes into a vector that includes just the lengths
of the vectors. We need to vertically add these to a "base vector" that
includes the other fields.

I do not actually need to reserve the top 8 bits like I thought I did.
So we can actually packed another size variable in there.

We do have to be careful about overflow. Capping object size at 256 is not
a good idea. We might want to use 10 or 12 bits instead so that we could
support a maximum object size of 1024 or 4096 (giving us a maximum of 6 or 5
type variables per function).

One final note: If I take this approach, then it becomes tricky to use
C structs when lowering polymorphic functions. The C structs need to have
all of their polymorphic fields (and VLAs) at the end. So the Cons constructor
for lists would look like this:

    struct Cons {
      struct Cons* tail;
      // In this case, just the head element. In cases like this,
      // we might be able to get the code generator to name this "head".
      unsigned char data[];
    }

Notice that we do not need to store the length of the `data` member in
the struct. The size information flows in from the outside (from sizes
passed around at runtime).

And what about alignment? The strategy I have considered today does not
support natural alignment. To naturally align fields, we would need to sort
the sizes into descending order and arrange the fields so that the fields
whose size was the biggest came first. So then we have this weird "permute"
step. Our vector of scales for a given field is no longer a constant because
the fields do not appear in a fixed order. At this point, we should probably
just cache the positions in the object. Let's adapt the earlier example:

    data Foo a b c d e = MkFoo a b b b c d e S32 S64

We would have this:

    struct Foo {
      field0 int64_t; 
      field1 int32_t; 
      unsigned char padding0[4];
      unsigned char offsets[7];
      unsigned char padding1[1];
      unsigned char data[];
    }

The `data` field is unconditionally 8-byte aligned. We have to do this because
any of the type variables might be have size 8. We also 8-byte align the
offsets. We might instead want to round up and represent the offsets as a
`uint64_t`, which would make this padding (and the byte of padding after
the offsets) superfluous. To load the field of type `c` in a context where
`c` is known to be `s16`, we generate this code:

    int16_t myValue = *(int16_t*)(myFoo.data + myFoo.offsets[4])

If all of the type variables are known, we do not even need to consult the
offset table:

    int16_t myValue = *(int16_t*)(myFoo.data + KNOWN_OFFSET_OF_MEMBER)

If the type `c` is unknown in the context, it gets tricky. We do not know
how much space a value of type `c` takes, so we do not want to copy it
to the stack. That requires `alloca`, which is terrible. We could instead
do this:

    unsigned char* myValuePtr = myFoo.data + myFoo.offsets[4]

The way that I am thinking about region types is both helpful and harmful
here. It's helpful because the value of the unknown type is required to
be in the same region as its parent object. This means that the parent
object is guaranteed to stay live while the child is live. It's harmful
because the parent can still be mutated, which invalidates the reference.
So unfortunately, that approach cannot be used in some cases, but with
the right static analysis, we can probably discover that the object is not
mutated before it is used. When we cannot prove this, we can just use
a crummy solution:

    unsigned char myValuePtr[256]; // or maybe only 32 or 64 instead of 256
    memcpy(myValue, myFoo.data + myFoo.offsets[4], sizeRepC);

With additional static analysis, we could identify an upper bound on the
size of type `c` (look at the type variables that the function can be
called with), but this requires whole-program analysis.

Thinking about this more, I've convinced myself that we certainly do not
need 256 bytes for an unknown value. We need at most 64, and we might be
able to get down to 32 bytes. These numbers are helpful because they let
us store values of unknown types in wide vector registers. For example,
a function that finds an element at an index in a list (or crashes) might
look like this:

    __m256i listIndex(uint8 szA, struct list* myList, int64_t index);

We can cheaply move a value of the unknown type `a` around. What is a little
more expensive is loading from and storing into objects where we do not have
an unlimited number of bytes available. In these cases, on AVX512, we need to
prepare a mask register and then perform a masked load or store. Or as a more
portable solution, we could just do this:

    struct opaque { unsigned char contents[32]; };
    opaque listIndex(uint8 szA, struct list* myList, int64_t index);

And then use `memcpy` to interact with object members. That's probably what
I should try first.

Returning to an earlier point in the thought process, we only guarantee
8-byte alignment, so if we instantiate a type variable to `u128`
(e.g. to store UUIDs or IPv6 addresses), we might perform loads or stores
that a cross cache line when we touch this field.
