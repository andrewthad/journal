# Checking Type Equality in Dependent Systems

Section 3.2 of "Dependent Types: Easy as PIE" talks about a conservative
behavior of Coq's type checker:

> The information that we gain from pattern matching should be automatically
> used when checking the right hand side of pattern match clauses. This
> automation is provided in a restricted form in some systems. For example,
> Coq does not allow this knowledge gain to be used to refine the types of
> pre-existing variables in the environment.

It goes on to explain that the tactics system makes up for this limitation.
Propogating equalities without guidance from the user is a desirable feature.
Both Agda and GHC do this. What does the algorithm for this look like?
I've not been able to find a clear explanation, but my understanding is
that checking if A0 = B0 goes like this:

    toWhnf : Type -> Type
    isSyntacticEqual : (Type,Type) -> Bool
    isEqual : (Type, Type) -> Bool
    isEqual (a,b) = if isSyntacticEqual(a,b)
      then True
      else
        AppCon conA argsA = toWhnf(a)
        AppCon conB argsB = toWhnf(b)
        conA == conB && zipAnd isEqual argsA argsB 

This is an incomplete presentation of the algorithm, and it ignores type
equalities. A type equality would take one of these forms:

* `x = MyCon a b c`
* `y = z`

The third possibility, two constructor applications matching one another,
does not exist because you would instead check that the constructors match
and then creating equalities for each of their arguments.

This is a nice, simple system so far. What if we want to extend it to capture
the most basic properties of arithmetic. We want to handle addition as a
bag. That's basically it. This is tricky. Let's say that we are trying to prove
that these terms are equal:

    x + y + z
    =
    a + b + c

We cannot just zip these togther like we did with constructor applications.
That is, it is incorrect to reduce this to

    x = a, y = b, z = c

And we might even have equalities like `y = -z` in scope. So, how do we
simplify this? Integer difference logic (IDL) does not work because all
of the atomic expressions may only reference two variables. Let's think
about simple reductions that should be sound. If we know that `x = a`,
then it should be sound to reduce the problem to solving:

    y + z
    =
    b + c

What if we instead knew that `x = a + 1`. Still fine to apply the reduction.
As long as we reduce the number of variables, then we are going in the right
direction. But what if we only have information that takes us in the wrong
direction. For example, if `x = r + s` and `a = r + s`, then our heuristic
would tell us to not use either of these equalities (since either of them,
in isolation, increases the number of variables). Taken together, however,
they can be used to make the expression smaller. What if we instead build
out a set of equivalent expressions. For example:

    Known Equalities:
    x = a
    x = r + s
    x = 2*b + 5
    y = b + 6
    Equal Expressions:
    x + y + z
    a + y + z
    r + s + y + z
    2*b + y + z + 5
    x + b + z + 6
    r + s + b + z + 6
    a + b + z + 6
    3*b + z + 11

This is an exponential blowup in the number of equalities, but we may be able
to bring the amount of space required down with something like e-graphs. We have
to be careful to normalize at each step. Otherwise, some substitutions would
lead to an infinite number of equal expresions. For example:

    Known Equalities:
    a = a + 1 + (-1)
    Equal Expressions:
    a
    a + 2 + (-2)
    a + 3 + (-3)
    a + 4 + (-4)
    ...

We also need to be careful about equalities that reference the same variable
on both sides. For example, we should not perform substitutions with this:

    a = a + k

since it would lead to a scenario in which we stacked up an unlimited number
of `k`. (Of course, the real information here is that `k = 0`.) So, the
restrictions are:

* Normalize everything (consolidate constants, use `2*a` instead of `a+a`).
* Do not allow the same variable in RHS and LHS.

And this should work. Eventually, we will hit a fixed point if we repeatedly
apply all substitutions to everything in our equivalent set of expressions.
And then we can just check if there is any intersection between both equivalent
sets.

One thing that is not clear is multiplication. The above system assumed that
only multiplication by constants was allowed. If we allow unrestricted
multiplication, then there are equalities that cause trouble. For example:

    a = k + a*a

This is a meaningful equality, and it cannot be reduced. Applying this to
something like `a + b + c` repeatedly, we get:

    k + a*a + b + c
    k + (k + a*a)*a + b + c ==> k + k*a + a*a*a + b + c

And this can go on forever. So I think that multiplication (of two variables)
must be treated as mostly opaque, possibly with a builtin understanding of
associativity. So, the compiler would perform this transformation:

    a = k + a*a
    ==>
    a_squared = a*a
    a = k + a_squared

And the expression `a_squared` would never be substituted in.

Darn, I feel like I really want an SMT solver to be part of the core theory
again. Oh well...

# Refined Types Inside of Type Constructors

I keep thinking about this awful difficulty with type construcors. The problem
is that we have two constructions that represent *exactly* the same thing:

    -- For some X : Type, F : Type -> Type, and G : X -> Prop
    -- and All : {a : Type} -> {h : Type -> Type} -> (a -> Prop, h a) -> Prop
    -- and Proof : Prop -> Type
    (xs : F X, Proof (All(G,xs)))
    ===
    F (x : X, Proof (G x))

Both forms have advantages. The first form makes it easier to weaken a
restriction. If the elements are integers smaller than 10, and we want
to instead say that they must be smaller than 20, we can reuse `xs`
and use something like this on the proof term:

    subtypeAll : (A : Type) -> (p : A -> Prop) -> (q : A -> Prop) -> ((a : A) -> p a -> p q) -> All(p,xs) -> All(q,xs)

The second from is really nice because any time we pull an element from
the container (`lookup`, `index`, etc.), we've got a proof sitting there
ready to go in the result. The second form means we can have:

    lookup : Map a -> Key -> Option a

instead of 

    lookup : (m : Map a) -> Proof (All(p,m)) -> Key -> (r : Option a, All(p,r))

The first one is much nicer to use. But the second form has a caveat.
It mingles proofs and terms.
