# Thoughts on Avro Serialization

This is related to the entry on 2022-01-05.

Avro has a feature called "logical types". They allow someone defining
a schema to control the presentation and possible the codegen for clients
that use that schema. Avro uses schemata aggresively to try to keep data
small. It also has a reasonably expressive schema language that includes
sums and products, both of which can be used essentially without restriction.
This means that the user needs a schema to deserialize anything.

Avro also supports user-defined enum types. Enum types are important because
they come up all the time in practice. Examples include playing-card suites,
months of the year, days of the week, car manufacturers, car models, countries,
states/provinces, etc. Some of these, like anything related to calendars, are
well established and immutable. But some of them are not. Countries can change
and so can parts, models, and vendors. Conceivably, countries could be dealt
with using an extensible enum tied to ISO 3166. But for models and parts,
user-defined enums are essential.

I want a serialization format that has way more built-in types. Things like
UUID, IPv4 address, MAC address, are candidates. I don't like the idea of
dumping a full schema into each message, but I do like the idea of being
able to identify records keys as either numeric identifiers or as
human-readable strings. For example:

    [ { "name": "foo"
      , "description": "bar"
      , "size": 11
      , "height": 100
      }
    , { "name": "baz"
      , "description": "bang"
      , "size": 20
      , "age": 50
      }
    ]

Would be compressed as something like this:

    { 0: "name", 1: "description", 2: "size", 3: "height", 4: "age"}
    [ { 0: "foo"
      , 1: "bar"
      , 2: 11
      , 3: 100
      }
    , { 0: "baz"
      , 1: "bang"
      , 2: 20
      , 4: 50
      }
    ]

This is more like protobuf than it is like Avro. That is, all fields are
prefixed by a key. We never use the order of fields to determine their
meaning. But unlike protobuf, we do not need a schema to figure out the
meaning of the message. We do need the map that tells us what the keys
are, but we do not have to track what type each field in an object is
supposed to be.

Records should be compared for equality by matching up key-value pairs
where the names (not the numeric identifiers) on field keys match. The order
of fields should not matter, but duplicate fields should be rejected.

I do not believe that JSON's approach of overloading the same structure
for both ordered maps and records is a good idea. Ordered maps should be
represented by arrays of two-field objects (the fields being `key` and
`value`).

Enums throw a bit of a wrench into things. Deduplication would accomplish
nearly the same thing. The difference is that deduplication would be able
to pick out arbitrary numbers for each value, and proper support for enums
would put users in control the number that backed each value. Explicitly
controling the number can be important since there might be a number that
we want to match on the other side.

After thinking about this, I think that it is probably best to omit support
for user-defined enums. But many common enums should be supported explicitly.
Not all of these should use numbers like traditional enums do. The states of
the United States of America do not have standard numbers, but they do have
standard two-character codes. These two-character codes could be interpreted
as 16-bit numbers. Here are some examples of enums:

* Countries (numeric codes 0-1000)
* United Staes (two-character state codes)
* Month (number, not sure if starts at 0 or 1)
* Weekday (number, not sure if starts at 0 or 1, ISO says Monday is first day)
* Booleans (number, 0 is false and 1 is true)
* Playing card suits (character, unordered)
* Playing card faces (character, unordered, include joker)
* Directions (north, east, south, west, character, unordered)
* Internet Protocol Numbers (number - tcp, udp, etc., IANA standard)
* Chess piece?
* Seasons?
* Continents? (disagreement on these)

The alternative to standardizing enums like this is to require each message
to include a list of all enum types in each header, and then if the expected
enum values do not match what is in the header, fail to parse it. Something
weird about user-defined enums is that combining two message streams is no
longer generally possible. This is a serious problem.

To me, the most important feature of a serialization format is that it should
be possible to just pick up a message with no additional context and start
figuring out what it is inside it. For this reason, out-of-band communication
of schema is no good. Trying to figure out all the enums upfront is not
possible, but I think that it should be possible to cast a pretty wide net.
Things like car models will fall by the wayside and must be represented
with strings. Maybe continents too.
