# Steps for Grammar-Based JSON Compression

It would be neat to have a way to compress json that let you run queries on
it without decompressing it. Grammar-based compression techniques make this
possible, although they need to be adapted a little bit to work well on JSON.
In particular, JSON objects don't care what order their keys are in.

The compression of JSON needs to be layered on top of a grammar-based string
compression technique so that the string values are able to be compressed.
A technique like Sequitur should work for this.

It is necessary to choose how to encode a model of a compressed entity.
Compressed Pattern Matching for Sequitur suggests arithmetic coding, but
the results show that, despite the good compression ratios, decompression
is incredibly slow. There might be other encoding strategies that make
decompression faster (or nonexistent if the data structure can be `mmap`ped)
at the expense of compression ratio. Consider this structure:

    Positions:
      offsets: position at which offsets begins (redundant)
      tokens: position at which token definitions begin
      sequence: position at which sequence of tokens begins
      seqlen: length of sequence
    Offsets (32 bits words): array of offset for each token definition
    Tokens (variable length): each token is defined by array of tokens and characters
    Sequence (variable length): array of tokens and characters

It is not necessary to decode such a structure. It could be `mmap`ped into
memory and used without even reading everything. The main decision to make
is how to encode tokens and characters. If characters are Unicode codepoints,
they can be UTF-8 encoded, and the illegal prefix 11111xxx can be used
for tokens. So, the 4 most common symbols could be make to each only consume
a single byte.

This seems much more straightforward than I had originally imagined it.
