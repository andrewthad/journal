# Lambda Calculus Syntax Tree Invariants 

I've been playing around with refinement types (both using them and creating
a system for working with them) in an experiment named `safeprop`. Here is
my syntax tree for terms:

    data Term
      = And Term Term
      | Lambda Term
      | Apply Term Term -- function term must have arity > 0, argument term must not be lambda
      | All Term Term -- first argument is a function
      | Truth
      | Integer Integer
      | Plus Term Term
      | Equals Term Term
      | Var Int -- debruijn index, never refers to a lambda

I have several Liquid Haskell measures for this type:

    arity :: Term -> Int
    wellFormed :: Term -> Bool
    freeVarBound :: Term -> Int

These measures are able to enforce that we do not apply `And` to
`Lambda` terms since `And` being well formed requires that its
arguments both have arity zero.

I do not attempt to track the type with a measure since that falls apart once
user-defined types are introduced. One thing that causes me problems is
simplifying known applications. For example, consider:

    Apply (Lambda (Var 0)) (Integer 5)
    ==>
    Integer 5

This is easy if we don't have the `All` construct because without `All`,
every well-typed term should reduce to something without `Lambda` in it.
But `All` throws a wrench in this since terms like this are well formed
and fully reduced:

    -- Interpretation: All elements of array are equal to 17
    All (Lambda (Equals (Var 0) (Integer 17))) (Var _)

I'm not sure how to deal with this problem. It might make sense to have
`Lambda` track its own arity though.
