# More DT Thoughts

I've thought about what I wrote on 2023-05-17 more, and I like the traditional
approach. I think that support for recursive join points can be added pretty
naturally by capturing what they close over.

What I'm thinking about now is how to enforce some kind of phase distinction.
One option is to do a complete split between "ground" functions (which
ultimately are lowered to machine instructions) and proof functions
(which are erased). This technically works, but it makes it difficult
to have any guarantees about bounds-check elisions. For example:

    sum : (n : Nat) -> Array n Int -> Int
    sum n array = ...

In the body, we must construct indices, index into the array, and add them
all up. When we index into the array, how do we know that it is safe. There
are two ways to write this:

    // unsafe, needs bounds check
    index : Array n a -> Int -> a 
    // safe, does not need bounds check
    index : Array n a -> (ix : Int) -> Proof (ix < n) -> a

We want the second one, not the first one, because it's very easy to
create the proof term that it needs. Once we start blending proofs
with ground functions, it becomes harder to know what we can erase.
Anything that looks like this can probably be erased:

    // Note: foo has to be total
    foo : A -> Proof X

The proof itself doesn't exist at runtime, so the function never needs
to be evaluated at runtime. All the functions I explored yesterday
looked like this:

    bar : Proof X -> Proof Y

But sometimes, we do have to match on a runtime value. For example,
we could prove that the length of a list is nonnegative:

    data List a = Cons a (List a) | Nil

    length : List a -> Int
    length Nil = 0
    length (Cons _ xs) = 1 + length xs

    lenNotNegative : (x : List a) -> Proof (length x >= 0)
    lenNotNegative Nil = trivial : Proof (0 >= 0)
    lenNotNegative (Cons y ys) = gteRhsSucc (lenNotNegative ys)

We have to examine the list to do this. However, this examination does not
need to happen at runtime because `lenNotNegative` is total.

What about using QTT like Idris 2? Is that necessary, or can we do something
more simple? A first question to ask is why Idris uses QTT. The
Erasure By Usage Analysis page for Idris 1 explains this nicely. First,
it gives an example where traditional proof erasure techniques work.
Then it shows `uninterleave`, an example where a natural number is not
erased by traditional techniques.

Also insightful is a criticism of `Prop` on a very old wiki page at
https://github.com/idris-lang/Idris-dev/wiki/Egg-%232%3A-Erasure-annotations
and I quote it extensively here in case it ever disappears:

> The core idea of having Prop is having a universe of types of values
> that are designed to be unable to affect what happens at runtime.
> The typechecker just checks that values from Type don't depend on
> values from Prop and code extraction simply erases everything that
> belongs to Prop. This sounds exactly like what we need to do.
>
> However, the realisation of this idea as a separate universe brings various issues.
>
> * The duality pervades the language; there are separate induction principles for
>   Prop and Type, there are separate sigma-types for Prop and Type etc.,
>   which is - I believe - what Edwin wants to avoid in a practical programming language.
> * Having a Prop still does not resolve all problems because vectors are still
>   indexed with Nat from Type and thus these numbers are still present at
>   runtime. I'm unsure whether a Prop version of Nat could be defined, but in Coq,
>   it apparrently isn't. Anyway, having two Nat types is something we probably want
>   to avoid.
>
> Note. Vanilla Coq does not have proof irrelevance (i.e. equality of any two
> proofs of the same proposition in Prop) but this can be postulated as an axiom.

The duality doesn't bother me, but the problems with using a `Nat` as an index
resonate with me. Maybe this can just be solved by making `Nat` efficient.

I've read a bit about Coq, and I think the trick for erasing `Prop` is that
anything of type `Prop` can just go away. So, if we have anything that returns
the type `Proof P`, we just throw that away when compiling. Nothing can
depend on the structure of a proof (usually, I think), so this ought to work.
Maybe we could mostly get rid of DT in parts of the language. For example,
instead of:

    data Ty = Int | Sym
    data Term : Ty -> Type where
      Integer : Int64 -> Term Int
      Symbol : String -> Term Sym
      Add : Term Int -> Term Int -> Term Int

We could do this:

    data Ty = Int | Sym

    data Term : Type where
      Integer : Int64 -> Term
      Symbol : String -> Term
      Add : Term -> Term -> Term

    hasType : Term -> Ty -> Prop
    hasType (Integer _) Int = True
    hasType (Symbol _) Sym = True
    hasType (Add a b) Int = hasType a Int && hasType b Int
    hasType _ _ = False

    data WellTypedTerm = WellTypedTerm (t : Term) (ty : Ty) (Proof (hasType t ty))

    typecheck : Term -> Option WellTypedTerm
    typecheck (Integer i) = WellTypedTerm (Integer i) Int trivial
    typecheck (Symbol i) = WellTypedTerm (Symbol i) Sym trivial
    typecheck (Add a b) = do
      WellTypedTerm a' Int pA <- typecheck a
      WellTypedTerm b' Int pB <- typecheck b
      pure (WellTypedTerm (Add a b) Int (pA && pB))

This is similar to how we would do this with LiquidHaskell, but LH 
requires all the predicates to be unary. This is different from the
GADT-style approach because this approach allows us to construct
ill-typed terms. This could be considered either a strength or a
weakness.

There are certain types (e.g. set, natural) that show up a lot as types.
With natural numbers, we might just be able to pay the cost and have
them show up at runtime. But with sets, we really don't want that.
For example:

    naturalJoin : (xs : Set) -> (ys : Set) -> Rel xs -> Rel ys -> Rel (union xs ys)

We could have some special types that cannot be scrutinized in most
contexts. Although, we would want to be able to scrutinize them when building
a `Prop`. So maybe things like `hasType` ought to use a special function arrow
that allows them to do more stuff. Or maybe not. Here are some set functions:

    empty : Set
    union : Set -> Set -> Set
    insert : Element -> Set -> Set
    singleton : Element -> Set
    member : Element -> Set -> Prop
    difference : Set -> Set -> Set
    insertionMembership : (s : Set) -> (e : Element) -> member e (insert e s)

What if set is parameterized by something? Should it be a ghost type or
a ground type? Maybe allow either:

    Kind
    Relevance = Relevant | Irrelevant
    Type : Relevance -> Kind
    Int : Type Relevant
    Prop : Type Irrelevant
    Set : {r : Relevance} -> Type r -> Type Irrelevant

What about scrutinizing irrelevant types? Do we need to be able to do that
in proofs or not? In runtime functions, it's not even an option since the
information is just not going to be present. But to build a proof, we might
need to tear apart a set. I'm not so sure though. Is there anything useful
that we would need to do that for? I cannot think of anything.

Maybe QTT makes the most sense for this. Every `Prop` must have multiplicity
zero, since it cannot be used at runtime. Types like `Set` would need to
work the same way:

    Kind
    Type : Kind
    Prop : Type
    Int : Type
    IntroBinder = IntroBinder Symbol Type Multiplicity 

We need to make sure that values of type `Prop` are only ever introduced
with multiplicity zero. Section 2.1.3 of Syntax and Semantics of
Quantitative Type Theory has a bit at the end that includes an example
of how to do this.
