# Decompression with Gather

This is a very simple idea: Use the AVX gather instruction for decompression.
I don't have any ideas for compression yet though. What if we have a string
that looks like this:

    [{"health": 55,"age":102},{"health":59,"age":19},{"health":95,"age":44}]
    ^         ^         ^         ^         ^         ^         ^  ^
    0        10        20        30        40        50        60 63
    |--------------------------------------------------------------|
              One x86 cache line, one AVX512 ZMM register

The duplication is evident. How can we remove it in a way that keeps
decompression fast? What about looking for repeated sequences of
a fixed length? Let's consider length 4, and let's require that
gather destinations (but not sources) be aligned. First, we will
break the input into tetragrams:

    [{"h
    ealt
    h": 
    55,"
    age"
    :102
    },{"
    heal
    th":
    59,"
    age"
    :19}
    ,{"h
    ealt
    h":9
    5,"a
    ge":
    44}]

Something sad happened. This didn't actually result in any duplicate tetragrams.
The alignment restriction means that the word `health` could show up four different
times before any duplication is detected. Let's try a larger input sequence:

    [{"health": 55,"age":102},{"health":59,"age":19},{"health":95,"age":44},{"health":95,"age":99},{"health":92,"age":4100}]

Again, break it apart:

    [{"h
    ealt
    h": 
    55,"
    age"
    :102
    },{"
    heal
    th":
    59,"
    age"
    :19}
    ,{"h
    ealt
    h":9
    5,"a
    ge":
    44},
    {"he
    alth
    ":95
    ,"ag
    e":9
    9},{
    "hea
    lth"
    :92,
    "age
    ":41
    00}]

Still, there are only two duplicate tetragrams here. Unlikely to result in any
significant compression. Let's give it one more try with something twice as big:

    [{"health": 55,"age":102},{"health":59,"age":19},{"health":95,"age":44},{"health":95,"age":99},{"health":92,"age":41},{"health":70,"age":16},{"health":75,"age":60},{"health":94,"age":20},{"health":98,"age":23},{"health":97,"age":3}]

Again, broken apart, but we will also deduplicate, count, and assign an identifier:

    00  1  ":23
    01  1  ":41
    02  1  ":94
    03  1  ":95
    04  1  0},{
    05  1  44},
    06  1  55,"
    07  1  59,"
    10  1  60},
    11  1  70,"
    12  1  97,"
    13  1  9},{
    14  1  :102
    15  1  :16}
    16  1  :19}
    17  1  :3}]
    20  1  :92,
    21  1  :98,
    22  1  [{"h
    23  1  e":2
    24  1  e":9
    25  1  h": 
    26  1  h":7
    27  1  h":9
    30  2  "age
    31  2  "hea
    32  2  ,"ag
    33  2  ,{"h
    34  2  5,"a
    35  2  alth
    36  2  ge":
    37  2  lth"
    40  2  {"he
    41  3  ealt
    42  3  heal
    43  3  th":
    44  3  },{"
    45  4  age"

So, now we might actually be able to get some compression out of this scheme.
Each tetragram is identified by a 6-bit number. For ease of implementation,
let's pad this to 8 bits. The original string will be encoded by storing the
dictionary of tetragrams (indices are implied by position) and then convert
each 4-byte tetragram in the original document to a 1-byte index.

    Original Size: 232B
    Compressed Size:
      Dictionary Size: 45 * 4  = 180B
      Indices:         232 / 4 =  58B
      -------------------------------
                                 238B

So, this did not actually compress the input. But, it almost did. Had the input
gotten much larger, the compression scheme would've become effective. Note
that, since we are using tetragrams, then greatest compression factor we
could hope for is 4x. Switching to 8-grams would make 8x compression possible,
but we would end up with 8, rather than 4, different positions for each
repeated term. The nice thing about either of these is that decompression
is extremely cheap. Like, cheaper than any scheme I've ever seen. The
encoding for a byte sequence is:

    (dict*, uint8_t[64]) * (LEN / 64)
      
To work through each byte:

    uint8_t ixs[LEN];
    int lenVec = LEN / 64;
    for(int i = 0; i < lenVec; i++) {
      __m512i vindex = ixs[i]; /* This needs some kind of shuffle to move ixs into place and pad */
      __m512i elems = _mm512_i32gather_epi32(vindex,baseAddrs[i],4)
      ... use the elements however you want here
    }

# Closing Thoughts

I am becoming less convinced that this would be effective as a general strategy
for handling byte sequences in a runtime. For the right data, certainly, but
I believe that the user would need to opt in. The key questions are:

1. How long does the byte sequence live? If it's going to be parsed and
   then discarded shortly afterward, compression is useless. On the other
   hand, if the byte sequences is going to stay around for a long time,
   then compression helps mitigate the penalties associated with cache
   misses.
2. What operations are going to be performed? Random access is possible with
   the suggested scheme, but it's not very fast. A byte-by-byte scan is
   possible, but it's going to be a little bit slower since it requires
   a double loop (probably unloading `elems` to the stack for normal access).
   A few lucky operations like counting occurrences of a character do not
   need to unload `elems` to the stack though.
