# Functional Programming Models

I read something delightful recently. In
[The Story of ISPC](https://pharr.org/matt/blog/2018/04/18/ispc-origins.html),
Matt Pharr summarizes why auto vectorization is difficult to work with:

> I think that the fatal flaw with the approach the compiler team was trying
> to make work was best diagnosed by Tim Foley, who's full of great insights
> about this stuff: auto-vectorization is not a programming model.
>
> The problem with an auto-vectorizer is that as long as vectorization can
> fail (and it will), then if youâ€™re a programmer who actually cares about
> what code the compiler generates for your program, you must come to deeply
> understand the auto-vectorizer. Then, when it fails to vectorize code you
> want to be vectorized, you can either poke it in the right ways or change
> your program in the right ways so that it works for you again. This is a
> horrible way to program; it's all alchemy and guesswork and you need to
> become deeply specialized about the nuances of a single compiler's
> implementation -- something you wouldn't otherwise need to care about one bit.
>
> ... With a proper programming model, then the programmer learns the model
> (which is hopefully fairly clean), one or more compilers implement it,
> the generated code is predictable (no performance cliffs), and everyone's
> happy.

I find this argument compelling, and I am willing to accept it on intuition
alone. It vaguely reminds me Jeff Atwood's claim that, broadly in the world
of software,
[only intermediate users matter](https://blog.codinghorror.com/defending-perpetual-intermediacy/):

> I'll take this a bit further: I think intermediate users are the only users
> that matter. The huge body of intermediate users is so dominant that you
> can and should ignore both beginner and expert users. Developing software
> to accommodate the small beginner and expert groups consumes too much time
> and ultimately makes your application worse at the expense of your core
> user base-- the intermediates.

It is less clear to me that this is broadly applicable or even necessarily
true. However, I'm just going to run with it for now. What's the intersection
between Pharr's claim and Atwood's claim? Simply that a "proper programming
model" (Pharr's language) is important because without one, only "expert users"
(Atwood's language) can use your language. Only an expert could possibly hope
to become "deeply specialized about the nuances of a single compiler's
implementation".

Before I get too carried away, it is worth addressing what the term
"programming model" actually means. Unfortunately, Pharr leaves this a little
vague. Wikipedia has pages for both Programming Model and Execution Model.
The page on Execution Model talks a lot about operational semantics, both
big step and small step. I am going to be little more hand wavy and just
say, for the purposes of the discussion here, that a programming model refers
to the process by which a user can reason about what generated assembly
should be expected from a given higher-level language. A bad programming
model implies that small changes to existing code need to be double-checked
by dumping intermediate (or final) compilation results. A good programming
model means that this is this is not needed.

This has been fairly high level so far. Let's be more specific. What's an
example of something from the functional programming world that is not a
programming model? Stream fusion. Anyone who has ever written code that
relies on stream fusion (probably by using the Haskell libraries `text`
or `vector`) knows that it is brittle and that the resulting GHC Core (an IR)
is difficult to predict. The only users that can wield this tool effectively
are users that:

* Understand rules about linear consumption patterns.
* Know how to use `-ddump-simpl` to dump GHC Core and can examine the results.
* Understand GHC inlining and phases. Some may argue that this knowledge is
  only needed to implement stream fusion, not to use it. This perspective would
  be mistaken. If a user lifts a term that had been burried inside an
  expression to the top level, he or she must often add an accompanying
  phase-controled inline pragma.

Who then can wield stream fusion? Someone who has become deeply specialized
about the nuances of a single compiler. An expert. For this reason, stream
fusion, as currently implemented in `text` and `vector`, is a bad deal for
users. It asks too much of them and offers unpredictable performance to
anyone unwilling to carry the heavy burden.
