# Non-Generational Garbage Collection

Yesterday (2020-12-31), I described a novel GC technique involving a liveliness
bitset on a per-object basis. Although this idea is simple, it has a huge problem.
The space overhead is unacceptable. Conventional GC schemes offer either
single-word (tagged objects) or zero-word (untagged objects) overhead. Multi-word
overhead is a show stopper. I think of the bitset-per-object approach as sitting
at the opposite end of a spectrum. On one end of this spectrum sits classic GC
schemes. These schemes require scanning the entire heap on every collection.
There are two things worth mentioning about these schemes:

* In their purest forms, the classical schemes (nonmoving collectors and
  Cheney-style copying collectors) do nothing to help themselves on
  future collections.
* Generational GC is often used to prevent scanning everything every time.
  In terms of space, it has effectively no cost. For copying collectors,
  every block is assigned to a generation (2 or 3 bits for 4KB of objects)
  and for nonmoving collectors, every object is assigned to a generation
  (2 or 3 bits for every object).

Generational GC does not reduce maximum pause times. At some point or another,
you still must scan everything. You just get to do it 10 or 100 times less
often, which can be a huge performance win. In a traditional setting where
every object is mutable, periodically scanning the entire heap is inevitable.
In this setting, preventing these occassional full-heap scans is just not a
point in the design space. Several thoughts on this:

* Although you have to do the full scan, you may not have to do it all
  at the same time. This is called incremental GC. Every incremental GC
  that I'm aware of works in a nonmoving setting. If you space out GC,
  you get lower maximum pause times.
* Incremental GC improves worst-case latency for some request-response
  workloads. However, it cannot improve throughput. Forklore is that it
  makes throughput worse, the reasoning being that rather than just
  thrashing your cache all at once, you thrash your cache more often,
  bringing your working set back into the cache after each of the
  incremental collections.
* The high number of cache-incoherent accesses resulting from scanning a
  heap makes all classical GC algorithms unsuitable for use with swap. This
  doesn't really matter though because using swap at all is generally
  unacceptable. Memory-mapped files (possibly read-only), which let the user
  be more explicit about possibly touching disk, are the more popular
  alternative to swap.

The goal is to avoid these these cache-incoherent accesses. Let's get back
to the spectrum. Two sides:

* Classical GC algorithms. On every collection, these redo all their work.
  Nothing is that they learn is preserved between collections. The benefit
  of this is low space overhead.
* Working-preserving GC algorithms. These are either nonexistent, proprietary,
  or tucked away in research papers that I cannot find. These algorithms only
  work on immutable objects. Such algorithms would preserve information about
  object dominance. Dominance is a graph theory term. Importantly, dominance
  is transitive. Tracking dominance means that certain unused objects would
  never be rescanned.

How could we go about tracking dominance? The idea from yesterday was bitsets,
but these have huge space overhead. A different approach would be to imply
dominance with memory locations. This seems really hard though. When you created
an object, you would need to inspect the pointers to its children to figure
out where to put the object itself. The computational overhead of this would
be high, and I think that allocations would end up scattered across the heap
(cache incoherent). If we could track dominated subsets cheaply, then generations
would no longer be needed.

# Ulterior Reference Counting

What about using reference counting combined with a nursery? When allocating
into the nursery, counters would not be adjusted. Counter adjustments would
be deferred until collection of the nursery. Figuring out the name for this
was tricky. There are several ideas with similar names:

* Deferred Reference Counting: Pointers that live on the stack do not
  contribute to the total. Not exactly what I'm looking for since you
  would still pay for short-lived objects.
* Deferred Increments
* Update Coalescing Method
* Ulterior Reference Counting (Blackburn and McKinley, 2003): Copying nursery
  with reference counting for the old generation. This is exactly what I had
  in mind.

An immutable and cycle-free heap could use a considerably simpler version of
the GC described in the Ulterior Reference Counting paper. Such a scheme
would suffer lengthy pauses when a large number of objects became unreachable
at around the same time. The Ulterior Reference Counting paper spends a lot
of time dealing with mutation. Since mutation is inevitable in any
general-purpose language, it is important to support it. However, I wonder
if by restricting mutation, the garbage collector could be more simple.

Suppose we have two spaces on the heap: nursery space and RC space. Mutable
objects must support GHC-style in-place freezing, allowing deep freezes as
well. Allocating an immutable object should have no immidate effect on
reference counts. These nursery allocations will later be traced, and their
impact on pointed-to objects will be incorporated into the objects during
collection. That's the easy part. What about mutable objects? If mutable
objects can point into the nursery, then we need to scan them during every
collection of the nursery. Let's contemplate several possible uses of
mutation:

1. Build-and-freeze: Usually a small (under 512KB) data structure. Handling
   something like this with a remembered set is fine because it will not
   survive more than one collection.
2. Long-lived mutable state, no immutable fields: Large (possibly several GB)
   data structures. These can be managed by manually freeing objects. No RC
   is needed.
3. Long-lived mutable state, immutable fields present: Same as previous but
   with a mixture of mutable and immutable fields. The immutable fields are
   likely byte arrays (text is a byte array). Freeing such an object must
   trigger a RC decrement on any immutable fields. Assigning an immutable
   object to a field in a mutable object must increment the immutable object's
   counter.

The tricky thing is figuring out how to make all of these possible with the
same machinery. Scenario 2 is just a degenerate case of scenario 3. Scenario
1 and 3 are very similar, but in 1, we get to skip all the RC stuff. We do
not want users to have to specify which one the are doing, because some
operations like inserts into a mutable map might be used for both. A library
author might not know which one is better since the context might dictate
that. We could say "it depends on what space the mutable object is in". That is,
always provide code for both paths, and at runtime, the appropriate path is
chosen. Free would similarly include two code paths. This is tricky. Also,
think about arrays to make sure this works for them too.
