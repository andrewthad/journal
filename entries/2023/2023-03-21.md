# Columnar Layout for Fixed-Width Integers

Having different columnars layouts for numeric types `s8`, `s16`, etc. is
common. I think that, in a database (or an analytics-oriented
data interchange format), it is a bad idea. Here are the reasons:

1. If you just have `s64` (and forego sub-machine-sized word types), you
   can recover the same space use characteristics with a good delta-coding
   compression scheme. Lemire [blogs about this](https://lemire.me/blog/2012/09/12/fast-integer-compression-decoding-billions-of-integers-per-second/)
   and publishes papers about it. See Lemire and Boytsov's
   [Decoding billions of integers per second through vectorization](https://arxiv.org/abs/1209.2137).
2. Fixed-width types have either correctness problems (overflow) or
   type-system problems when you perform arithmetic on them.
   If we add two `s8` words, we do not know that the result will
   fit inside of an `s8`. So, it is necessary to promote to `s16`. This
   gives addition a strange type.
3. Distinguishing between signed and unsigned types is another source of
   pain for type systems. What happens when we add a `u8` and a `s8`?
   What do various flavors of SQL do? What does C do? It's not intuitive.

To me, the biggest challenge is figuring out how to deal with values larger
than `s64`. I will refer to these as "bigint" values. It could be the case
that a column has nearly all bigints. And it could be the case that a column
has only a few bigints. How can we unify the treatment of bigints with the
treatment of their low-magnitude brethren? Section 2.8 of Decoding Billions
describes patched coding, a technique for moving integers that compress
poorly into an exception list. In the context of the paper, this only
deals with integers with a common maximum size (`s32` or `s64`). Perhaps
bigints could be handled with this same technique. In the system the paper
describes, patched coding runs over blocks of 128 integers. For each block,
maybe we could additionally communicate whether there are any integers that
are larger than `s64`. These could go in a separate exception list.

When we decode a block (possibly just onto the stack), we have to signal
where the bigints are. Let's say that we add two 128-integer blocks together
(vector addition). We have two cases:

1. All integers in both blocks were represented as `s64`. Use a simple loop,
   possibly with SIMD, and also check for overflow. If anything overflows,
   restart this process using a slower technique that can overflow results
   into bigint.
2. One or more integers in either block were represented as bigints. Do
   something slow that processes one pair of elements at a time.

There are other possible cases that we could speed up. If everything fits
in `s32` (or `s16` or `s8`), we know that addition cannot overflow an `s64.
Even an aggregating addition could not overflow in this case.
