# More About Function Properties

This continues thoughts from 2023-05-16. Let's consider an insert function
for maps with integral keys:

    insert : Map a -> Int -> a -> Map a

What are properties this might have?

    insertAllVal      : (allVal f m, f v) ==> allVal f (insert m k v)
    insertAllKey      : (allKey g m, g k) ==> allKey g (insert m k v)
    insertCardinality : cardinality (insert m k v) >= cardinality m + 1

Any time we have a map formed by calling `insert`, we may use these to
create additional information. Note that the first two have preconditions,
and the last one is kind of like a nullary function (technically, they
all take `m`, `k`, and `v` as arguments, so it isn't actually nullary).

Here is how we use these:

    myFunc(m0,v)
        { kprop : Int -> Prop
        , vprop : a -> Prop
        , kall : Proof (allKey kprop m0)
        , vall : Proof (allVal vprop m0)
        , ksat : Proof (kprop 42)
        , vsat : Proof (vprop v)
        }:
      m1 = insert m0 42 v
      m1AllKey = insertAllKey kall ksat
      m1AllVal = insertAllVal vall vsat

How do we actually write the property functions? Here's an idea I've had,
but I cannot find anywhere where this has been suggested before. I call
it a "refinement overlay". The idea is that the author would write out
the same function multiple times. One time is just the normal value-level
version of the function, and it cannot have any proofs. The other variants
are improved versions that may have preconditions and postconditions.
For example, here is `length` written with a recursive join point:

    length : List a -> Int
    length xs =
      go : (List a, Int) -> Int
      go(ys,acc):
        case ys:
          Nil -> acc
          Cons -> go(ys.tail,acc + 1)
      go(xs,0)

And here is a proof about the result:

    lengthNonNegative : (zs : List a) -> Proof (length(zs) >= 0)
    lengthNonNegative zs = magic(length(zs))
      length : (xs : List a) -> (n : Int){Proof (n >= 0)}
      length xs =
        go : (List a, acc : Int){p : Proof (acc >= 0)} -> (r : Int){Proof (r >= 0)}
        go(ys,acc){p}:
          case ys:
            Nil -> (acc){p}
            Cons -> go(ys.tail,acc + 1){incrementGteLhs p : Proof (acc + 1 >= 0)}
        go(xs,0){reflexiveGte 0 : Proof (0 >= 0)}

We write out a copy of `length` that is syntactically identical to the first
version. The names of all the binders have to be the same. But, we are allowed
to introduce proof terms as well. The weirdest part of this is that the return
type of the new length includes a proof term, but we do a weird switch to rewrite
it and make the proof be applied to a lifted variant of the original `length`
function rather than a newly introduced `n` binder.

This has a little too much magic going on. I do like the overlay thing though.
I think it's pretty clear to read. I just feel like the step at the very end,
the one involving the unnatural swap, is strange.

Other thoughts:

* An interesting feature of this way of writing a proof is that we do not
  have to be concerned with non-termination. If the proof doesn't terminate,
  then neither did the original function.
* What kinds of bounds-checking ellisions can we gain from these? If we
  keep proof terms out of the value fragment of the language, then we have
  a hard time being able to confidently say: this index is in bounds.
  But maybe we could have array-indexing functions used at the proof
  level that do have preconditions. It's a kind of roundabout way to get
  the guarantees that we want. The proof level could track termination,
  and then we could even get termination guarantees that way (although
  we need a structural recursion checker for that).
* Integer overflow checking. We could do that too. It would use the same
  mechanism as array indexing.
* What about mutually recursive functions? Just stick both of them in
  there with refined signatures. It should work fine.

Let's try a more traditional approach:

    length : List a -> Int
    length xs =
      go : (List a, Int) -> Int
      go(ys,acc):
        case ys:
          Nil -> acc
          Cons -> go(ys.tail,acc + 1)
      go(xs,0)
    makeProof : (xs : List a) -> Proof (length xs >= 0)
    makeProof xs =
      -- Here, type systems knowns this: length xs = go{}(xs,0)
      makeProofGo xs 0 trivial
    makeProofGo : (xs : List a) -> (acc : Int) -> Proof (acc >= 0) -> Proof (go{}(xs,0) >= 0)
    makeProofGo xs acc p = case xs of
      Nil -> p
      Cons -> ...

What's going on here is that `go` is understood to exist only within a context,
and when we refer to it in a proof, we have to pair it with an environment. In
this case, the environment is empty because `go` doesn't close over anything,
but in general, a join point can close over binders.
