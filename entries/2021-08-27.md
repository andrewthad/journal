# Trees and Tries

The cool thing about a B-Tree is that it's height balanced. The bad thing is
that, if the keys are strings, you have to perform a `O(log n)` number of
`memcmp` operations. The cool thing about a crit-bit trie (or a qptrie) is
that it only performs a single `memcmp`, but the bad thing is that it isn't
height-balanced, and an adversary can easily craft input that destroys its
performance.

One option for combining these data structures is masstrees from the paper
Cache Craftiness for Fast Multicore Key-Value Storage. This is just a trie
where every node in the trie is a B-Tree with 64-bit keys. This doesn't
solve the height-balance problem at all.

Another option is some variant of prefix compression. Emphasis on the word
"variant" because the technique I'm going to disuss actually makes the B-Tree
slightly larger, not smaller. First of all, what is prefix compression? It's
an old technique, used in databases, where every node rips the common prefix
out of the beginning of each key. This is effective because the keys are
sorted, so keys near each other are likely to have a long common prefix.
This is done in both the branches and the leaves. That's the original
technique, and it's used in databases, or at least it was in the 1970s.
It might still be today. The benefits of it are that it saves space and
processing time.

What's good for an in-memory ordered-map library and what's good for a database
are not necessarily the same. One disadvantage of prefix compression is
that you don't store the byte array of the full key anywhere. You have
to reconstruct it. This doesn't matter for databases since you have to
serialize the results anyway (i.e. no sharing), but for an in-memory
data structure in a garbage-collected setting, it's nice to be able
to do walk the map or to ask for the lowest (or highest) key-value pair
without triggering additional allocation for reconstruction of the key.

Prefix compression doesn't really cut down on all those `memcmp` operations
that I was hoping to avoid. What I've been working up to is this: instead
of prefix compression, store the full keys, but also compute the prefix
and store the first four bytes of each key that occur after the prefix.
For example, consider this node with 6 keys and 7 (unshown) children:

    thanos
    thesaurous
    thorax
    thornbush
    thoroughbred

This would become:

    prefix length: 2
    anos | thanos
    esau | thesaurous
    orax | thorax
    ornb | thornberry
    ornb | thornbush
    orou | thoroughbred

This does not illustrate the possibility of a short key that doesn't have at
least 4 bytes after the prefix, but let's ignore that possibility for now.
Notice that `thornberry` and `thornbush` illustrate a kind of "bad case"
where two adjacent keys do not differ in the four bytes that follow the
prefix. When we look something up, we first compare the appropriate slice
of it against all of the four-byte prefixes. This can be done quickly with
SIMD. Let's consider some possibilities:

                          + thistle + thoraxical + thynes + thornburst
    anos | thanos         + GT      + GT         + GT     + GT
    esau | thesaurous     + GT      + GT         + GT     + GT
    orax | thorax         + LT      + EQ         + GT     + GT
    ornb | thornberry     + LT      + LT         + GT     + EQ
    ornb | thornbush      + LT      + LT         + GT     + EQ
    orou | thoroughbred   + LT      + LT         + GT     + LT

The best case is when there is no `EQ`. This means that we don't need
to perform any `memcmp` operations. If there are one or more `EQ`s
(and they will always be adjacent), we must `memcmp` against the
corresponding keys, stopping when we encounter one that the needle
is less than.

There are several variations of this idea. One is to use only 1 or 2
bytes instead of 4. Another idea is to forgo the SIMD possibilities
and try to pack things differently in memory. All of them have the same
goal of trying to reduce the number of `memcpy`s to one or two for
an average-case lookup.

Pathological input is possible, and in this case, the height invariant
would be upheld, but you would end up performing a logarithmic number
of `memcpy`s. Although it's a logarithmic number, it's a bit worse
than the number that a balanced binary tree would perform, but it's
only worse by a constant factor (the constant factor is related to
the fanout).

# Different Primitives for Arrays

When programmers think of arrays, the classic primitives come to mind: read,
write, index, whatever. And there are a few other ones, maybe `memcpy` to
prevent explicit iteration for copying, but with reading and writing, you've
got enough to do anything.

However, this minimal core of primitives comes at a cost. On simple CPUs from
a bygone era, these primitives worked fine. Now, however, CPUs have
instructions that operate on many elements simultaneously. SIMD is the
first thing that comes to mind, but even the older BMI instructions work
the same way. It's difficult for compilers to recognize high-level patterns
that can be compiled to these primitives, so in C at least, intrinsics
are popular.

There should be a better way. Autovectorization is possible, but it's
unreliable. Could it be made more reliable? But also, just generally,
on fixed-width or variable-width vectors, we should be able to have
instructions like this:

    countMatches : Vector {n} a -> a -> (Int | _ <= n)
    sum          : Vector {n} Int -> Int -> Int
    insert       : Vector {n} a -> (Int | _ <= n) -> a -> Vector {n + 1}

You don't need a magic compiler to recognize high-level patterns if
you just make the high-level patterns into primops. And I know, some
of this gets into the nasty business of distinguishing arrays with
an equality or ordering relation from arrays without this. But I
don't think that's an insurmountable issue. We really do need an
array "typeclass" (or module signature) so that Word16 and Word32
can have nicely packed vector types. Trying to distinguish the
representations of smaller fixed-width word sizes from Word64 does
not seem like a good idea. Then we end up with this:

    class ArrayElement a
    class Eq a
    class Eq a => Ord a
    class (Eq a, ArrayElement a) => EqArrayElement a
    class (Ord a, ArrayElement a) => OrdArrayElement a

The `ArrayElement` class feels like nonsense. Everything boxed should
be able to be stored in an array without incurring any constraint.
But the unboxed stuff really does need a constraint. We have to know
if we are talking about putting Word16 or Word32 in an array because
we must generate different code depending on which one we are dealing
with.

Let's try to adapt SML's monoarray:

    signature Eq
      type A
      equal : A -> A -> Bool
    signature Contiguous
      type A
      type Array {Nat}
      index : Array {n} -> (Int | _ < n) -> A
    signature ContiguousEq
      type A
      structure contiguous : Contiguous where type A = A
      structure eq : Eq where type A = A
      countMatches : contiguous.Array {n} -> A -> (Int | _ <= n)
    

It would be nice to be able to just alias `contiguous.Array` to `Array` in
the `ContiguousEq` signature, but aside from that, this seems totally fine.
Let's say we want to write a swap function on top of `Contiguous`. It would
have a signature like this, assuming that we extended SML with convenient
syntax for having a function take a module as an argument:

    swap : (Contiguous c) => c.Array {n} -> Int {<n} -> Int {<n} -> c.Array {n}

More ambitiously, let's try a permute function where the indices can
be stored compactly if the caller can guarantee that they are small enough:

    permute :
         (m : Nat)
      -> (n : Nat)
      -> (i : Contiguous where type i.A = (Int | <m))
      -> (c : Contiguous)
      -> i.Array {n}
      -> c.Array {m}
      -> c.Array {n}

This is neat. Let's try to specialize to 16-bit indices and see what happens:

    module Word16Contiguous : Contiguous
      type A = (Int | < 2^16)
      type Array {Nat} = Word16Array {Nat}
    permute {i = Word16Contiguous}
      ==> expected type i.A = (Int | <m) but got type i.A = (Int | <2^16)

This would rightfully fail because `m` is not constrained in any way.
However, if we could put constraints on naturals, then:

    permute {m :< Nat | <2^16, i = Word16Contiguous}
      ==> expected type i.A = (Int | <m) but got type i.A = (Int | <2^16)

Still no good. We need to be able to constrain the element type of
`Word16Contiguous`. We cannot do this in a `where` clause because that
would not be sound (`Word16Contiguous` could return something that was
not appropriately bounded). Where to do it? This feels like it's coming
back to the idea of keeping the element type separate. Here are some
different places in the design space:

* SML-style monoarray, shown above
* Tie array to representation type, good for boxed vectors.

Let's try the second option:

    signature Contiguous
      type R : Rep
      type Array {Nat} R
      index : forall (a : R). Array {n} a -> (Int | _ < n) -> a

Now what happens:

    permute :
         (m : Nat)
      -> (n : Nat)
      -> (i : Contiguous where type i.R = WordRep)
      -> (c : Contiguous)
      -> i.Array {n} (Int | _ < m)
      -> c.Array {m} a
      -> c.Array {n} a
    module Word16Contiguous : Contiguous
      type R = IntRep
      type Array {Nat} a = Word16Array {Nat} a

Oh, right. We cannot define `Word16Contiguous` any longer because we
have no place for the refinement to go. Maybe this isn't too bad
though. We could just ask that people not apply `Word16Array` to
types that it cannot handle. But it would be nicer to enforce this
somehow. Wait. Let's look at the first strategy again:

    -- Allow the upper bound to be configurable.
    module Word16Contiguous (m : Nat | m < 2^16) : Contiguous
      type A = (Int | <m)
      type Array {Nat} = Word16Array {Nat}
    permute {m :< Nat | <2^16, i = Word16Contiguous m}
      ==> expected m <2^16 and this is satisfied
        , expected type i.A = (Int | <m) and got type i.A = (Int | <m)
        , looks good
      ==>
         (m : Nat | <2^16)
      -> (n : Nat)
      -> (c : Contiguous)
      -> (Word16Contiguous m).Array {n}
      -> c.Array {m}
      -> c.Array {n}

This works and has the type that we would expect. However, to accomplish
this, we had to parameterize `Word16Contiguous` by an upper bound on
the elements. This is a weird thing to do. Also, `m` is not relevant,
which is very important above since it means that all that `Word16Contiguous`
implementations are identical, so we may need to start tracking relevance
as well.

An alternative implementation of `Word16Contiguous` is:

    module Word16Contiguous (r where r :< (_ >= 0 && _ < 2^16)) : Contiguous
      type A = (Int | r)
      type Array {Nat} = Word16Array {Nat}
    permute {m :< Nat | <2^16, i = Word16Contiguous (_>=0, _<m)}
      ==> expected (_>=0, _<m) :< (_>=0, _<2^16) and this is satisfied
        , expected type i.A = (Int | >=0, <m) and got type i.A = (Int | _>=0, _<m)
        , looks good

This makes refinements a little more first class. It is nice to be able
to do this. In Haskell, it is difficult to do this, have it perform well,
and have it set up to disallow out-of-bounds accesses.

# Mergesort steps with simple refinements

In mergesort, the heart of the algorithm looks like this:

    step :
         i | >=0,<n
      -> j | >i,<n
      -> MutArray {n}
      -> MutArray {n}
      -> ST ()
    step start end bufA bufB = if end - start < SMALL_CONSTANT
      then simpleSort start end bufB bufA
      else do
        let mid = (end + start) / 2
        step start mid bufB bufA
        step (mid + 1) end bufB bufA
        merge start end mid bufA bufB

You flip the buffers at every step so that on your way back up, the `merge`
operations swaps its source and destination. If the function for computing
`mid` had a type, what would it be:

    calculateMid : (a | >=0,<n) -> (b | >=0, >a, <n) -> (c | >=a, <b, <n)

This is getting outside of what can be comfortably accomplished in this
setting. Liquid haskell could handle this easily though. Hrmmmm...
