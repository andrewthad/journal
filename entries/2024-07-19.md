# More Low-Level PL Thoughts

I've been thinking more about a low-level language. I think that the language
needs to be representation monomorphic but type polymorphic. For example:

    booleanEliminator[4] : (a : Type4) -> Bool -> a -> a -> a
    booleanEliminator[8] : (a : Type8) -> Bool -> a -> a -> a
    booleanEliminator[16] : (a : Type16) -> Bool -> a -> a -> a

Here, we have three different specializations of the `booleanEliminator`
function. The number in square brackets does not enforce anything. It is
really more of a hint. The function bodies could be totally different.
We just don't want them to be.

What size classes should exist? I was thinking of these: 1, 2, 3, 4, 6, 8,
12, 16, 20, 24, 32, 64. That's a total of 12 size classes. For some of
these, I have very specific applications in mind. RGB values take 3 bytes,
MAC addresses take 6 bytes, and SHA1 hashes take 20 bytes. I cannot think of
anything that needs exactly 5 or 7 bytes (and these could just be rounded up),
so I left them out. With 12 size classes, we end up with 144 specializations
of a function with 2 type variables. At 3 type variables, we would need 1728
specializations, which is probably too many. But how often do we need 3 type
variables? Not often. Looking at `Data.List` in GHC Haskell, I see that some
of the worst "lots of type variables" offenders are `zip` functions, the
greatest of which is `zip7`. But these cases can be handled with inlining.
Folding and traversing can lead to lots of type variables:

    mapAccumL :: (acc -> x -> (acc, y)) -> acc -> [x] -> (acc, [y])

We have 3 type variables here, but again, we want this function to be inlined.
As a rule of thumb, if a function has a function argument that may be called
more than once, it is desirable to inline the function. Everything for
traversing maps and lists falls into this category.

# Undetermined Types

If the lowest level is not fully monomorphized, this leaves more options on
the table for lightweight dependent types. Possible applications:

* Regions
* Finite numbers with a natural number as a bound

The interaction between natural numbers (used for DT) and integers is a little
tricky. Consider:

    index : (n : Nat) -> Array {n} Foo -> Fin n -> Foo
    withLen : Array Foo -> ((n : Nat) -> Array {n} Foo -> a) -> a

The low-level language does not have higher-order functions, so `withLen` needs
to be some kind of built-in construct. That's fine. The callback should have
access to any join points that were already in the context. That is because
the callback is actually on a straightline path. It's called instantly, not
stashed away for future use.

Also, `withRegion` would work nearly the same way:

    withRegion : ((r : Region) -> b {r} -> a) -> a

However, join points would need to be blown out of the context. The region
has to be deallocated at a predictable place. 

# Mutable Maps (With S32 Keys) That Are The Correct Size

I don't think this is actually worth pursuing, but it would be kind of neat
if a node in a mutable map (either a branch or a leaf) was always 128 bytes
(or maybe 64 bytes). In a B+ tree that's mutable, we always have to allocate
the full amount of space we might need for each node even when we only use
a little bit of it. Let's say we have 4-byte keys (maybe s32 keys) and 4-byte
objects. How many key-value pairs can we pack into 128 bytes? We need a tag
byte to distinguish leaves from branches. And we need a byte that tells us
how many key-value pairs are actually present. So, for a leaf, we could have:

    15 keys   = 60 bytes
    15 values = 60 bytes
    1 tag     = 1 byte
    1 size    = 1 byte
    ---------------------
                122 bytes

And for branches, we would have:

    15 keys     = 60 bytes
    16 children = 64 bytes
    1 tag       = 1 byte
    1 size      = 1 byte
    ---------------------
                  126 bytes

We might also want to track the height of each node. That's fine. It only
requires one additional byte, and we have space to do that.

Sticking with 4-byte keys, here are some other leaf sizes (branches are
always the same because children are always 4 bytes):

* 2-byte values: 21 values in leaf (fully saturates 128 bytes)
* 4-byte values: 15 values in leaf
* 8-byte values: 10 values in leaf
* 16-byte values: 6 values in leaf
* 32-byte values: 3 values in leaf
* 64-byte values: 1 values in leaf (not acceptable, must use more than 128 bytes)

One problem with doing this is that it violates parametric polymorphism. It
means that the size of the element changes something about the data in a way
that is observable to the end-user. But if all the functions for dealing
with maps are built into the compiler (and if the types are opaque), maybe
this is alright. Other languages (golang, python) have map types built into
the language, and users cannot (or should not) build competing implementations,
and it works out fine.

# Refinements

I'm not really sure how refinements should work. If I have finite numbers
available, then I may not actually need refinements. Refinements aren't
necessary for dealing with data constructors. I could use a different
strategy to explicitly upcast from the subtype (a data constructor) when
needed. And that would work fine.

But the one place I really do want refinements is when dealing with `U8`
values that are ASCII characters. There are tons of protocols where the
result of certain actions is something like "ASCII characters in the printable
range" or "lowercase, uppercase, and underscore". Sometimes, we want to
just forget about this additional information. But sometimes, we care about
it, or we care about it a little bit. Maybe we don't care that the entire
unprintable range cannot be used, but we do care that NUL cannot be present
in the sequence. So we might want to weaken things.

For bounded integers, we end up with two types: `Int` and `Fin`. And
this is fine. For 8-bit words, we might not need the same split. We could
just have something like: `W8 : Predicate -> Type`. And syntactically,
we could let the user omit the predicate to mean "all values accepted".
We have to decide several things:

* Do we want variable predicates (ones that can be determined at runtime)?
  A predicate can be represented with 256 bits. It would let us have
  functions like `test : (p : Predicate) -> U8 -> Option (U8 {p})`.
* Do we want subtyping? If coercions have to be explicit, then we must
  have a zero-cost way to lift them over type constructors.

It's the "lift coercion over type constructors" part that's difficult. For
integers and finite numbers, we do not need to convert between them often.
They're used for different things. (Finite numbers are indices into arrays.)
But U8 often appears inside an array, and we want to upcast the whole thing.
For example:

    decodeHeader : Parser MyError (Array U8{Printable})
    fullParser : Parser MyError Foo
    fullParser = do
      name <- decodeHeader
      doSomething(upcast(name))

Maybe we just need this:

    upcastArray : (p <: q) -> Array U8{p} -> Array U8{q}

But in theory, we can have this instead:
      
    upcastAny : (p <: q) -> f U8{p} -> f U8{q}

I like the idea of having it be explicit, at least in the low-level language,
because that makes typechecking more simple.

Also note that we could, in theory, do the same thing to allow users to
convert finite numbers to integers.

# Type Constructors

How much support for type constructors should there be? Some languages do
not include them at all. Let's consider type constructors in two cases:

* Low-level language
* High-level language

In the low-level language, what could we even do with a type constructor?
There are no modules, and there are no typeclasses. And HOFs exist, but they
are pretty weak. So I do not see a reason to have type constructors in the
low-level language.

But in the high-level language, type constructors feel more reasonable.
But still, I might be able to avoid them. Here are places where I have found
that they can be useful:

* Writing effect-polymorphic or monad-polymorphic code. Although in the
  language I am thinking of, this kind of thing is done without a type
  constructor.
* Module signatures that require implementations of a map that can accept
  any element type. In theory, this can be worked around by instead require
  a bunch of different maps, each one accepting a different element type.
  And then the user would probably supply the same map for each one.
