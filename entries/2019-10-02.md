# Message Multiplexing

Linux does not have any IPC mechanisms for one-to-many
broadcasting. There was an [attempt in 2003](https://lwn.net/Articles/27001/)
and [another one in 2012](https://lwn.net/Articles/482523/).
To the best of my understanding, neither of these were accepted.
The criticism of the second attempt seems to center around
the fact that its sole motivation was to make D-Bus faster.
Responding to an inquiry about a hypothetical `AF_MCAST`,
Eric Dumazet [writes](https://lists.openwall.net/netdev/2012/03/01/89):

> It seems application logic and complexity pushed into kernel,
> for a very single user (even if used in a lot of products): D-Bus

The only OS-backed option currently available for IPC multicast is
using an `AF_INET` datagram socket on the loopback interface.
Javier Martinez Canillas [argues](https://lists.openwall.net/netdev/2012/03/01/80)
that the lack of ordering guarantees make this inappropriate for
some applications. Dumazet [contends](https://lists.openwall.net/netdev/2012/03/01/110)
that applications that want the best performance possible should
simply use shared memory instead of passing messages.

What I thought about today is how to do accomplish many-to-many
multicast with a little bit of backpressure. That is, at least one
receiver must receive whatever is being sent. If there are no
receivers, the sender should just block since there is no
purpose in generating messages. It is not possible to get this
behavior with an `AF_INET` socket. Also, a [bunch of people
on StackOverflow](https://stackoverflow.com/q/2128701) claim
the loopback interface is unreliable. It this rest of this entry,
I outline a possible architecture for a simple pub-sub system
that has the guarantees I want.

At the heart of this is a small hypothetical application
named `msgmux`. It is expected to be running constantly,
and it output sinks are reconfigured as it runs. It
uses two messages queues, each for a different purpose.

1. Messages. A number of sources of messages can dump into
   this queue, and `msgmux` simply reads them out one at a time.
   This lives at a fixed place on the filesystem and does
   not change as `msgmux` runs.
2. Sinks. This is the interesting one. Every sink that wants
   to receive messages must register itself by pushing a
   a filename corresponding to UNIX domain datagram socket
   (that the sink client had called `bind` on already) onto
   the sink message queue. These filenames should probably
   be for the abstract socket namespace to avoid accidental
   leaks. `msgmux` should periodically read from this queue
   to maintain an up-to-date list of sinks.

One might think that `msgmux` itself only needs to open a single
UNIX-domain datagram socket, sending everything out with `sendto`.
Unfortunately, this would mean that there was only a single send
buffer. If a sink hangs, this would mean that other sinks would
lose messages. I'm not totally sure about this since there was
[a patch](https://patchwork.ozlabs.org/patch/226393/) to fix this,
but I have no idea whether it was merged. To be safe, I'll say
we need a socket for each sink. If we ever get `ECONNREFUSED`
when we try to write, we know that the sink has either closed its
socket or terminated, and then we would want to close our end as
well to free up the file descriptor and the entry in the abstract
socket namespace. There are more complications. The interaction
between `poll` and datagram sockets
[is bad](http://developerweb.net/viewtopic.php?id=4573), and
I am not sure if this was ever improved. So, if `poll` tells
you that you can send, you might want to check to see if any
datagrams actually got sent anywhere. If none were sent, it
would be prudent to perform some kind of delay. Regardless of
how which sockets you get `POLLOUT` for, I think you probably
want to try to write to all of them in case any of the other
ones happened to become available.

Datagram sockets and POSIX message queues are similar. One
difference that is irrelevant here is that message queues let
you have multiple receiving processes that split a load (like
a Kafka consumer group). The big difference that matters in
the context of `msgmux` is what happens when you send to
something that's been abandoned. With message queues, there
is no way to figure out that the peer process terminated,
so you don't know when to quit trying to push data to it.
With a UNIX-domain datagram socket, you'll get `ECONNREFUSED`
when the peer exits. This is really helpful because it
opens up the possibility for short-lived consumers (think
about something like `kafka-console-consumer`). Interestingly,
for message reception, this distinction vanishes. That is,
neither message queues nor UNIX-domain datagram sockets
tell you if you are trying to receive from nobody. Consider
this example:

    #include <stdio.h>
    #include <stdlib.h>
    #include <sys/socket.h>
    #include <sys/types.h>
    #include <unistd.h>
    
    int main() {
       int sockets[2], child;
       char buf[1024];
       if (socketpair(AF_UNIX, SOCK_DGRAM, 0, sockets) < 0) {
          perror("opening stream socket pair");
          exit(1);
       }
       close(sockets[1]);
       if (recv(sockets[0],buf,sizeof(buf),0) < 0) {
          perror("could not send data");
          exit(1);
       }
       printf("Done\n");
       return 0;
    }

This hangs forever rather than terminating with an error. This
could be considered an advantage or a detriment depending on what
you are trying to do. Thinking about it more, the message queue
that handles sinks should probably actually be a datagram socket
since there's no point in letting sink requests pile up if
nothing is running. For sinks, it is probably useful to support
both dynamicly requested datagram sockets (identified in the
abstract socket namespace, requested on the sink request queue)
and statically requested datagram sockets (that live in the
normal filesystem). The former are for cheap tricks, one-off
processes that consume messages for a minute or so and then
are done. It's nice to be able to run these without having
to reconfigure `msgmux`. The latter are for processes that run for
a long time. What this buys you is the ability to restart `msgmux`
without long-lived clients becoming aware of the restart. I'm not
sure if it's actually worth it though. I don't think you could
restart the clients themselves without losing messages. It
might be better to make long-lived clients use message queues
instead. That way, a restart on either would not cause any
message loss, assuming that everything had been keeping up
well to begin with.
