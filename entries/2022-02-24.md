# Bitsets, Caching Encoding

Two unrelated thoughts today.

## Bitsets as the Values of Maps

What if, as a data structure for accelerating queries with an exact-match
predicate, you had a map (probably an ordered map) with bitsets as the
values? The bitset is used to tell you at which indices the predicate is
satisfied. What then would be the space characteristics of this data
structure? If you did something naive like not compressing bitsets in
any way, then you would get:

    O(C * N)

Where C is the cardinality of the field and N is the total number of rows.
For a fields in which all values are distinct, this degrades to

    O(N^2)

which is terrible. But, if you compress the bitsets with something like
roaring, then this would not happen. In fact, the above case would become

    O(N)

since a roaring bitset with a single true index takes constant space. But
with different distributions, what happens? What's the worst case?

## Caching Encoding

Often, I find myself building a syntax tree that is going to later be
encoded. Parts of the tree are constant. Consider an expression like:

    data Html = Node String [Html] | Text String

    myNode :: String -> Html
    myNode topic = Node "div"
      [ Node "h1" [Text "Hello World"]
      , Node "p"
        [ Text "Today we will be talking about "
        , Text topic
        , Text ", my favorite subject!"
        ]
      , Node "footer" [Text "Thanks for reading"]
      ]

There are parts of this tree that are constants. The way that HTML is encoded,
it is possible to encode adjacent fragment independently and then concatenate
the results. This means that it should be possible to encode the constant parts
a single time. But how? One option is to change html to cache its own encoding:

    data Html = Html ByteSequence Step
    data Step
      = Node String [Html]
      | Text String

But this has some drawbacks. You have to start using smart constructors.
Additionally, your language needs to support laziness if you want to be
able to modify a syntax tree without building byte sequneces you don't want.

Here's another option:

    data Html = Html (Maybe ByteSequence) Step
    data Step
      = Node String [Html]
      | Text String

This distinguishes itself from the previous option by making the byte sequence
optional. Any part that is truly constant would be annotated with its encoding.
Other parts would not be. The advantage of this is that it doesn't require
laziness. However, it still needs smart constructors. Let's go one step further.
What if this was a language construct? What might it look like?

    data Html = Node String [Html] | Text String

    example :: Html<toUtf8>
    example = Node "div" ...

    toUtf8 :: Html<toUtf8> -> ByteSequence
    toUtf8 h = case h of
      Node _ _ -> ...
      Text _ -> ...

Under the hood, the compiler would turn any calls to `toUtf8` into calls that
first checked for a special common field, the optional encoded fragment.
If it is present, just return it instead of casing on the html node. All
html values have to be tagged to indicate what encoding is ultimately expected.
And crucially, the compiler is tasked with calling `toUtf8` on any constant
fragments (at compile time) to populate their pre-encoded fields.

The suggested syntax is a little confusing since `toUtf8` references itself
in its own type signature. It is crucial that only the `toUtf8` function
have access to the annotation. If other functions could access it, then
optimization level could change the behavior of a program. In particular,
it should be perfectly acceptable for the compiler to omit the annotations
entirely.

Various transformations and folds would be agnostic to the encoding.

Hmmm... now that I think about it, there is at least one other function
that would benefit from access to the annotation field: a decode function.
It could help avoid needless reconstruction of something that is already
available. But now I'm torn between the options of letting the user
access this and not letting them access it. Perhaps the user could be
allowed to *supply* the annotation but not observe it. Or maybe just
trust the user a little more:

    Html[enc]:
      annotation: Option ByteSequence [computed by enc]
      subtypes:
        Node:
         tag: String
         children: Html
       Text:
         value: String

The compiler is free to do its thing and the user can also do whatever they
want to do with it. Now `toUtf8` would become:

    toUtf8 :: Html<toUtf8> -> ByteSequence
    toUtf8 h@Html{annotation} = case annotation of
      Some b -> b
      None -> case h of
        Node{} -> ...
        Text{} -> ...

The self-reference is still strange. It would be nice to be able to break out
the tag for `Html` into something different. Although at some point, it would
be necessary to make sure that the compiler understands that the tag is related
to the function. Otherwise, it would not be able to insert the necessary
annotations. However, this knowledge is not needed in the `toUtf8` function
itself:

    data Utf8
    set Html[Utf8] = toUtf8

This is antimodular. The assignment is global (it is much like a typeclass
instance). Framing it in this light helps me see that something similar
could be accomplished with modules:

    -- An encoding indicates how text should be converted to bytes. It
    -- could also include special deforestations like going from
    -- numbers to bytes.
    signature Encoding: ...
    module function OptimizedHtml(Encoding enc):
      data Html = ... -- like before with the annotation explicit, references encode
      encode :: Html -> ByteSequence
      encode h@Html{annotation} = case annotation of
        Some b -> b
        None -> case h of
          Node{} -> ...
          Text{} -> ...

The main difference here is that every html type is now different. If a library
wants to provide some generic HTML operation (parser, tag soup, extract field
at position, make all text lowercase, etc.), they pay a penalty for something
that they do not need. Maybe just a UTF-8 the default or make one with no
annotation at all the default.

Something about both html and json that I had overlooked is the pretty printing.
A user might want to encode with extra space in the document.

Sadly, this idea appears to be going nowhere. The extra complexity is a burden
that users should not have to shoulder. The only way that this would become
usable would be if:

* Encoding was hard-coded to UTF-8
* No pretty printing was supported
* Encode and decode are builtins. Users cannot touch annotations.

Then users would get a faster encode function but without anything else
becoming more complicated.
