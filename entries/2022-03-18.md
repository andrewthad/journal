# Compiler Thoughts

## Metaprogramming

Several bad experiences I've had been metaprogramming were the result of
metaprograms creating top-level identifiers. If I'm browsing some source
and I see an identifier, I expect to be able to use grep (or any grep
replacement) to find where that identifier is defined. There is a simple
solution to this problem: require definitions of any created identifiers.
For example, in pseudo-GHC:

    data Foo = ...
    encodeFoo :: Foo -> Bytes
    encodeFoo = $(makeEncoder Foo)
    decodeFoo :: Bytes -> Maybe Foo
    decodeFoo = $(makeDecoder Foo)

Here are several other cases and possible solutions:

* Problem: Metaprogram creates a data type that is visible to users.
  Solution: Don't allow this. Require the data type to be defined
  by the user, and the metaprogram can instead verify that it matches
  a certain shape.
* Problem: Metaprogram binds many top-level identifiers.
  Solution: Add some kind of syntax for binding many identifiers.

## Compilation Stages

Many features in a GHC-core-like IR need to be compiled to lower-level
constructs before we can ultimately convert a program to assembly.
Some of these lowerings are architecture-neutral, but some are not.
For example:

* Case on Int. Densely populated translates to a jump table, and sparsely
  populated becomes a binary tree of comparisons. If we ignore the possibility
  of a jump table, then the straightforward lowering is to compile case-on-int
  into a series of case-on-comparison.
* Case on String. Even more complicated, and there are a bunch of strategies
  depending on what you are checking against.
* ANF. Make functions and case statements only accept literals and variables.
* Guard bit functions on x86. Architecture dependent. On x86, the BSR instruction
  is used to count leading zeros. When its argument is zero, its result is
  undefined. ARM has a CLZ instruction that is the same except that it is
  well defined when the argument is zero. In this particular case, x86 later
  remedied the issue with a LZCNT instruction. However, there is a similar
  issue with bit shifting where x86-64 behaves poorly when instructed to shift
  by 64 or more, and ARM produces the expected zero output. So, at a high level,
  the user should be able to say "shift right by n" and expect that a large n value
  with cause a zero result, but somewhere in the compilation pipeline, the
  shift instruction must become a different instruction depending on what
  is really being done. It may even correspond to two or more instructions,
  like on x86, where count leading zeros could be lowered to either CLZ or
  BSR depending on what is known about its argument.
* Loops. Lower these to join points.
* SIMD. I think that giving the user explicit control over vectorization
  is generally not good. Somewhere in the compilation pipeline though,
  vectorization must become explicit. That is, the types `Int32` and `Int32x8`
  must be different. These will be very architecture dependent.
  
