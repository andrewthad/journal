# Compressing Small Arrays of Pointers

I've been thinking about a good way to do pointer compression on and off for
a while now. The JVM has this cool feature where, if you use less than 32GB
of memory (actually usually less than 31GB because of something I cannot
remember), all of your pointers get to be represented by 32 bits instead of
64 bits. You get a 32GB limit instead of a 4GB limit because in the JVM, you
cannot address anything smaller than an 8-byte word.

The JVM's feature is neat, but it has these two characteristics that I
find unsatisfying:

1. Only works with heaps up to 32GB.
2. When it works, pointers take up 32 bits. Greater compression is
   possible. For a sufficiently small data structure, pointers
   could be 16 bits. Or 24 bits.

I am interested in a form of pointer compression that does not have
those characteristics. I have two additional constraints:

1. Single code path at runtime. No JIT.
2. Supports mutable and immutable variants with in-place freezing
   (converts mutable to immutable).

I believe that there is a way to accomplish these goals. The idea is simple.
Instead of attempting to compress all pointers in an application, only perform
compression on arrays of pointers. Use base-offset + LEB128 to encode the
offsets. But do not use delta coding. (LEB128 is not essential. Any kind of
variable-length encoding should be fine). The drawback to this form of
compression is that modification is not generally possible. However, random
reads are possible. Let's say that we have a medium-fan-out tree-like
structure:

    struct Node
      { values   : Array 8 W32      // exactly 32 bytes
      , children : Array [1-8] Node // worst-case 48 bytes (6 bytes per pointer)
      }

The representation for mutable nodes is:
    
    struct Node
      { values     : uint32_t[8]
      , base       : uint64_t
      , offsetBits : bit[48], all set to 1 // 6 bits * 8 possible objects (6 bytes)
      , children   : uint48_t[] // Flexible array of up to 8 48-bit pointers
      }

The representation for immutable nodes is:
    
    struct Node
      { values     : uint32_t[8]
      , base       : uint64_t
      , offsetBits : bit[48] // 6 bits * 8 possible objects (6 bytes)
      , children   : uint8_t[] // Flexible array of up to 8 LEB128-encoded offsets
      }

Let's start with `offsetBits`. This tells us how many bits were needed for each
field. With a mask and a `popcnt`, we can find the start index of any element.
A second mask and `popcnt` will tell us the number of bytes in the element. And
then an unaligned load and a mask will give us the offset. Then we add to the
base and we've recovered the pointer.

Mutable objects have the same layout as immutable ones. However, mutable objects
make pointer take the maximum possible number of bytes. The compression is done
by the garbage collector when the object ages. Notice that thawing objects is
somewhat expensive since it requires decompressing `children` rather than the
simple `memcpy` is typically used to thaw an object.

An alternative design would just use the same number of bits on each child. This
is a little more simple and might work just as well. If this were done, `offsetBits`
could be a single byte rather than 8 bytes. Since the base is really only a 48-bit
number, it could then be packed together with `offsetBits`.

An additional enhancement is to track pointer alignment. This could be done
statically or dynamically. If all children are 8-byte aligned, then you could
omit those bits. In practice, many cache-coherent data types will be 64-byte
aligned. The gain from this is potentially huge since recovering those 6 zero
bits at the end of all 64-byte-aligned children would make it possible for a:

* 32-bit offset to cover 256GB of addressable memory
* 24-bit offset to cover 1GB of addressable memory
* 16-bit offset to cover 128MB of addressable memory
* 8-bit offset to cover 512K of addressable memory

What is the expected impact of the originally described scheme?

1. If you are using less than 4GB memory (common), your arrays of pointers
   end up taking 4 bytes per child pointer after promotion. This requires no
   intervention from the user.
2. With some intervention (the user tells the GC to rebuild a data structure
   in a way that minimizes pointer size), it should be easy to get nearly
   all pointers in medium-fan-out trees down to 16 bits. (Most would drop
   to 8 bits with the alignment enhancement.) It would be expensive
   for the GC to rebuild something in this way, so this should only be used
   for very long lived data. (Excellent for read-only memory-mapped data).

Let's consider the revised representation:

    struct Node
      { values     : uint32_t[8]
      , base       : uint48_t
      , alignment  : uint8_t
      , N          : uint8_t
      , children   : uintN_t[] // Flexible array of up to 8 N-byte numbers
      }

For something with fan-out of 16, you'd be looking at a maximum size of 96
bytes for the children, plus the 8-byte overhead of base + alignment + N.
And then you've probably got 8 bytes that the GC needs for tagging and
layout or whatever. So you'd have 16 bytes left to play with (assuming you
want to always stay under 128 bytes).

* For a qp-trie node, that's fine. The only other fields you need are
  the two-byte bitset and a eight-byte position (storing a string larger than
  4GB in a trie seems ridiculous, so maybe just 4 bytes for the position,
  or maybe 6 bytes and merge it with the bitset).
  Those 96 bytes of pointers could easily drop down to 48 bytes of pointers
  after compression (less than 1GB heap, which is common, means 24 bits per
  pointer). So, for a full node, you'd be looking at 48 (children) +
  8 (layout/tag) + 8 (base/alignment/N) + 8 (position/bitset). So, 72 bytes,
  just over one cache line. And if you have 13 or fewer child nodes, you
  would be down to one cache line. Or if the pointers were compressed down
  to 2 bytes each, that would also bring you down to a single cache line.
* For a b tree node (all of the example `Node` structs), you're going to
  end up with higher overhead because of the keys, but it should still
  be easy to stay under 128 bytes per node.
