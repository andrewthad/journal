# A Different Kind of HTTP Proxy

Apache, nginx, and (to a lesser extent) haproxy are the most common HTTP
proxies that Linux users know and love. Apache and nginx are extremely
heavy though. They do a lot of stuff that I don't need (serving static
files, directory listings, matching paths with regex), and they are
difficult to configure. What I really want is just a proxy with features
like auth proxying and TLS termination. Certainly, haproxy is closer
to this goal. But there are other things I would like to see:

* Support for tapping requests based on path and headers.
* Possibility of using a simple API on the backend rather than having
  to fully implement HTTP. This means that the backend:
    * would not have to worried about chunked encoding
    * could avoid reparsing all of the headers (maybe this is a bad idea)
    * would not have to think about certain DOS attacks like slowloris
      (possible if the web server does not submit the request until
      the entire body has been received) 
    * could be a standard synchronous application
* More intelligent blocklist capabilities. Think fail2ban but smarter
  and more aware of HTTP. This would be integrated with auth proxying.

The idea of the backend being synchronous is not new. This is basically
how fastcgi works, and fastcgi has its own binary protocol for communication
between the application and the proxy. Fastcgi
[has been criticized](https://ef.gy/fastcgi-is-pointless) for adding a
needless layer of indirection that hinders application development:

> Using your own HTTP server has another advantage: it's easier to debug
> since you can just run it on a TCP/IP socket and connect to it with your
> browser. You can't do that in FastCGI - although you could run that
> directly on the command line. But then you'd have to set up tons of
> environment variables and hope the library you're using acts the same
> with regular CGI requests than with FastCGI requests and the bug isn't
> just in the latter.

And fastcgi doesn't even make it possible to avoid writing a multithreaded
application. From [FastCGI - The Forgotten Treasure](https://www.nongnu.org/fastcgi/):

> Some of you may wonder now why the message header contains a request ID.
> After all, if we have one connection per request, that type of information
> is redundant. That's right, only that there may be more than one request
> per connection! That is the reason why FastCGI uses a packet-oriented
> protocol rather than a single data stream: Multiple requests can be
> intermixed in one connection; software engineers will know that as
> "multiplexing". Because every packet contains a unique request ID,
> the server can deliver an arbitrary number of requests simultaneously - using
> only a single connection. Furthermore, the FastCGI application
> can receive data for an arbitrary number of connections, each of which
> can carry an arbitrary number of requests.

However, HTTP proxying [has been criticized](https://news.ycombinator.com/item?id=9202411)
for its inability to distinguish the original request from information
added by the proxy:

> Reverse proxying isn't the same. Information that's critical to some
> apps, like client certs or socket info, isn't available without hacking
> up custom headers and mixing that trusted data in with the untrusted
> client message. Due to HTTP's supremely obtuse parsing rules, this can be
> exploitable even when being defensive. I've personally seen this in
> SIP proxing (ganked the parsing from HTTP) leading to simply unparseable
> messages due to irreconcilable differences in deployed software.
>
> Yeah, FastCGI introduces a whole other attack surface, but it's on a
> trusted boundary, at least. Mixing trust levels within content seems
> like one of the primary classes of security problems. 

I find both of these criticisms (of different things) compelling. It would
be nice if there were a way to get the best of all worlds though. The best
of all worlds would mean that backend applications would:

1. be synchronous and single-threaded (with optional multithreading)
2. respond to `curl` and web browsers (for local debugging)
3. accept additional information (client certs, socket info) from the proxy

Only (3) is tricky, but a simple solution here might be to augment HTTP.
Typically, an HTTP request start with method, path, version, and then
a list of headers:

    GET /foo/bar HTTP/1.1
    Host: example.com
    User-Agent: curl/7.58.0
    ...

Typically, proxies tell the backend bonus information about the original
request by adding a header. What if instead, the proxy prefixed the entire
request:

    PROXY
    Client-Cert: ...
    Socket-Ip: 192.0.2.133
    GET /foo/bar HTTP/1.1
    Host: example.com
    User-Agent: curl/7.58.0
    ...

This optional header would never cause ambiguity because HTTP only supports
a few request methods. Additionally, the list of extra fields would be
binary encoded without any arcane escaping rules, and there would only
be a small supported number of them. Under this system, an application
could function as a normal web server that accepts legitimate HTTP requests,
but it could also accept these proxied http requests (PHTTP), which would
give it access to this bonus information.

Other closing thoughts:

* Linux has `splice` and `tee`, which are very cool. They could help reduce
  the number of copies performed.
* Talking to the backend over a pipe or a unix domain stream socket seems best.
  Datagram sockets and POSIX message queues are cool, but they seriously limit
  the size of payloads, and splicing does not work with them.
