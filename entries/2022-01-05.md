# Grab

Here are several totally unrelated ideas.

* Compression for JSON. I've done this in a project named `juice`. I just
  have not actually used it anywhere. In `juice`, strings are deduplicated,
  which means that a deserializer in a high-level language with immutable
  strings can just share the one backing string (effectively preserving the
  compression during deserialization) rather than having a zillion
  copies of it. It is possible to take this idea further and support
  deduplication of objects and arrays as well. There is less benefit for
  numbers and not benefit for `true`, `false`, and `null`.
* Preservable byte array compression (or any kind of array). We have a ton
  of different ways to compress data, but typically people decompress data
  back to an array before operating on it. But if we instead decompressed
  to some kind of chunked format, we could preserve the compression. The
  cost of doing this is that nearly operations on the data would take a
  greater number of machine instructions.
* A better JSON that supports a binary format. A bunch of people have tried
  to do this, and none of these new interchange formats have really caught
  on. We need more types. JSON has six, which is not enough. There are lots
  of types that have meaning across industries: IP address, MAC address,
  RGB color, country (or "geopolitical unit of interest"), timestamp (arbitrary
  precision), etc. JSON forces users to represent all of these as text.
  A binary format could represent all of these more concisely. More specific
  types make it possible to choose a better serialization scheme. For example,
  integers are common encoded with LEB128. However, LEB128 is terrible for IPv4
  addresses and MAC addresses because they are not distributed logarithmically.
  The other half of the problem is figuring out what to do about schema. You
  cannot require the schema to be shared out of band. Protobuf does this, and
  it's never going to take off. Maybe it's fine if one party controls all the
  endpoints. The coordination effort for multiple parties is just too great.
  If someone changes something about the data they are publishing and they
  forget to document the change (or maybe the whole thing was never documented
  to begin with), it needs to be possible to look at the data and figure out
  what it means. JSON lets you do this. Protobuf does not. In a binary format,
  object keys need to be listed at the beginning of a file and then backreferenced
  throughout the document. Additionally, the "stream" format
  (what newline-delimited JSON is for JSON) needs to support
  (and possibly require) declaration of keys at the beginning of the stream.
  This means that tiny messages that are not part of a stream and have no
  arrays of objects and only use each key once pay a significant penalty.
  I think that this is an acceptable price to pay.
