# Apache Arrow

## The Good

I've been working with the Apache Arrow data format recently, and on the whole,
I like what's there. In particular, I think it gets streaming exactly right.
That's the thing about it that wasn't clear to me at first, and as I've learned
more about it, I've realized that it is an important characteristic. Originally,
I thought that Apache Arrow was just a columnar representation of data. It is,
but it is slightly more than that. It is a stream of chunks, where each chunk
is columnar data. The stream must begin with the schema, and then after that,
the schema cannot be changed. And then next comes a possibly unlimited of
chunks, and then finally, an optional trailer, used for random access. So,
in addition to the possibility of memory mapping a file with Apache Arrow data
(for random access), there is also the possibility of working through it as a
stream from beginning to end. Or perhaps there is no known end, and you can
just start from the front and go forever.

## The Uncertain 

I've not worked with dictionaries yet, so I don't know how well they work. One
thing that's interesting is that there is an `isDelta` flag. This gives you
control over whether a dictionary should replace or be concatenated with the
previous dictionary (where the dictionary ID matches). Concatenation seems
great if you are processing the data as a stream. I am concerned about the
interaction between dictionary concentation and random access. It seems like
you would have to work your way through the file (in a streaming fashion)
to build the current dictionary. It would be difficult to memory map the
dictionary components since they need to be layed out contiguously. Memory
maps have stringent 4KB alignment requirments.

# The Ugly

This is everything that I have found unsatisfactory:

* Apache Arrow uses flatbuffers to store metadata. This is a strange choice.
  Flatbuffers is a middle ground between Cap'n Proto and Protobuf. It offers
  the user Cap'n Proto's "zero-copy" feature (in theory, you can memory map
  a flatbuffer) and Protobuf's extensibility. To use the zero-copy feature,
  you have to use non-native objects with accessors produced produces by
  code generation. Apache Arrow does not need the metadata to have high
  performance though. It would make much more sense to just use protobuf
  and then marshal the metadata to whatever the native objects are for a
  language (e.g. POJO). Tellingly, the sister project Apache Arrow Flight
  uses Protobuf instead of flatbuffers.
* This is more of a marketing problem than a technical problem, but the
  name Arrow is used loosely but in Apache Arrow's own documentation
  and in projects that work with it. There is an in-memory format and an
  on-disk format (Arrow IPC) that are either identical or nearly identical.
  The on-disk format supports LZ4 compression of blocks, and the in-memory
  explicitly does not. But if you were working with Arrow IPC that used
  compression, then you would have to make your in-memory Arrow tools aware
  of compressed blocks. Either that or you would have to just decompress
  all of the blocks (including the ones that you don't need!). Datafusion
  is an analytics tool that uses Arrow internally, but it takes a while
  to realize that it does not actually let you load data from Arrow IPC
  files.
* Compression. The spec claims to only allow lz4 and zstd, but pyarrow
  supports several others. Lots of blocks will have integral data, so
  It would be nice if Arrow supported compression schemes that were
  specifically designed for integers. Daniel Lemire has published
  papers that analyze several integer-compression schemes.
* Dearth of logical types. Apache Arrow rightfully draws a distinction between
  logical types and physical layout. There are a small number of physical
  layouts and a somewhat larger number of logical types. Among the logical
  types, we find the usual suspects: `Int` (8-bit,16-bit,etc.), `FloatingPoint`,
  `Bool`, `Utf8`, `Binary`. (Note that `Utf8` and `Binary` use the same physical
  layout.) There are a few oddball pairs that show up because some users needed
  lists and strings with more than 4B elements: `Binary` and `LargeBinary`,
  `Utf8` and `LargeUtf8`, `List` and `LargeList`. And then there are the more
  specialized ones: `Time`, `Timestamp`, `Interval`, `Duration`. These let
  you configure the timezone and unit of precision (seconds,milliseconds,etc.).
  Someone was thinking hard about timestamped data when they wrote this, which is
  good. Unfortunately, it's only timestamps that get this special care.
  Nothing for IPv4 addresses, IPv6 addresses, MAC addresses, UUIDs, geographic data
  (e.g. lat-lon). Despite the thought put behind time-related units, we have
  nothing for other units (height, mass, joules). All of this other stuff is
  relegated to "Extension Types". Extension mechanisms for data serialization
  formats never get used, so none of these types of data are ever going to
  be rendered correctly by general-purpose tools. Notably, all of the time-related
  logical types could have been implemented as extension types instead.
  But they were not, so now they are their own special class of types, and
  everything else is left behind. As of today, here's what's written in the Arrow
  documentation as the official list of canonical extension types:
  "No canonical extension types have been standardized yet."
