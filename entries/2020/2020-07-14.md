# Possible Optimizations for Hash Array Mapped Tries

I have not done any of these things yet. They are just ideas. In
`unordered-containers`, hash array mapped tries are implemented as:

    data HashMap k v
      = Empty
      | BitmapIndexed !Bitmap !(Array (HashMap k v))
      | Leaf !Hash !(Leaf k v)
      | Full !(Array (HashMap k v)) -- always length 16 
      | Collision !Hash !(Array (Leaf k v))

Notice that `Leaf`, `Collision`, and `Empty` are leaf nodes.
One easy optimization in a language that supports fixed-length arrays
would be to replace `Full` with:

    Full 16x(HashMap k v)

That is almost guarantees to be a win, since it removes an indirection.
What else can we do? It would be cool to be able to replace the array
in `BitmapIndexed` with something that could be unpacked. There are
two options here:

* Unpacked variable-length array. No GCed languages appear to support
  these. They must be very difficult to generate code for.
* Fixed-length 16-element array. This is easy to unpack, but it could
  waste a lot of space, especially since nodes near the leafs would
  be sparse.

There is also the possibility to compromise. What about:

    data HashMap k v
      = Indexed 4xW4 4x(HashMap k v)
      | Full 16x(HashMap k v)
      | ... as before

Here, we avoid the full 16-child allocation for `Indexed`. We waste
some space but not as much as before. Once you reach five children,
you would have to switch to `Full`.

Another possibility is optimizing `Leaf`. `Leaf` does not make good
use of the heap. It only has two fields. What if, instead, multiple
leaves were unpacked into something else? That is, if `Indexed` only
had `Leaf` children, then you could store all the leaf data contiguously.
One difficulty with this optimization is that HAMTs are not balanced.
If everything were balanced (like in a B tree), the opportunity to
unpack leaves would show up constantly. This would roughly half the
number of pointers in the data structure (I think).

What if, instead, you shove everything into a B Tree, using the hashes
as the keys, and turn on prefix compression? HAMT relies on the randomness
of hashes to keep itself mostly balanced, but B Trees provide a balance
guarantee, making it possible to unpack leaves. One nice thing about
HAMTs is that at a full branch node, you can just index into an array
to find the next node. But with SIMD, it is possible to scan 16xW4
values in a single instruction, so maybe HAMT's advantage does not matter.

I realized after thinking about this more that B trees would not
do a good job compressing keys.

One more optimization idea. What if a HAMT started out with high
fanout (16) at the root but then dropped down to something lower (4)
as you got closer to the leaves. You want higher fanout when there
are lots of children, and you want low fanout when there are not.
This would look like:

    data HashMap k v
      = Small 4x(HashMap k v) // only consume two bits of hash
      | Big 16x(HashMap k v)
      | ...

I wonder if we could just do away with all of the optimizations if we
went two bits at a time instead of four:

    data Leaf k v = Leaf k v // keys are likely pointers
    data HashMap k v
      = Empty // 0 byte payload
      | Full 4x(HashMap k v) // 16 byte payload
      | Leaf !Hash !(Leaf k v) // 12 byte payload, probably
      | Collision !Hash !(List Leaf k v)) // 8 byte payload

With lower fanout, `Full` (assuming 32-bit pointers) only takes 16 bytes
of memory (plus possibly some kind of tag for the data constructor). One
might complain that this doubles the number of indirections, but we already
have to pay an extra indirection for a dynamically sized array in every
GCed language, so we are actually no worse off than we started. This
approach is amenable to a mutable implementation since it never discards
allocated constructors except for when collisions happen. Technically, we
could consolidate `Leaf` and `Collision` into something like
`Leaf !Hash !(Leaf k v) !(List (Leaf k v))`, although this might waste
space in the common case of no collisions. We would jump from a 12-byte
payload to a 16-byte payload. Consolidation does seem appealing though
since it makes the whole data structure more straightforward.

The nice thing about reducing the fanout to 4 is that, in the worst
case, `Full` only wastes 12 bytes. I do not know whether or not this
waste would be common.
