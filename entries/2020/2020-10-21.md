# Applications of Small Hash Maps

Hash maps are useful for key-value collections where:

* The keys are variable-length byte sequences
* The order of the keys does not matter

There are other circumstances under which hash map are useful (e.g. keys
that are not strings), but I am not going to focus on those here. The case
I want to focus on is where:

* There are a small number (less than 256, possibly much less)
  of key-value pairs.
* The data is built once, never updated, but probed sparingly.
  Most keys are not looked up more than once. Some are never
  looked up.

I will motivate this with two specific examples: parsing JSON objects and
parsing HTTP headers. The data structure I suggest is this:

    data Map a =
      { hashes : Array 8xW32 // SIMD, elements 32-bit words, proper alignment, length m
      , objects : Array (Bytes,a) // Array of structures, length n
      }

The number 8 is chosen for machines with 256-bit vector registers. You
would want to pick 16 on a machine with 512-bit vector registers. The
critical invariant is that `⌈n / 8⌉ = m` (swapping out 8 for 16 on machines
with 512-bit registers). The `hashes` array has some garbage values at
the end. These 32-bit numbers are set to some tombstone constant.

Insertion does not probe. It just pushes the hash and the key-value pair
onto the ends of the appropriate array builders. It could be made to check
to see if the key already exists. It depends on whether you want a multimap
or not. Notice that this means that the original order is actually preserved,
which can sometimes be useful even though it was not considered a requirement
above. Also, notice that this effectively has a load factor of 100%.
Once frozen, the map can be probed by simply scanning `hashes`.
It is not possible to stop on the first match. The scan must go all the
way to the end. If there is only a single match, then the corresponding
index is returned. This can be looked up in `objects` and then the keys
can be compared. If multiple hashes matched, then we must endure the
slow path. You have to walk `hashes` an extra time, element by element,
and check `objects` every time you see a match. The tombstone constant
probably means that there is some key guaranteed to exhibit this worst-case
behavior every time it is searched for. It is possible to instead reserve
a bit for this purpose. Anyway, that part needs a little more thought.
Let's consider the applications:

## HTTP Headers

Most web server proxies cap requests at somewhere around 100 headers by
default. There are also restrictions on the size of a request header.
Size restrictions range from 4KB to 16KB. So, because of common industry
practices, the likelihood that you ever see a request with a thousand
headers is low.

Some HTTP headers (like `Date`) are typically ignored. Others (`Host`)
are almost always examined. Still, others (`Content-Type`) are a bit of
a grab bag, with some applications ignoring them and others inspecting
them. The tricky thing about HTTP headers is that they are not case
sensitive. Fortunately, they also use a restricted character set. From
[RFC 2616](https://www.ietf.org/rfc/rfc2616.txt):

    token      = 1*<any CHAR except CTLs or separators>
    separators = "(" | ")" | "<" | ">" | "@"
               | "," | ";" | ":" | "\" | <">
               | "/" | "[" | "]" | "?" | "="
               | "{" | "}" | SP | HT

The notion of equality needs to overridden to support this. Rather than
`memcmp`, we need to do some case-insentive byte-by-byte walk. We
want to preserve the original key in the data structure since this
makes it possible to slice (better sharing). Similarly, hashing
needs to be tweaked to normalize characters while hashing the string.
Not difficult. This, by the way, really lends itself to ML-style
modules for creating variants of map. Even though the key is always
the same, the notion of equality and the approach to hashing both
need to be pluggable.

## JSON Object Keys

This is easier than HTTP header name because JSON object keys are case
sensitive. One difference in access patterns is that with JSON, it is
more likely that all of the lookup are right next to each other. For
example, from the `aeson` documentation:

    instance FromJSON Person where
      parseJSON = withObject "Person" $ \v -> Person
        <$> v .: "name"
        <*> v .: "age"

This can easily be 10 or 20 fields when dealing with larger objects.
It matters because, for a sufficiently large number of lookups, it
may make sense to query differently. Normally, each lookup is a scan
(eight 32-bit hashes at a time). But, if we are looking for a lot of keys,
it might make sense to go one 32-bit hash at a time and compare it against
all the hashes we are looking for. A compiler can build a perfect hash
table for and implement it as a jump table for this kind of thing. But
I don't know at what point it becomes better to query that way. Also,
maybe it's possible to vectorize those kinds of tests (test against many
values), but I don't think so. Regardless, to really support this well,
some kind of metaprogramming will be needed so that the user does not
have to do it on their own. Something like:

    parsePerson = withObject "Person" $ \v -> do
      r <- $(manyFields v {name:"name",age:"age"})
      Person
        <$> parseJSON r.name
        <*> parseJSON r.age

In the example above, all fields in `r` are JSON values. Depending on
the number of fields requested, `manyFields` would generate different
code: either consecutive linear scans or one big scan. Surely, it's
possible to make this even more terse with metaprogramming, but I won't
explore this any further here.

## Upgrading

One last thought is that, in the event that the humble linear-scan hash
map is not up to a task (tons of probing, JSON objects known to be massive),
it is always possible to upgrade to a hash map with better query performance.
This upgrade is inexpensive because:

* All hashes are already calculated. No need to peek at the key payloads.
* The full number of entries in the table is known. No resizing will be
  needed while building it.

Of course, the user would have to be aware that they needed to do this.
It wouldn't just magically change into a different data structure.
