# A Difficulty with Explicit Uncurrying

Explicit uncurrying means writing `uncurried` instead of `curried`:

    uncurried :: (Int,Int) -> Int
    curried :: Int -> Int -> Int

I suspect that uncurrying makes code generation easier and a little
more predictable. Here's a puzzle: How does requiring uncurrying play
with `newtype`ing functions? For example:

    newtype Parser e a = Parser ((ByteArray,Int,Int) -> Result e (ByteArray,Int,Int,a))
    tokens :: List Token -> Parser MyError (List Token)
    tokens = \xs -> ...

Now, we're in a bind. We want the generated code to be a function that
accepts everything, both the token list pointer and the three internal
parser arguments, together on the stack. It is not clear how to make
this happen. Here is a nasty workaround:

    newtype Parser r e a = Parser ((r,(ByteArray,Int,Int)) -> Result e (ByteArray,Int,Int,a))
    tokens :: Parser (List Token) MyError (List Token)
    tokens = do
      xs <- argument
      ...

Maybe with a little syntactic sugar, this would not be so bad: 

    functype Parser e a = Parser (ByteArray,Int,Int -> Result e (ByteArray,Int,Int,a))
    tokens :: List Token ->- Parser MyError (List Token)

How would this impact a `Monad` typeclass? It's pretty bad because
only some monads are functions. For example, Haskell's `Maybe` and
`Either` are just plain data, but `State` and parsers are functions.
Let's say that we just gave up on caring about the ones that are
just data:

    Foo : Data
    Int -> Bool : Function
    functype Parser e r a = Parser ((r,(ByteArray,Int,Int)) -> Result e (ByteArray,Int,Int,a))
    Parser E R A : Function
    class Monad (m : Data -> Data -> Function) where
      pure : m a a
      bind : m e a -> m a b -> m e b
      (alias bind to >=>)

This monad is just category (the Kleisli thing). It isn't clear how this
would be used. Let's try:

    twoFooOnto : Parser MyError (List Foo) (List Foo)
    twoFooOnto = arg xs ->
      parseFoo >>= \x ->
      parseFoo >>= \y ->
      pure (Cons x (Cons y xs))

That doesn't make any sense at all. Let's try again:
    
    functype Parser e a = Parser ((ByteArray,Int,Int) -> Result e (ByteArray,Int,Int,a))
    Parser E A : Function
    (+) : Data -> Function -> Function  // augments argument
    class Monad (m : Data -> Function) where
      pure : a + m a
      bind : m a -> (a + m b) -> m b
    twoFooOnto : List Foo + Parser MyError (List Foo)
    twoFooOnto = \xs ->
      parseFoo >>= \x ->
      parseFoo >>= \y ->
      pure (Cons x (Cons y xs))

This requires some amount of magic to make lambas work. I don't like it. Also,
code generation of `bind` is unclear. For example, we want for `bind` to not
do any inlining when we write:

    bar : Parser MyError MyData
    bar = one >=> two

How can we actually get this guarantee? It's tricky. The difficulty is that,
usually, we want HOF to just inline. But here, we want bind to inline until
it reaches a point where known functions are being applied (unknown ones
are bad). So if bind *only* accepts known functions, which might be a
possibility, then there is a straightforward way to inline it. Let's try
to make this more explicit with types:

    bind :
         ((ByteArray,Int,Int) -> Result MyError (ByteArray,Int,Int,a))
      => ((a,(ByteArray,Int,Int)) -> Result MyError (ByteArray,Int,Int,b))
      => (ByteArray,Int,Int)
      -> Result MyError (ByteArray,Int,Int,a)

The first two arguments to `bind` are of kind `Function`. The third is of
kind `Data`. (Functions only get one `Data` argument, and it must come last.)
Since `bind` itself takes `Function` arguments, it must always inline.
We need a mechanism for preventing a `Function` from depending on `Data`.
I think that an answer might be to restrict all functions to this form:

    Function => Function => ... => Function => Data -> Data

I need to look into this more, but it seems like it would work. Now, let's
get back to the question of how to get a monadic interface to parsers
working. We want our `do` notation to expose the extra argument in the
tuple to the user while implicitly threading the other part of the result
to the next computation. This is more complicated than Haskell's `do`
notation. It also feels very state-centric. What about short-circuiting
(or even nondeterminism)? Consider this:

    example : M Bool
    example = do
      c <- nextChar
      case c of
        'T' -> do
          demand "rue"
          pure True
        'F' -> do
          demand "alse"
          pure False
        _ -> fail

I feel like I'm moving closer to Paterson's arrows than to monads, but
let's try specifying a desugaring. The desugaring should be:

    example : M Bool
    example {st0} = do
      let (st1,c) = nextChar st0 in
      case c of
        'T' ->
          let (st2,()) = demand ("rue",st1) in
          pure True st2
        'F' -> do
          let (st2,()) = demand ("alse",st1) in
          pure True st2
        _ -> fail st1

So, the idea is that we must identify any possible monadic computations
that follow `nextChar` and feed its state into them. This isn't really
a general handling of monads though. It's just state. That is,
this scheme does not handle short-circuiting. Bummer. But it does have
nice guarantees about the generated code, which is nice. If `M` had
been something that could fail, we would have instead wanted:

    example : M Bool
    example {st0} = do
      let r1 = nextChar st0 in
      case r1 of
        Failure -> Failure
        Success (st1,c) -> case c of
          'T' ->
            let r2 = demand ("rue",st1) in
            case r2 of
              Failure -> Failure
              Success (st2,()) -> pure True st2
          'F' -> do
            let r2 = demand ("alse",st1) in
            case r2 of
              Failure -> Failure
              Success (st2,()) -> pure True st2
          _ -> fail st1

Again, state gets pushed down, but now we also case to check for failure.
And again, this is very different from how Haskell desugars `do` notation.
Haskell uses a very simple and straightforward desugaring. It uses
higher-order functions to accomplish this, which means that GHC optimize
things to get good performance out of the code. What I've suggested
above is much more like a macro system, a sophisticated one that can
look through case statements to find the next use of something monadic.
This probably means that single-statement monadic expression would
require `do`. The desugaring would need to happen before typechecking.

This is a lot of complication, so let's go back and spell out what we get
in exchange for all of this. The original problem was that functions like
`Int -> Parser MyError Foo` cannot be written in a system that mandates
uncurrying. However, the monadic interface to computation requires such
expressions. And the monadic interface to computation is extremely useful.
Haskell is evidence of this, and several other languages imitate this
interface (without using the word "monad"). So, the idea was to recover
this interface without compromise on uncurrying. The idea I arrived at
was a macro system (probably a built-in one) that gives monadic computation
a very special place in the language itself.

The one unsolved problem is what to do about that extra type argument.
It would be nice to write something like this:

    myParser : Int ->> Parser MyError Foo

But we cannot, so I think that just shoving the `Int` into the parser
types itself is best:

    myParser : Parser MyError Int Foo

And now `myParser` has an interface that is more categorical than
monadic. So what we might need is actually something more like this:

    typedef Parser e r a = Parser ...
      with kleisli

    typedef Optional a = Some a | None
      with monad

And then there would be a `kleisli` `do` notation and a `monad` one. They
would differ ever so slightly, particularly in their treatment of an
argument to a function. For example:

    myParser : Parser MyError Int Foo
    myParser = kdo i   // brings the argument into scope
      { x <- charAbove i
      ; ...
      }

But:

    transform : This -> Maybe That
    transform = \e -> mdo
      { ...
      ; ...
      }

That's the one syntactic difference that jumps out to me. So the big question
is "Is this worth it?" I wish I knew. There is a lot about Haskell that I
enjoy, but the biggest frustration is that it does not offer a good model
for predicting what kind of code the compiler will generate. GHC does an
excellent job applying certain optimizations, optimizations that a sufficiently
skilled user can often predict. But the rules take a lot of time to become
familiar with, and the code you want does not always get generated. If
everything is uncurried and partial function application is banned, we can
get one more guarantee about the machine code a compiler generates. We
know that function application is the usual thing, pushing stuff into
registers and onto the stack via a calling convention, never building
function closures or PAP closures. It would be a nice guarantee to have.
