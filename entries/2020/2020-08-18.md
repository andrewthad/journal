# Card Tables for GC-Managed Slicing

In the entry on 2020-08-10, I wrote down some thoughts about how
a copying collector could be tweaked to slice into arrays when
appropriate. The earlier discussion only covered the possibility
of discarding the unneeded edges of an array, but what about
situations where a large area in the middle of an array is unused?
We could instead use something like GHC's card tables.

GHC's card tables use one byte for each 128-element chunk, and
they must waste 7 bits in every byte. They are for a very different
purpose. We may be able to do much better. Let's say we have
64 bits available for a card table. This card table will track
(conservatively) which bytes are referenced by slices. If every
bit refers to a 32-byte chunk, then 64 bits would cover 2KB. Not bad,
but also not sufficient for large arrays. What if instead the
granularity was determined by the size of the backing array.
A 512-byte array would get 8-byte granularity, and a 512KB array
would get 8KB granularity. This way, we would always use exactly 64
bits for the card table, which is convenient because fixed-length
is always a little bit easier to deal with than variable length.
Let's say that you did have a 512KB array and sliced into it so
that only a few bytes at the beginning and the end were preserved.
The first time the GC copied it, it would knock out the middle
sections. That is, 62 of the 64 chunks would not be copied, and
the one heap object would become two heap objects. A later collection
would have finer granularity and would shave off even more of the
unneeded bytes.

Since every heap object carries some amount of overhead, we actually
don't want to take all opportunities to split objects. There needs
to be at least 16 (maybe 32) unused bytes between slices before it
becomes worthwhile to break them apart. The natural exception to
this rule (or extension of it, depending on how you look at it) is
that it is always worthwhile to eliminate unused bytes and the beginning
and end. Consider a small example:

    0 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0
      ---------------         ---

In the above example, every bit refers to an 8-byte chunk. There is one
chunk that is preserved even though it is not needed. The run of four
unused chunks is not preserved. The ends are also not preserved.

Does this actually work though? The forward-pointer business gets messed
up badly because now a single heap object may need multiple forwarding
pointers, and which forwarding pointer you want depends on your slice.
However, it may be possible to put the forwarding pointers in the payload
of the byte array when we evacuate it. At smaller sizes, there is an
issue of even having enough space for all the forwarding pointers. Let's
say that each one takes 8 bytes. The greatest possible number of forwarding
pointers is 64. This would happen if all chunks were split. (Technically,
that cannot happen since adjacent preserved chunks remain coalesced, but
whatever, better to be conservative here.) And 64 * 8 = 512, so a 512-byte
array has enough space to handle the greatest possible number of forwarding
pointers. What do we do when the payload is smaller? For example, a 32-byte
payload could only hold 4 forwarding pointers. This is fine though because
a 32-byte array could not be split into more than 4 arrays. The invariants
on the minimum size of a chunk enforce this. Any minimum greater than or
equal to 8 bytes would work.

There is still a difficulty. How do we figure out which forwarding pointer
to use? If all arrays were large (over 512 bytes), it would be easy. Just
write the new pointer at the index of each chunk. In smaller arrays, there
is not enough space to do this.
