# Laziness

I keep oscillating on how I feel about laziness in functional programming.
This disadvantages are:

* Pervasive laziness (a la Haskell) leads to building lots of thunks
  in situations where deferring computation harms performance. If the
  thunk is guaranteed to be forced at least once, then building a thunk
  in the first place was the wrong choice. Maybe there is a way to use
  static analysis to weed out more of these, but I am skeptical.
* Any laziness at all complicates generational garbage collection since
  it makes it possible for an old generation to point to a new generation.
  One possibility would be to immidiately promote the result to the older
  generation or to simply allocate directly into the older generation.
  This is possible since the thunk in the older generation could not possibly
  reference the younger generation. But during the evaluation of a thunk,
  other thunk could get evaluated, and this all starts to get messy. I need
  to think about this more.

The advantages are:

* Lots of dynamic programming algorithms have elegant, purely functional
  implementations when laziness is available. (Sometimes infinite data
  structures are needed as well.)
* Not having `&&` and `||` be built-ins is nice. Similarly, Haskell's
  `Data.Bool.bool` eliminator is useful.
* Haskell's `let` is convenient. Deferring computation *within an expression*
  typically means that a thunk is not built. Rather, the computation is
  just floated inward. I like this behavior. Strict-by-default languages
  need a different keyword with different semantics.

The authors of [Kinds are Calling Conventions](https://www.microsoft.com/en-us/research/uploads/prod/2020/03/arity-poly.pdf),
propose a kind system that allows both lazy and strict types. This system
does not involve any gross hacks. It does not solve the garbage collection
issue. Still, let's consider what some functions might look like. Here,
I will use `!` to mean strict, `+` to mean lazy, and `_u` to mean variable
strictness:

    bool : a_u -> a_u -> Bool! -> a_u
    (&&) : Bool! -> Bool+ -> Bool!    // this one?
    (&&) : Bool! -> Bool_u -> Bool_u  // or this one?
    (||) : Bool! -> Bool_u -> Bool_u  // same idea
    (>) : Int! -> Int! -> Bool!
    decode : ByteArray! -> Xml!
    insert : Key! -> Value! -> Map! -> Map!

Note that it makes sense for most functions would be strict in all of their
arguments and in their return value. Only when we start talking about
`let`-bound expressions and general boolean combinators does laziness become
a reasonable choice. There is a weird covariant-contravariant thing going on
here. A strict argument can always be given as an argument to a function that
requests a lazy argument. Conversely, a function that returns a strict value
could always return a lazy value instead. I suspect that monomorphic sigatures
would be better in many cases though. What should right fold look like?

    foldr : (a! -> b_u -> b_u) -> b_u -> List! a! -> b_u
    foldr f b Nil = b
    foldr f b (Cons x xs) = f x (foldr f b xs)

This signature disallows a lazily-constructed list, but it allows you to use
the accumulator for boolean shortcircuiting. It is both the strict right fold
and the lazy right fold, depending on how `u` is instantiated. When lazy in
the accumulator, this is good for things like builders and for boolean
reduction. Notice though that with boolean reduction, it might often be
desirable to make the result strict. For example:

    hasNonZero : List! Int! -> Bool!
    hasNonZero xs = force (foldr (\x b -> (x > 0) || b) False xs)

Something strange is going on in `hasNonZero`. Should the result be lazy
or strict? If the function were defined with explicit recursion, it would
naturally be strict, but with `foldr`, it ends up lazy. I do not have a
good explanation of what is going on, but I find this unsettling.

What does it mean for a function itself to be lazy or strict? Online, it
is possible to find examples of a monadic `Parser`s where the strictness
of the language affects how well defined certain combinators are. If
function type are simply of a different kind than data, maybe this
question would just go away. That is, functions could be considered
neither strict nor lazy because they are not passed around at runtime.

What about the values inside of data constructors? In most cases, we want
these to be strict. Why? Coming at it from a very operational point of
view, data constructors tend to imply that something is getting written
to the heap. We do not want to build closures for lazy values unless we
really really need to. There may be a way to implement some amount of
laziness where thunks are never built, and lazy values imply some amount
of inlining. This requires whole program optimization. However, this
really only works if lazy values do not get put into data constructors.
Let us consider how a strict data constructor would impact things:

    data List a = Cons a (List a) | Nil   // strict in everything
    
    myCalculation : Foo! -> Int!
    myCalculation y = ...
    
    foo : Foo! -> List! Int! -> List! Int!
    foo x numbers =
      let onFalse, onTrue : List+ Int!
          onFalse = Cons 42 numbers
          onTrue = Cons 12 (Cons (myCalculation x) numbers)
          // Weird forcing again just like in hasNonZero
       in force (bool onFalse onTrue (predicate x))

Both `onFalse` and `onTrue` are lazy, so when `bool` is inlined, everything
goes into the right place, and no thunks are needed. The expensive
`myCalculation` is not performed unless it is needed. Something is wrong
here though. Simply floating things up means that you would need to use
`force` later down in the function. For example, if `myCalculation` were
floated up into a separate let binding, we would need to call `force` on
it when feeding it into `Cons`. I do not like that. Maybe there is a way
to make this feel more natural.

What if everything were lazy but data constructors were strict? Would it
be possible to implement non-strict semantics *without* thunks? That would
be pretty cool. The big problem with this is that it makes compilation
difficult. If a function is strict in all of its arguments, we really just
want to have code generated for that function rather than inlining it all
the time. It would be important to have a way to get a guarantee about this
from the type system. And explicit annotations are one such way.

Something to keep in mind is that explicit annotations always have a cost.
Once users are allowed to make a choice, they are forced to make that choice.
This can be offputting and paralyzing.

To summarize, the main question here is: Is it possible to offer non-strict
semantics without ever creating thunks? It might be, but it would require:

* Guaranteed inlining of functions with lazy arguments
* No lazy fields in data constructors
* Tuples might need to be a magic (non-heap-allocated) data type that
  support per-field strictness annotations. This would be important if the
  language disallowed curried functions.
