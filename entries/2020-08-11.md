# Collecting Nurseries Concurrently

See the entry from 2020-07-30.

In a general setting with unrestricted mutation, Concurrent GC is a devil.
The trick is that you really want to prevent "old points to new" from
happening, and mutation makes this invariant impossible to uphold. If
everything is immutable (no laziness, no mutable references), things might
be easier. In a single-threaded program, you could just use a generational
variant of Cheney's algorithm. As you collect the nursery, if you encounter
something in an older generation, you do not need to follow it.

With threads, things are more difficult. It would be nice for every
thread to have its own nursery (possibly its own heap). If threads
have their own nurseries, then they can collect them without any
synchronization. But what about when a thread spawns another thread?
There are several ways this can happen (GHC-style sparks or more
traditional lightweight threads). From the entry on 2020-07-30:

    A -> B -> C -> D
         ^
    X ---+

It's a little difficult, but I think that the "old points to new"
can be prevented. We need to track the following information for each
thread:

* Whose heaps am I pointing to?
* Which of my their generations am I pointing at?

If every heap has generations 0, 1, and 2, then it might be the case
that A (any generation, it doesn't matter which one) points to B0. If
that is the case, then any collection of A requires a collection of B0.
That is, we must wait perform a Gen-0 collection on B while we are
collecting A. And if X points to B0, we also have to do X at the same
time as well. This may seem onerous, but keep in mind that after this
collection, A (and possibly X) will no longer point to B0. They cannot
ever point to B0 again.

We do not want any A-to-B0 pointers because if these exist, B cannot
collect its own nursery without synchronization. However, once A no
longer points to B0, we are in a better place. Also, note that
dependencies must always be computed transitively. If `A -> B0` and
`B -> C1`, then `A -> C1`.

How would this play out in the two common situations where threads
are used:

* GHC-style sparks: Nicely. Let's say that thread T spawns workers
  to parallelize operations over an array. Every thread gets a single
  element, or the elements are loaded into a queue and the worker
  threads pull them off. All worker threads will need access to some
  environment in T's heap (or in the heap of something that T's heap
  refers to). T will probably contribute as a worker thread. At some
  point, all the workers will need to synchronize (along with T) for
  their first collection, but after that, they will point to T1, not
  T0. And then nursery GC will no longer require synchronization.
* Server threads: Less nicely, but still pretty good. In this scenario,
  one thread receives connections and then spawns a worker for each
  connection. There is probably some kind of shared environment. It
  is possible for hundreds of threads to require synchronization, but
  this should not happen often.

What inter-thread communication?

* GHC-style sparks: All communication happens at the beginning and at
  the end. At the beginning, arguments are passed into the spark. This
  is a small amount of data. At the end, results must somehow be handed
  back to the calling thread. This is tricky. There are two options:
  trace-and-copy or page-gifting. Trace-and-copy is a little weird
  because we cannot copy anything back into the parent thread. Gifting
  (possibly combined with tracing) probably makes more sense.
* Server threads: All communication happens at the beginning. There
  usually is not any kind of return value. This is easy.
