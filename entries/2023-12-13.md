# Questions About Practical Static Memory Management

I posted this on GitHub, but I thought it would be nice to track it here as well. 

First, thanks for doing all of this. I'm not sure that I would have been able to
understand the original ASAP paper without your thesis, and I think it's a
fascinating space to explore for automatic memory management.

I have two questions about the paper. The first is the claim, in section 3.4,
that "wild paths sets are, in general, not closed under + ... the + operator
no longer serves to compute the meet of two paths". Are wild path sets not closed
under + because a wild path has no defined return type? That is, if we took the
wild path of a type Person { age : s64, name : string }, we have:

    (Person . age) + (Person . name)
    ==>
    Person . (age + name)

Which doesn't really make sense (but I didn't even need to take the meet of paths
to create the situation). I'm just not totally certain what the claim in the paper
means or what an example of it looks like.

The other question is about the super-linear slowdown in `list_len.mmtn`.
The paper doesn't discuss why this happens. Based on my understanding of ASAP,
I would expect that this could be caused by the tail of the list being traced
over and over again before each recursive call to `list_len`. That would degrade
performance from O(n) to O(n^2). But it doesn't seem like ASAP should cause this
to happen. I wondered if you had any additional insight into what was going on.

# Assuming Responsibility for Heap Objects

Here are some common memory allocation patterns:

    function foo(...) => Baz {
      ...
      x = new MyObject(...)
      ... // A. use x several times
      ... // B. no more uses of x, not even through other objects
    }

This is straightforward. Once we are in position B, we may deallocate x.
In fact, x could even be allocated on the stack. One way of saying this is
that x "does not escape". Here is another pattern:

    function foo(...) => Baz {
      x = new MyObject(...)
      y = new MyObject(...)
      z = new MyPair(x,y)
      ... // Possibly use x, y, and z any number of times
      return z
    }

This is the opposite situation and it is, again, straightforward. Here, all
three heap objects are certain to escape the function. At runtime, there is
no need to check for liveliness at the end of this function. All of these
are live.

There are several simple variants of these. Control flow can cause an object
to have different lifetimes on different branches. Maybe it is certain to
escape in one branch but not the other. We cannot stack allocate it in this
case, but we can perform prompt deallocation on one branch and no deallocation
at all on the other branch.

Now, suppose that we have built up some large object that consists of many
other objects, possibly even involving aliasing (the same subobject appearing
more than once). Immidiately after the object's creation, we may say that
the entire object is live. But once we start projecting fields from it,
this might no longer be true. How might we access a subobject?

* Projection
* Passing the superobject to a function that projects recursively

Now the problem starts to look more difficult. What is the most simple
solution? After each instruction (for some vague definition of instruction)
in the program, use the stack as the roots and scan the heap to see what
is still reachable. That's very expensive. How might we speed this up?

