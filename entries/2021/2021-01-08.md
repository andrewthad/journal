# Updating Immutable Maps

If you have a large immutable map and you want to update it by inserting
100 additional key-value pairs, what's the most efficient option? Clojure
has something called transients that makes it possible to do a bunch of
updates on an immutable data structure that are as in-place as possible.
That is, you could take an existing map with several thousand entries,
perform an O(1) conversion to a transient map, edit the transient map
mostly in-place, and then freeze the transient map, which would share as
much structure as possible with the original persistent map.

However, implementing transients (in any language) is complicated. I am not
aware of any other languages that have this feature. So, I wanted to explore
a simpler approach that provides some, but not all, of the benefits for
updating maps. I call this sort-build-union (SBU). The steps are:

1. Gather all new key-value pairs into an array. AoS and SoA layouts are
   both fine.
2. Sort the array of key-value pairs, resolving duplicate keys somehow.
   The sorting algorithm must be stable.
3. Convert the sorted array of key-value pairs to a persistent map.
4. Union the new persistent map and the old one.

Compared with folding over the original key-value-pair array and inserting
them into a persistent map one-by-one, I expect SBU to perform
fewer allocations. Normally, a single insert into a persistent map of
size N requires `log(N)` allocations. So, with M new key-value pairs and
`N >> M` (i.e. N is much larger than M), we would expect `M * log(N)`
allocations. This allocation count is expected in all cases. For SBU,
there is a "good case" where keys are clustered tightly (allows more
of the original tree to be shared) and a "bad case" where keys are
distributed uniformly across the existing range of keys. Let's consider
the bad case. Going back through the steps of SBU and consider allocations:

1. Array of data requires `O(M)` space.
2. Any decent sorting algorithm requires `O(M)` space.
3. If the array were unsorted, we would have to perform `O(M * log(M))`
   allocations. However, converting a sorted array to an ordered map
   only requires `O(M)` allocations.
4. This is where it matters that we are considering the worst case.
   In the worst case, none of the nodes in the original map can be
   shared. That is, we perform `O(N)` allocations.

So, SBU performs `O(N + M)` allocations in the worst case. Since we
are assuming that `N >> M` here, this simplifies to `O(N)`. If we
compare this to the `O(M * log(N))` of the naive approach, it is unclear
who is doing better. But, the worst case implies some relationship
between M and N that makes it possible to compare these. In the worst
case, M actually must be within a constant factor of N. We must have
enough new (uniformly distributed) keys to have messed up all existing
leaf nodes. And any ordered map must have `O(n)` leaf nodes. So, the
`O(M * log(N))` becomes `O(N * log(N))`.

I totally cheated in some of the complexity analysis, and that's because
I'm just a hack. Earlier, I required that `N >> M`, but then later decided
that `M = c * N`. In spite of this, I think the results are actually the
same. Having `M = c * N` instead of `M >> N` doesn't impact any of the
calculations.

Also, keep in mind that, although shaving the logarithmic factor off
of allocations in the worst case is nice, there is a best case where
SBU does even better. In the best case, we would see heavily clustered
or even duplicate keys. This means that the `O(N)` allocations when
unioning in step 4 would drop to `O(M)`. Asymptotic analysis makes
this look like it's the same as before, but there is actually a hefty
constant factor that has dropped out, since most of the original persistent
map can be shared rather than being rebuilt. It might be possible to
describe this more accurately by tracking the fanout of the ordered
map as another variable. I'll leave that for another day.

One possible optimization for this algorithm is to combine steps 3 and 4.
Rather than building the auxiliary map that is immidiately used and thrown
away, it should be possible to walk the array and the map simultaneously.
Essentially, this is a deforestation that would need to be implemented by
hand, probably for little performance gain.

# Unioning B+ Trees

If you do key compression on B trees, how hard does it become to implement
union? I don't know. It might be pretty tough though. First of all, what is
the general algorithm for unioning B trees? I've implemented it for 2-3 trees
once before, but I cannot find a description of it anywhere online. It's
something like this:

* Any merge step may return a either return a single node or two nodes with
  the key that splits them.
* If two nodes you are trying to merge are of the same height, find the median
  of their keys. If you are lucky, the median divides them the way they are
  already divided. You can just return them immidiately in this case. If not,
  you must merge any subtrees that overlap. This is tricky.
* This is getting too difficult.

This is really tricky. A more simple solution that I saw online is to only
consider then leaf nodes. Any leaf node that doesn't overlap a leaf from
the other tree can be shared. After merging (and possibly splitting) leaf
nodes, you rebuild all the branches. The drawback to this is that it never
shares branch nodes even in cases where sharing branch nodes should be
possible. For an 8-ary tree though, 87.5% of the nodes are leaves. I wonder
if there is some trick for recovering sharing with this approach though.

The biggest thing I would be worried about losing is the nice performance
in the special (but somewhat common) case where the two ordered maps have
disjoint key ranges. It should be possible to link them in `O(1)` (or maybe
`O(log N)`?) time.

What about a hybrid approach that starts by working from the top and then
switches to working from the bottom when it looks like sharing is unlikely?
The goal is to find disjoint, equal-height subtrees. For example:

     +- 42 --- 61 --- 73 --- 99 -+ (height: 2, lo: 35, hi: 114)
     |      |      |      |      |
    ...    ...    ...    ...    ...

If we have a tree of height 1 that we want to merge into this this, then
if it is bounded by 119 and 150, we could just glue it onto the right and
leave all the children unvisited. If it were bounded by 50 and 100, then
we should just give up and build from the bottom up. If it were bounded
by 44 and 59, we could drill down into the 42-61 child, but what we we
do then? Let's rethink.

If the bigger tree is height 5 and the smaller tree is height 2, what
should the goal be? I propose that we find the height-3 subtree of the
bigger tree and then look to see it the height-2 tree has any easy
place to go. If it does, great. We're done. If not, then what? When we
discover that the height-2 tree has nowhere to go, we should break it
apart into its height-1 subtrees and attempt this process on them. Oh,
this is starting to make sense. It's starting to feel like a real algorithm.

So, returning to the example diagram above, there are really just two
cases to consider:

* Height-1 subtree fits cozily into a place. That is, it can be positioned
  between two of the height-1 children (or at the beginning or end). This
  can only be discovered by actually inspecting the relevant child nodes,
  since a B+ tree branch provides an overestimate of what keys may be
  present in a subtree.
* Height-1 subtree crosses between two of the key ranges. Take the height-0
  children and see if any of them fit into any of the gaps. If they do,
  recurse with the relevant height-1 child of the height-2 tree.

There are some caveats. Inserts into full nodes are not allowed because we
do not want to have to consider overflow. So, if we see that a child node
is full, before attempting to insert, we go ahead and split it. (That's
actually a common B+ tree insertion trick for normal inserts).

This still isn't really working. I'm not actually getting anywhere. This
strategy is getting to complicated. Here is a more simple alternative:

1. Find all subtrees of the smaller tree that can be grafted into the
   bigger tree without any splitting. Graft them in.
2. Take all leaf nodes that did not make it in to the bigger tree in the
   first pass and put them into a (trivially sorted) array.
3. One-by-one, insert these leftover leaves into the bigger tree. These
   inserts can be done while walking the tree to avoid repeatedly
   rebuilding the root nodes over and over. 

So, we need two functions:

    steal :
         Map k v -- bigger map
      -> Map k v -- smaller map
      -> (Map k v, [Array (k,v)]) -- augmented map and leaf nodes
    insertSorted :
         [Array (k,v)]
      -> Map k v
      -> Map k v

The first function does not seem difficult to write. It's main purpose is
to find any easy sharing opportunities. This will always happen, for example,
if the ordered maps have disjoint key ranges. The second function is harder
to write. It's like an insert function, but it can perform bulk inserts
via sort-merge at the leaves.

I'm still having a hard time figuring this out. I think that the first step
(the `steal` function) is the right way to go. It is inexpensive. But what
about handling stuff that just doesn't fit into place? The implementation
of `insertSorted` seems complicated. Traditional B tree insertion is made
easier by a pleasant invariant that an insert into any nodes results in
either no split (one node) or one split (two nodes). But bulk inserts throw
this out of the window. If there is no restriction on the the size of the
bulk insert, then we are screwed for sure. However, if the size of a bulk insert
is restricted to be no larger than the leaf node size, then we should be alright.
With such a restriction, a leaf node could double in size. A branch node could
end up having multiple children split, but each child could only split once.
So a branch node, could, in the worst case, double in size, which still only
leads to one split.

So, we could just fold over these leaf-node key-value pairs and insert them,
chunk by chunk, into the new map. This is good but not great, since we do
end up rebuilding the root node over and over again.

Here is another possibility. What if we temporarily relaxed node-size
invariants while building the tree? Then, everything would need to be
fixed up at the end. The main way this could happen is if we needed to
attach a taller tree into a shorter tree. The taller tree would have
to have the root lopped off (possibly more than once), and you would end
up with something the height of the shorter tree but with a root node
that is much to large.

This strategy just kicks the can down the road. The hope is that there
is a more simple way to fix the node-size invariant later in the process,
but I'm not really sure that this hope is well-founded. As far as I'm
aware, the easiest way to fix the node-size invariant is to violate the
height invariant.

There might be an easier way. And, of course, it comes full circle. It's
basically the same idea as transients, but freezing is certainly not
going to be O(1). What if we start by going back to the two-phase plan
(`steal` + `insertSorted`), but we do `insertSorted` with a transient?
How would the transient `insertSorted` look? In a Haskell-like language,
we would have:

    -- The n lengths are just there to help understand what the arrays
    -- mean. Essentially, they are existentially quantified.
    data Map k v
      = Branch (Array[n] k) (Array[n+1] (Map k v))
      | Leaf (Array[n] k) (Array[n] v)
    data TMap k v
      = Immutable (Map k v)
      | TBranch (MutableArray[F] k) (MutableArray[F+1] (TMap k v))
      | TLeaf (MutableArray[F] k) (MutableArray[F] v)

The `Immutable` data constructor is a rotten deal for performance. Better
for performance would be something the GADT:

    data Map m k v where
      Branch :: Array[n] k -> Array[n+1] (Map Immutable k v) -> Map Immutable k v
      Leaf :: Array[n] k -> Array[n] v -> Map Immutable k v
      MutBranch :: MutableArray[n] k -> MutableArray[n+1] (exists m. Map m k v) -> Map Mutable k v
      MutLeaf :: MutableArray[n] k -> MutableArray[n] v -> Map Mutable k v

The existential quantification is nasty. There are probably other ways to
accomplish the same thing. But the general idea is that you do not want
anything mutable to show up inside an immutable map. But for mutable
stuff, anything can show up inside. It may be possible to accomplish this
same thing without existentials by using subtyping (`Immutable :< Mutable`)
instead. But you still need GADTs. You could also just do away with all of
it and enforce the invariant by hand in the library. But also, it's a little
disappointing that you have to deal with the extra machinery that's only
needed to speed up bulk updates on a map. It would be cool if you could instead
define a type extension with:

    data Map k v
      = Branch (Array[n] k) (Array[n+1] (Map k v))
      | Leaf (Array[n] k) (Array[n] v)
    data TMap k v extends Map k v with
      = IBranch alias Branch
      | ILeaf alias Leaf
      | TBranch (MutableArray[F] k) (MutableArray[F+1] (TMap k v))
      | TLeaf (MutableArray[F] k) (MutableArray[F] v)
    -- Casting is a no-op. Only works in one direction though. To go
    -- the other way, you need to do something else.
    cast : Map k v -> TMap k v

The idea here is that `TMap` inherits all of `Map`'s data constructors. An
important implementation detail would be that the data constructor tags for
`TBranch` and `TLeaf` would be chosen to not conflict with `Branch` and `Leaf`.
Why does this end up being so much easier than the GADT version? The GADT
version is awful because of the extra type-level tricks. But this version
has some weirdness to it. It might be better if it were somehow possible
to use the originally proposed type (with the `Immutable` data constructor)
and instruct the compiler to have `Immutable` be a no-op. That requires
less weird type-system extensions.

Anyway, regardless of what strategy is used to define the data type, the
idea is that we should be able to perform lots of consecutive inserts
without reallocating the high-up nodes each time. If you ever have to
insert into an immutable node, you instead convert it to a mutable one
and then it stays mutable for the remainder of the modifications on
the map. Splits on mutable nodes should reuse the mutable node that was
split. This would improve performance even for a series of
single-key-value-pair inserts, but bulk inserts are likely to do
even better. Let us call refer to bulk inserting into a transient
B+ tree as BITB. This operation is the real workhorse behind several
others. There are various heuristics that can be used for certain
operations:

* Union B trees: Graft in any easy wins into a transient B tree.
  Then use BITB for everything else.
* Insert Many: Sort array. Then use BITB. If the array is way bigger
  than the original map, just rebuild from leaves instead, discarding
  all branches.

The real trick is to figure out when a transient is going to win and
when it's better to just rebuild from the leaves. There are certainly
cases where one or the other is better. The obvious cases are:

* Disjoint key ranges: BITB wins. 
* Highly interleaved keys: Rebuild from leaves wins. In fact, in this
  particular case, you don't even get to reuse any of the leaves. You
  just want to dump everything into two arrays, merge them, and then
  cut out the new leaves.

How can a library figure out whether or not it wants to use BITB or rebuild?
You could walk the B tree and the sorted array (together) and count the
number of B tree nodes that will be invalidated by the insert. A leaf is
invalidated if anything will be inserted into its key range. (If a child
node is invalidated, so are all of its parents). You might want separate
counts for leaves and branches. If 95% of branches are going to be
invalidated, it's probably better to rebuild from scratch. But if only
5% of branches are invalidated, then we'll want to use a transient.
Somewhere in the middle is a cutoff. Thinking about it more, counting branches
and leaves separately is pretty important. With a fanout of 8, it is
possible that 12.5% leaf invalidation could invalidate every branch, but
it's also possible that 12.5% leaf invalidation could invalidate only 12.5%
of branches. It just depends on the distribution. We really care about
invalidation of branches, not leaves. This is awesome because it means
that we do not actually need to touch the leaf nodes during this calculation.
With low branch invalidation, we want BITB so that we get good sharing.
With high branch invalidation, we want to just rebuild. Keep in mind that
rebuilding might be able to reuse leaf nodes, and it might not, but we
do not care because that reuse does not impact the choice between the
two insertion strategies.
