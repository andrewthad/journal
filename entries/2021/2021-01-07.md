# Composing Effects with Linearity

I'm going to be a bit wobbly with the term "effects" in this entry. Typically,
"algebraic effects" refers to systems where both effects and handlers can be
defined. What I am interested in is effects with a single concrete
interpretation. Of these effects, six are commonly used in Haskell:

* Exceptions (`Either` and `Maybe`, as transformers `ExceptT` and `MaybeT`) 
* Nondeterminism (`[]`, various `ListT` transformers)
* State (`State` and `StateT`)
* Environment (`Reader` and `ReaderT`)
* Mutation (`ST`), including allocating, deallocating, and freezing
  objects with identity.
* IO (`IO`)

The important difference between mutation as an effect and IO is that mutation
can be closed off (`runST`) and IO cannot. When mutation is paired with other
effects, linearity starts to matter. Why? Because duplicating deallocation is
unsound. There are three different flavors of linearity for effects:

1. Linear
2. Affine
3. Nonlinear

The non-rigorous description of these that I'd like to offer is that, let's
say you have some effect-polymorphic function that uses a variable linearly:

    foo : X ->{M} Y
    foo = proc x.
      a <- bar -< <0,x>
      b <- baz -< <0,a>
      pure -< b

In a non-effectful setting, if `bar` and `baz` consume their arguments
linearly, then `foo` consumes its argument linearly too. If the effects
state, environment, mutation, IO, or any combination of them is used, then
the consumption remains linear. For nondeterminism, all bets are off. For
exceptional effects, the guarantee degrades to affine.

We can probably throw nondeterminism out because its implementation has
bad performance guarantees. But exceptions are important.

The reason I think it is important to track this in some way is to help
users correctly do in-place traversals over certain structures. Consider:

    traverseIntMapWithKey :
      (MutIntMap v, (Int,v) ->{M} v) ->{M} MutIntMap v

Assuming that the user is responsible for freeing all nodes in a `MutIntMap`,
the effect `M` must be linear. If `M` can throw exceptions, then our function
could leak memory.

We do want two different kinds of mutable arrays though. One would be managed
by the runtime and one would be unmanaged. The one managed by the runtime would
be useful for build-and-freeze operations, and it could support exceptions. The
unmanaged one would be used for data that was expected to live a long time. The
one managed by the runtime would also always be considered a nursery object.

Is there a way to take this "effectful arrow" idea and line it up with a module
system? Not sure.

# Simple Extension of Generational Reference Counting for Mutable Objects

In a system with only immutable objects, a GC scheme that augments reference
counting with a nursery is easy. See entries from 2021-01-01 and 2021-01-05
for more on this. However, mutable objects are more difficult because they
make it possible (even likely) that old objects will point into the nursery.
I had been thinking about complicated schemes to deal with mutable objects.
I propose a simple alternative here. It has the following properties:

1. A single mutable array type addresses all use cases (short-lived and
   long-lived).
2. Array writes are not compiled to anything with branching instructions.

The general idea is that, after being promoted from the nursery, all mutable
objects live in a remembered set where their previously observed pointer set
is duplicated. The remembered set must be scanned during each garbage
collection. This scheme greatly penalizes long-lived mutable objects, but it
does support them. The roots for garbage collection are the stack and the
remembered set. But it cannot be quite that simple. On each GC, we must take a
delta of the stack to figure out which objects should have counters incremented
and decremented. Then, we must look at all pointers in the remembered set.
The "did this change" flag (a bit) lives in the object itself. This means
that updates of the object (while the mutator is running) are cache-coherent,
but checking to see what was updated is not. As we scan each objects, we either
learn that the set of pointers has not changed, or we learn that it has
changed. In the first case (not changed), we do nothing. In the second case
(changed), we decrement counters for all the old values and increment counters
for the new values. Then, we copy over the new set to replace the old set.
There are other ways to go about this that might be more cache coherent. We
could do some kind of big "sort and diff" that may result in a smaller
number of counters being touched. Regardless, as we go through all the
mutable objects and adjust counters, we might find that the mutable object
we are considering has itself a count of 0. In this case, we should ignore
all of its current values and decrement all of the old values. Depending on
the order in which the remembered set is processed, it is possible for
the count on a mutable object to reach 0 after it has already been processed.
The simplest solution is to just deal with them during the next collection.
This is slightly unsatisfying though since it would mean that unreachable
data would be kept alive in a nondeterministic way. Really, we would like
to do an immidiate purge when the count hits zero, possibly skipping over
it if it is encountered later while walking the remembered set. To do this
though, we would need for every mutable object to store its own index into
the remembered set. That kind of space overhead is not appealing though.
Another possibity is to just walk the remembered set multiple times during
every minor GC. That's probably the most simple option. As you walk the table,
you just keep track of whether you free any mutable objects with a lower
index that your own index. If you ever do, then you have to do GC on the
remembered set again.

So, every heap object (mutable and immutable alike) would look like this:

    +-------------------+----------------+--------------+---------
    | Counter (32 bits) | Type (24 bits) | Tag (8 bits) | Data ...
    +-------------------+----------------+--------------+---------

A bit in either type or tag would need to be the dirty bit. Not sure which.
64 bits of overhead per heap object is what people are used to paying in
other garbage-collected languages. I wish that there was a way to make it
smaller. Most counters are going to be numbers like 1 or 2, so it seems
like a waste to dedicate that space to them. The lowest overhead we
could possibly hope for is 32 bits. To do that, we would have to come up
with some trick for overflowed counters, and that would be complicated.

Now I'm thinking about a multithreaded setting, just sharing immutable objects
across threads, not mutable objects since that is madness. One option
(incompatible with the prompt deallocation technique) is to just build a big
increment-decrement buffer and sort-merge it down to make moved objects
disappear. Then, you could take a lock for performing all of the actual
incrementing and decrementing. Actually, this does not work. Nevermind.

I think that it might be possible to achieve concurrent collection with
minimal cross-thread synchronization. If each object's current reference
count includes any stack/register references to it (from any thread),
then you cannot hit zero unless there really are no more references
to something. So I think that the scheme I described in the previous
paragraph does actually work. I need to think about MVars though. MVars
are kind of tricky because they make it possible for a thread with
no references to an object to pull a reference out of no where. MVars
to mutable objects should probably be disallowed entirely, but an MVar
to an immutable object should be allowed. Maybe the solution is:

* Pushing into an MVar immidiately increments the total count for
  an object by 1. If you push into an MVar, then after the push,
  the count must be at least two.
* Pulling from an MVar leaves the count alone and adds the object
  to the thread's previous stack set (since it is now in a register).

Why do MVars require immidiate action? Think about it this way. Stacks
and mutable objects are owned by a single thread. Ownership is
nontransferable. So threads can typically just generate counter updates
by walking their stacks and remembered sets. An MVar itself is managed
just like any immutable object. Multiple threads can refer to it, and
that's all good. But what about the object that an MVar points to?
Normally, with a mutable object, you can look at the snapshot that
tells you what it previously was. You can do this because one thread
manages this snapshot. But since MVar is, by design, shared across threads,
we cannot ask "what was your object during the last GC cycle?"

Another way of thinking about it is this. Assume this scenario:

* Thread A has the sole reference to a object OBJ (refcount 1)
* Thread A pushes OBJ into an empty MVar
* Thread B pulls OBJ from the MVar
* Thread A runs GC

Without any special interaction between MVar and refcounting, thread A
will see that OBJ has disappeared from its stack. This will cause
the refcount to drop to zero. Game over. However, if we follow the
proscribed steps, then the push will raise the refcount to 2. The
pull will leave the refcount at 2 and put it in Thread B's previous
stack set. Then, Thread A will bring the refcount down to 1 when it
runs GC. Then, if thread B happens to run GC immidiately after,
the refcount would stay at 1 (the stack diff would reveal no change),
which is what we want. Also, just for completeness, if Thread A ran
GC before thread B was able to pull OBJ from the MVar, Thread A
would decrement the count from 2 to 1, and the object would be
kept live by the MVar.

Pushing to an MVar has the traditional non-deferred cost of reference
counting. Pulling from an MVar is even worse that the traditional cost
since it requires messing with the stack set. But MVar operations are
inter-thread communication and are never expected to be cheap. At the
least, the operations on them always imply memory barriers. But I think
all of this is fine. It's worth mentioning that if all mutable objects
were allowed to be shared across threads, then their reads and writes
would end up expensive as well.

How can one prevent mutable objects from being shared across threads
with types? In GHC, a fork function can close over anything. We cannot
allow that. We could make mutable things and immutable things have
different kinds. We would also need to prohibit mutable objects from
appearing inside immutable objects, although oddly, we could allow
MVars to appear inside immutable objects. MVar and immutable types
would have kind `Box Shareable` (or something like that) and then
mutable types would have kind `Box Mutable`. Is it ever useful to
have an immutable map where the values are mutable objects? Honestly,
I think it's a yes. So, this is kind of awful then. We could do
something like disallowing a forked thread to close over anything
polymorphic. I don't know. This invariant will be tricky to enforce,
and I'd probably have to go for a super conservative option.

Final thought: Storing counters in a separate table rather than in
heap objects might be better. Two reasons:

1. Heap objects can be 4 bytes smaller.
2. In a multithreaded setting, this would mean that updating the counter
   of a shared object would not blow out other threads' cache entries.

The table should be simple to maintain. If you have a 1MB block for 16K
64-byte objects, then you need a 64KB block of counters for the 16K 4-byte
counters. You may also need a 2KB bitmap of 16K bits, prepared right before
relocating objects into the space, to cheaply test what is available.
