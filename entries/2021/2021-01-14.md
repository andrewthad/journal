# Nonmoving Collection with Bump Allocator

Bump allocators are fast and make good use of caches on modern computers.
Every real-world implementation of a bump allocator that I'm aware of
requires that copying collection is performed after the arena fills up.
I wonder if there is a way for bump allocation to be paired with non-moving
collection. In [Allocation with Increments in a Non-moving Collector](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.42.4594&rep=rep1&type=pdf),
Chung explores this idea. The algorithm proposed there is simple:

    p <- t (t is beginning of free area)
    t <- t + size
    if t > b (b is end of free area)
      switch to another free area

That's it. To work well, this strategy requires that large runs of space
are available. If you end up with a heap like this (1 means used and 0
means free):

    0010001001001010010101010100101010101110101011001

Then you will end up with tiny arenas and end up scanning your free list
all the time. That is, fragmentation is a killer. Another possibility
(not discussed in the papre) is to allocate into an arena with a bitmap
layed over it marked whether or not memory is in use. Pseudocode where
all objects are the same size:

    while()
      p <- t (t is next arena slot)
      pm <- tm (tm is arena bitmask)
      t <- t + SIZE (bump the arena pointer)
      tm <- tm + 1 (bump the bitmask pointer)
      if t > b (b is end of free area)
        switch to another free area
      if bitmask[pm] == UNUSED
        continue
      else
        break

This is more complicated that Chung's collector. Notably, it's in a loop.
A branch predictor will do well on the loop when the heap is not very
fragmented. But, let's consider the worst case. Chung's collector suffers
most when a heap alternates between free and used slots (assuming fixed-size
allocation at the moment). It suffers badly because every allocation becomes
a function call. This hypothetical collector does not suffer in the same
way. In the common case, even on a fragmented heap, there are no function
calls. However, we do pay for the extra reads (not writes though) of the
bitmask.

There is a more serious issue with the allocator suggested above though.
How do we handle allocations of different sizes? This is difficult. We
could attempt to do something goofy with the bitmask where every 8 bytes
are handled by a single bit in the bitmask, so for example a 24-byte
allocation would need to find a run of three zeroes. But I don't see any
simple way to do this.

More generally, here is another dilema. One of the big selling points
to me of a nonmoving garbage collection is that most nonmoving collectors
make interior pointers easy. Interior pointers are useful for embedding
arrays in other structures. Consider:

    data Foo
      age : int
      items : array[5] int

It is nice to be able to project the `items` field out of a `Foo` and
then pass it around to something that accepts `array int` as an argument.
Nonmoving collector typically make this easy because objects are broken
into size classes. This means that, from a pointer to any heap object,
the runtime can figure out what the containing object is. By contrast,
a bump allocator results in a runtime that makes it difficult to figure
out where a containing object begins. It's not impossible, but I've never
heard of a language/runtime that uses copying collector and supports
interior pointers.

So, the big issues with the bitmask approach I suggest are:

1. The bitmask only works when all objects are the same size
2. Interior pointers are easiest when finding a containing object is
   cheap. Abandoning size classes makes finding the containing object
   difficult.

Both of these are serious problems. If it were possible to instead
guarantee that a long contiguous free area were available, then
we wouldn't need the bitmask to track available space, and a standard
bump allocator would work. Is there a way to do this though? Let's
say that we want an arena to be 4KB. I'm intentionally picking a
small number. What do we do if a handful of objects keep the whole
block from being reused? We could use something like MESH to combine
these stragglers. In the worst case, terrible fragmentation could
still happen. MESH avoids this by randomizing a free list for allocation.
I really don't want to do that since it destroys some of the nice
properties of a bump allocator (cache coherence and simplicity). There
are probably things that can be done to make bad cases less likely.
Forcing everything to be aligned to the power of two equal to or
greater than its size is one such thing. Having a 24-byte allocation
straddle a 256-byte boundary could negatively impact compaction.
Padding everything is to a power-of-two size wastes at most 50%
of space. Power users could mitigate the impact of this though by
avoiding structure of certain sizes.

To support interior pointers (easily), we need to be able to figure out what
the objects in a block are. This is harder when everything on the block is not
the same size. It would be best to avoid doing extra to track this while
allocating. If everything is power-of-two aligned, then maybe this helps
us? Let's see. A pointer to location 3 must be part of one of these:

* Base 3, Size 1
* Base 2, Size 2
* Base 0, Size 4
* Base 0, Size 8
* Base 0, Size 16
* Base 0, Size 32
* Base 0, Size 64
* Base 0, Size 128
* Base 0, Size 256
* Base 0, Size 512
* Base 0, Size 1024
* Base 0, Size 2048
* Base 0, Size 4096

So, how do we figure out which one? There will only be 12 options, so that's
nice. I think that it is possible to figure it out by walking the block. But
that's expensive. Also, it requires zeroing out the block before using it
so that we can "see" unused space in the block. This might not be too bad
though. After using up a block, you could scan it and build a structure
that tells you where each object starts. If we say that the minimum object
size is 8B, then a 4KB block could house up to 512 objects. This is tricky.
But each object could only start at about 12 spots. We can represent the
number 12 with 4 bits. And then 4 bits times 512 objects is 256 bytes.
That's considerable overhead (one sixteenth of the space). An option that
requires less space would be a 512-bit bitmap (64 bytes) that alternates
between zero and one as the objects change. Actually using this would be
a pain though because you would have to find the bit and then walk backwards
through matching bits.
