# Linear Programming and Booleans

When we formulate logical AND with linear programming, we do this
(for r = p AND q):

    r <= p
    r <= q
    r >= p + q - 1
    r <= 1

For the cases where p and q are both 0 or 1, we get nice exactly
results, but when they are between 0 and 1, q is merely bounded:

    p=0,q=0 ==> r=0
    p=1,q=0 ==> r=0
    p=0,q=1 ==> r=0
    p=1,q=1 ==> r=1

    p=0.5,q=0.5   ==>   0.00 <= r <= 0.50
    p=0.6,q=0.6   ==>   0.20 <= r <= 0.60
    p=0.8,q=0.7   ==>   0.50 <= r <= 0.70
    p=1.0,q=0.8   ==>   0.80 <= r <= 0.80   ==>   r = 0.8

One notable result is that we only get an upper bound of 1 on `r` when
both `p` and `q` were 1. Additionally, we only get an upper bound of 1
when the lower bound is *also* 1. We *never* get a fully indeterminate
`r` with the full 0-1 range.
We want to be able to assert that `p AND q` is equal to `q AND p`, but
this formulation makes it difficult. If we form `r` two different times
as `r1` and `r2`, the constraints indicate that it's possible for either
to be strictly greater than the other.

Maybe we can reformulate equality. What if consider inequality to mean
that r1 and r2 have a difference of at least 1. This cannot be expressed
because it requires taking the absolute value.

If we maximize `r2 - r1`, we get 0.5, and if we minimize it, we get -0.5,
which indicates that there was no situation in which r2 was 1 and r0
was zero (or the other way around). This implies that the expressions
corresponding to r1 and r2 are equal. Here's a formulation of this that
a solver can use:

    Maximize
      OBJ: R2 - R1
    Subject To 
      ROW01: R1 - P <= 0 
      ROW02: R1 - Q <= 0
      ROW03: P + Q - R1 <= 1
      ROW04: R2 - P <= 0 
      ROW05: R2 - Q <= 0
      ROW06: P + Q - R2 <= 1
    Bounds
      0 <= P <= 1
      0 <= Q <= 1
      0 <= R1 <= 1
      0 <= R2 <= 1
    End

I wonder if we can prove DeMorgan's law with this. The law is:

    NOT (P AND Q) === (NOT P) OR (NOT Q)

As an LP file, this is:

    Maximize
      OBJ: NOTCONJ - DISJ
    Subject To
      ROW01: CONJ - P <= 0
      ROW02: CONJ - Q <= 0
      ROW03: P + Q - CONJ <= 1
      ROW04: NOTP + P = 1
      ROW05: NOTQ + Q = 1
      ROW06: DISJ - NOTP >= 0
      ROW07: DISJ - NOTP >= 0
      ROW08: DISJ - NOTP - NOTQ <= 0
      ROW09: NOTCONJ + CONJ = 1
    Bounds
      0 <= P <= 1
      0 <= Q <= 1
      0 <= CONJ <= 1
      0 <= DISJ <= 1
    End

Again, we get a minimum and maximum of -0.5 and +0.5. Very cool. This
seems like it's headed in the right direction. There are a few other
wrinkles I'd like to try to explore soon though:

* What if the result is a pair of booleans? I think we can just probably
  just maximize/minimize them separately.
* What about `popcnt`? If we take `popcnt [0.5 AND 0.5, 0.5 AND 0.5, ...]`,
  we get a result `r` where `0 <= r <= n/2`.

How could we assert that one `popcnt` is greater than another? Let's say
that we have two 8-bit vectors. Both have 7 identical trailing values that are
undetermined, but the first value of W is 1 and the first of V is 0. That is:

    W = abcdefg1
    V = abcdefg0

A solver can tell us:

    popcnt(W) >= 1
    popcnt(V) <= 7

What happens when we assign values to the bits. Let's choose 0.5 ANDed with 0.5
for all of them:

    1 <= popcnt(W) <= 4.0
    0 <= popcnt(V) <= 3.5

If we maximize the absolute difference, we get a difference as high as 4.0.
Since the ANDed values are merely bounded, we run into trouble when we
interpret them as numbers.

Question: Is it even possible to have 8-bit words A and B where the bounds
on popcnt for one of them completely contain the other? Not sure.

To maximize two independent variables at the same time, we can just sum them.
To maximize two related variables (A and B), we have to use multi-objective
linear programming, which seems complicated.

What if, for `popcnt`, we maximize and minimize each bit individually. This
would tell us if the bit is 0, 1, or undetermined. The idea is that the only
possible bounds that we should be able to get are [0,0], [0,1], and [1,1].
The middle one is the complicated one. Revisiting the above example, let's
assume we had produced W and V with:

    W = setBit (and X X) 1 True
    V = setBit (and X X) 0 True

And we have no knowledge of X. Now, we learn that:

     W[0] = 1
     W[1..7] = U
     V[0] = 0
     V[1..7] = U

And then our `popcnt` constraints are:

    1 <= popcnt(W) <= 8
    0 <= popcnt(V) <= 7

But we still didn't learn what we wanted to, which is that `popcnt(W) > popcnt(V)`.
We have lost the information that W and V share bits. What if we only

# NAND Representation

What if we pick an underconstrained representation of AND. Let's weaken it
to this:

    and(p,q) = 0.5 p + 0.5 q
    not(p) = 1 - p
    nand(p,q) = 1 - 0.5 p - 0.5 q
    or(p,q) = nand(nand(p,p),nand(q,q)) = nand(1 - p,1 - q)
      = 1 - (0.5 (1 - p) + 0.5 (1 - q))
      = 0.5 p + 0.5 q

This representation of `or` is nonsense. It is not possible to represent logical
`or` in this system. What if we try to shuffle around the nondeterminism for AND.

Let's go back to this:

    W = abcdefg1
    V = abcdefg0

If all the high bits are undetermined but equal, we can see that:

    popcnt(W) = popcnt(V) + 1

We only run into problems with:

    W = (x AND y)bcdefg1
    V = (x AND y)bcdefg0

Because we introduce an undetermined `r1` and `r2` whose relationship to one
another is unclear. What if we rewrite all boolean clauses to CNF and then
capture any equalities. For example, the above would become:

    z = x AND y
    W = zbcdefg1
    V = zbcdefg0

Let's try a weirder one:

    W = ((NOT p) AND (NOT q))bcdefg1
    V = b(NOT (p OR q))cdefg0
    ==>
    z = (NOT p) AND (NOT q)
    W = zbcdefg1
    V = bzcdefg0

We need to convert to CNF so that we can identify equal expressions. Doing this
might sometimes create large terms, but I'm not worried about that. Let's test
that CTO (count leading ones) is LTE popcnt on 4-bit words:

    popcnt(w) = w[0] + w[1] + w[2] + w[3]
    cto(w) =
        w[0]
      + AND(w[0],w[1])
      + AND(w[0],w[1],w[2])
      + AND(w[0],w[1],w[2],w[3])
    ==>
    t0 = AND(w[0],w[1])
    t1 = AND(w[0],w[1],w[2])
    t2 = AND(w[0],w[1],w[2],w[3])
    t0 >= t1
    t1 >= t2
    // Note: no need for w[0] >= t0 constraint since its implied by AND
    cto(w) = w[0] + t0 + t1 + t2

And this should basically work.
One thing that's a little weird is that we have to desugar this to include
constraints that relate `t0` and `t1`. We have to do this because we are
recomputing the conjunction of the same variables. We could instead have:

    t0 = AND(w[0],w[1])
    t1 = AND(t0,w[2])
    t2 = AND(t1,w[3])

And now we no longer need the inequalities. It's not always possible to
do this kind of clean-up though. For example, we might have a conjuncted
trio of variables `AND(p,q,r)`, and elsewhere we reference `AND(p,q)`,
`AND(q,r)`, and `AND(p,r)`. Now that I think about it, we do have the
option to just perform the assignment three different times. Let's look
at a possibility:

    AND(p,q,r)
    ==>
    pq = AND(p,q)
    qr = AND(q,r)
    rp = AND(r,p)
    pqr = AND(pq,r)
    pqr = AND(p,qr)
    pqr = AND(rp,q)

This has the huge advantage of correctly expressing all relationships between
the subterms. It has the disadvantage of being verbose. A large conjunction
would produce a ton of subterms. I can worry about this later.

What is the relationship between these terms:

1. `p or q`
2. `p or not(q)`

Truth tables:

    p q   p or q   p or not(q)
    T T |   T    |   T
    T F |   T    |   T
    F T |   T    |   F
    F F |   F    |   T

They are both related to p and q in the obvious way (learning
anything about p or q tells us the result). They also have a
relationship with one another. If I learn that `p or not(q)`
is F, then I know that `p or q` is T. But that relationship
is really through `p` and `q`, since we could discern that `p`
is F and that `not(q)` is T and `q` is `F`. Let's make sure
we can work backwards like this with `OR`:

    x = OR(p,q)
    y = OR(p,not(q))
    ==>
    x >= p
    x >= q
    x <= p + q
    y >= p
    y >= 1 - q
    y <= 1 + p - q

If we find that x = 0, we immidiately learn that p and q are
both zero as well. What about something odd like knowing that
`p >= q`. This should force `p OR not(q)` to be T since the (F,T)
assignment is no longer possible. Let's see:

    y >= p
    y >= 1 - q
    y <= 1 + p - q
    p >= q

We learn that y > 0, but we don't become certain of any values of
p or q. For example, we could have y=0.5, p=0.5, q=0.5.

I'm not sure if there is a problem here or not. We want to know
things like:

    P or Q ==> not (not P and not Q)

Both of these terms are in CNF:

    P or Q
    not(P) and not(Q)

And they are very related to one another. Look:

    p q   p or q   not(p) and not(q)
    T T |   T    |   F
    T F |   T    |   F
    F T |   T    |   F
    F F |   F    |   T

The determine one another perfectly, but they would show up as two unrelated
terms. Or would they? Let's see what would happen if we learned that `p or q`
was 1:

  pq >= p
  pq >= q
  pq <= p + q
  ==> [pq = 1]
  1 >= p (redundant)
  1 >= q (redundant)
  1 <= p + q
  ==>
  1 <= p + q

Now let's examine the other side (`not p and not q`):

  npnq <= 1 - p
  npnq <= 1 - q
  npnq >= 1 - (p + q)
  npnq <= 1
  ==> [1 <= p + q]
  npnq <= 1 - p
  npnq <= 1 - q
  npnq >= 0 (redundant)
  npnq <= 1
  ==>
  npnq <= 1 - p
  npnq <= 1 - q
  npnq <= 1

Here, we learn that `npnq < 1`, which is kind of like learning that it's
F, but not exactly the same.

What can we do? One option is to just accept that the solver will
not understand this. But that's pretty weird if we are already going
to be rewriting things to CNF. That would mean that the solver would
understand that `not (not p or not q)` is the same as `p and q`, but if
we stripped off the outer boolean negation, it would appear unrelated
to either term.

We could just remove negation from the theory entirely. Rather than
having an explicit operation for taking the complement of a word, these
operations could just have their own primitives. For example:

    setBit   : (w : Word8) -> (i : Int) -> (v : Word8 | v = w OR bit(i))
    clearBit : (w : Word8) -> (i : Int) -> (v : Word8 | v = w AND unbit(i))

The `unbit` function could be implemented as `complement(bit(i))`, but we can
avoid negation by making it its own primitive. How would these expand?

    bit(i) = k
    popcnt(k) = 1
    kt[0] <= 8 + (i - 0)
    kt[0] <= 8 + (0 - i)
    k[0] >= kt[0] - 7
    kt[1] <= 8 + (i - 1)
    kt[1] <= 8 + (1 - i)
    k[1] >= kt[1] - 7
    kt[2] <= 8 + (i - 2)
    kt[2] <= 8 + (2 - i)
    k[2] >= kt[2] - 7
    ...
    kt[7] <= 8 + (i - 7)
    kt[7] <= 8 + (7 - i)
    k[7] >= kt[7] - 7

The `popcnt` here does a lot of heavy lifting. Suppose that we set `i`
equal to 2. We end up with a constraint that `k[2] >= 1`, and for all
the others, there is no constraint. But since we have the `popcnt`
constraint, all the others are forced to be zero. It is remarkable that
this works. So an expression like `and(bit(i),bit(i+1))`, without any
special treatment, would expand into an LP that could determine the
exact values of each bit (not just bounds, exact values). This also means
that an LP solver might understand that `and(bit(i),unbit(i))` has
all bits cleared. Let's try it:

    Maximize
      OBJ: R1
    Subject To
      A1 + A2 + A3 + A4 = 1
      A1T - IX <= 4
      A1T + IX <= 4
      A1T - A1 <= 3
      A2T - IX <= 3
      A2T + IX <= 5
      A2T - A2 <= 3
      A3T - IX <= 2
      A3T + IX <= 6
      A3T - A3 <= 3
      A4T - IX <= 1
      A4T + IX <= 7
      A4T - A4 <= 3
      A1 + B1 = 1
      A2 + B2 = 1
      A3 + B3 = 1
      A4 + B4 = 1
      R1 - A1 <= 0
      R1 - B1 <= 0
      A1 + B1 - R1 <= 1
      R2 - A2 <= 0
      R2 - B2 <= 0
      A2 + B2 - R2 <= 1
      R3 - A3 <= 0
      R3 - B3 <= 0
      A3 + B3 - R3 <= 1
      R4 - A4 <= 0
      R4 - B4 <= 0
      A4 + B4 - R4 <= 1
    Bounds
      0 <= IX <= 3
      0 <= A1 <= 1
      0 <= A2 <= 1
      0 <= A3 <= 1
      0 <= A4 <= 1
      0 <= B1 <= 1
      0 <= B2 <= 1
      0 <= B3 <= 1
      0 <= B4 <= 1
      0 <= R1 <= 1
      0 <= R2 <= 1
      0 <= R3 <= 1
      0 <= R4 <= 1
    End

This does not work. The solver finds the maximum value of R1 to be 0.5,
not zero. The maximum value of R1 + R2 + R3 + R4 is 1 (we would like it
to be zero). What's happening here is that the solver can instantiate `IX`
to 0.5. This makes `A1` and `A2` both be equal to 0.5, and then when we
AND 0.5 with itself, the result has the range [0.0,0.5].

# The Problem

What's the real problem here? The problem is that AND and OR provide ranges
of possible values for their output when we formulate them in LP. This
creates two different problems:

1. If apply AND/OR to the same arguments twice, we are unable to see that
   the result is the same both times. We can try to work around this by
   commoning up the terms that use these, but this doesn't really seem to
   work well.
2. When we convert boolean expressions (represented by the [0.0,1.0] range)
   to rational numbers with `popcnt`, we let bad (non-integral) values corrupt
   the result. 
