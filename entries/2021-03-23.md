# Compression Heap Data, Again

I had thought about this before but couldn't find a good way to make it work.
I've got a mediocre solution now. Here's a sketch of it. The question is:
What is a good way to compress data on the heap? Consider something like
this:

    data JValue
      = JObject (Array Member) Int Int
      | JArray (Array JValue) Int Int
      | ...

Every `JObject` must have three fields: an array, an offset, a length. The
offset is nearly always going to be 0. The length is usually under 256.
Virtual address space is usually restricted to only use 48 bits. What
happens is that when we represent this at runtime on a 64-bit platform,
we see this:

    0x0000_4B99_9011_FFA0
    0x0000_0000_0000_0000
    0x0000_0000_0000_0003

That's a lot of zeroes in there. Let's ignore the 8-byte header since
compressing it would be difficult (the header is probably mutable). We're
looking at 24 bytes for the heap object payload. Can we do better? Perhaps.
The compression schemes that I'm consider involve decompressing the entire
object to the stack when it is needed. SIMD provides some cool shuffle
operations. Here's one that will not do us any good:

    __m512i _mm512_shuffle_epi8 (__m512i a, __m512i b)

What's the problem? We've replaced each 8-bit word with an 8-bit lookup
index. So it does not actually shrink the payload. We could instead use
either of these:

    __m512i _mm512_permutexvar_epi16 (__m512i idx, __m512i a)
    __m512i _mm512_permutexvar_epi32 (__m512i idx, __m512i a)

These provide 16-bit and 32-bit compression granularity. Let's see how each
of them would fare:

## 16 Bit

    Dictionary: (10 bytes)
    0x0000
    0x4B99
    0x9011
    0xFFA0
    0x0003
    
    Indices: (12 bytes)
    0x00010203
    0x00000000
    0x00000004

We are down from 24 bytes to 22 bytes. Not very impressive. Let's try the
other way:

## 32 Bit

    Dictionary: (16 bytes)
    0x00004B99
    0x9011FFA0
    0x00000000
    0x00000003
    
    Indices: (6 bytes)
    0x0001
    0x0000
    0x0004

Oh no! 22 bytes again! Not good at all.

# More Thoughts

This would probably work better if
we had more fields though. With 16-bit elements, the greatest compression
we could possibly hope for is 2x. With 32-bit elements, we could get up
to 4x. These best cases would only ever happen if the entire payload
was just the same byte repeated over and over. Maybe we could move the
dictionaries out of the objects. What if we used 32-bit elements, and the
dictionary sat behind a 16-bit indirection (is 16 bits realistic?). Now
the second example gets to drop the dictionary, and we drop down to 8
bytes. Of course, the dictionary hasn't really disappeared. It's just
sitting somewhere else, but we might be able to share a single dictionary
across many heap objects. Effectiveness will depend on the similarity
of the ingested data. The low 32 bits of pointers will have high entropy.
This could be a problem.

# Limitations

Every 64-byte block has to be compressed separately. That's the only
way to do this if the goal is for it to be fast. The goal is to avoid
any loops. If you have a data constructor with a 96-byte payload, you'll
have to do the first 64 bytes together and then the last 32 bytes together.
You have to store the indirection twice because you cannot be certain
that they can share a dictionary.

# Questions

How can we quickly identify an existing dictionary to reuse? I have
no idea. All we need to do though is find one that works at all. No
dictionary will provide better compression than another. You just
store indices (4x smaller than values) and an indirection. I guess
it's possible to scan all the dictionaries. That seems bad though.
Maybe you could build a set of all 32-bit words needed for each
element, use these sets as keys, and then walk the dictionaries,
performing a lookup at each step to see who could use any of the
existing dictionaries. Maybe.

What about high-entropy data? The low 32 bits of pointers are
unlikely to be reused. It should be possible to copy those onto
the stack and not handle them with the other fields. This makes
decompression more expensive, but it decreases the total number of
dictionaries that must be created.

# Arrays

Arrays are interesting because often times, we don't need random access.
Sometimes we do, but UTF-8 text doesn't offer random access, and people
are usually fine with that. The operation UTF-8 does support is folding
over all the characters in an array. What kind of compression schemes
preserve the ability to fold over elements like that? It would be nice
to be able to compress arbitrary byte sequences, especially byte sequences
that are known to be human-readable text. Experimentally, conventional
compression schemes are should to squeeze JSON
[down to 5-10x smaller](https://gist.github.com/jordansissel/3044155).
The question is, is it possible to get 2-3x and end up with something
that supports folding, preferably in either direction?
