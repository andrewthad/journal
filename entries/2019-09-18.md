# Data Store Clients: Dream and Suggestion

This entry consists of two different things I've been thinking
about. The first is a nebulous dream or maybe an exhortation, and
the second is a specific arena in which I hope to materialize this
dream.

# Dream

Most open-source data store client libraries are not
flexible. I'm talking about all kinds of data stores: RDBMS,
persisted queues, key-value stores, etc. I work mostly in Haskell,
but this problem isn't unique to the Haskell ecosystem. The
successful libaries that tend to emerge are very opinionated
and do not separate concerns in the way that software developers
typically hope to do. Part of the problem is that data store client
libraries tend to be wrappers around C libraries. Consider `libpq`
and `librdkafka` for example. Most postgresql and kafka clients
in high-level garbage-collected languages are using these underneath.
Data store clients written in C tend to be a kind of all-in-one
solution. They encode requests, decode responses, open connections,
perform DNS lookups, manage connection pools, and send and receive
using these connections. Data store client libaries written in C
regularly do all of these. With this level of functionality, there
is no way to not be an opinionated solution. Clients in
higher-level languages are bound to inherit both the opinions
and the confluence of concerns from their building blocks.
The issue I have is not that high-level libraries are opinionated.
It's that their building blocks, or rather their one monolithic
building block, are opinionated. This makes it more difficult than
it should be to build new competing high-level libraries.

The pure single-concerned library that I want to see exist for
each data store is one that focuses on encoding requests and
decoding responses. That's it. No sockets and no actual interaction
with the data store. No fancy tricks with GADTs to prevent
unrepresentable combinations of options. Writing such a library
is a lot of work, but it can make the lives of higher-level
data store clients easier. My hunch (not backed by any kind of
empirical evidence) is that the existence of these lower-level
libraries would encourage authorship of better high-level
libraries.

# Suggestion

If I am going to build a library for encoding requests and decoding
responses, how should the data types look? Of course it varies by
project, but there's a broad pattern I'd like to examine here.
From the Apache Kafka documentation, let's examine common
formatting terms.

API Keys:

- Produce: 0
- Fetch: 1
- ListOffsets: 2
- ...
- FindCoordinator: 10
- ...

Primitives:

- `STRING`: Represents a sequence of characters. First the length N is
  given as an INT16. Then N bytes follow which are the UTF-8 encoding
  of the character sequence. Length must not be negative.
- `NULLABLE_STRING`: Represents a sequence of characters or null. For
  non-null strings, first the length N is given as an INT16. Then N bytes
  follow which are the UTF-8 encoding of the character sequence. A null
  value is encoded with length of -1 and there are no following bytes.

Now the descriptions of common structures:

    RequestOrResponse => Size (RequestMessage | ResponseMessage)
      Size => int32
    Request Header => api_key api_version correlation_id client_id 
      api_key => INT16
      api_version => INT16
      correlation_id => INT32
      client_id => NULLABLE_STRING
    Response Header => correlation_id 
      correlation_id => INT32

Note that the `api_key` for `FindCoordinator` is 10. Now let's examine
the structure of `FindCoordinator` (version 2) requests and responses:

    FindCoordinator Request (Version: 2) => key key_type 
      key => STRING
      key_type => INT8
    FindCoordinator Response (Version: 2) => throttle_time_ms error_code error_message node_id host port 
      throttle_time_ms => INT32
      error_code => INT16
      error_message => NULLABLE_STRING
      node_id => INT32
      host => STRING
      port => INT32

So, the way that both requests and responses work is that you start with
the size. For requests, four metadata fields follow the size, and then the
real payload starts. Responses do not include an `api_key` field. There is
no indicator of what the response type is. That is, given an arbitrary
response, it is impossible to decode it without knowing the context in
which it was sent.

The question I'm interested in is what approach to encoding and decoding
will lead to the **greatest flexibility**. As a secondary goal, this should
also perform well, but leaving a little performance on the table is fine if
it means that the library can be used by more high-level clients. There
are several types whose necessity is immidiately evident.

    data FindCoordinatorRequest = FindCoordinatorRequest
      { key :: !Text -- probably prefer a variant that uses UTF-8 internally
      , key_type :: !Int8
      }
    data FindCoordinatorResponse = FindCoordinatorResponse
      { throttle_time_ms :: !Int32
      , error_code :: !Int16
      , error_message :: !MaybeText
      , node_id :: !Int32
      , host :: !Text
      , port :: !Int32
      }

Already, there is a little room for different opinions. What flavor
of text should we be using? Sliced text from `text` or `text-utf8`
or unsliced text from `text-short`? I think that `text-short` is
actually most approach for these strings since they are all short,
but I'll punt on this question for now. What about the field names?
I've used snake case so that they agree with the Kafka documentation,
but it's possible to use camel case (more common in Haskell) as well.
I put bangs on everything. This is important for performance because you
really do want all those sub-machine-word-sized integers to unpack
into the data constructor.

But what next? Let's define some of the additional metadata:

    newtype ApiKey = ApiKey Int16 -- hide data constructor, use pattern synonyms
    data RequestHeader = RequestHeader
      { api_key :: !ApiKey
      , api_version :: !Int16
      , correlation_id :: !Int32
      , client_id :: !MaybeText
      }

We cannot encode `FindCoordinatorRequest` as anything that can
immidiately be sent over the wire, but we can encode as a fragment
of a request. In fact, let's add some of the other encode functions
as well:

    encodeReqHeader :: RequestHeader -> Builder
    encodeFCR :: FindCoordinatorRequest -> Builder
    prefixSize :: Builder -> Builder

    -- | How to use this function:
    -- 
    -- >>> encodeRequest (RequestHeader{api_key = FindCoordinator, ...}) (encodeFCR fcr)
    encodeRequest :: RequestHeader -> Builder -> Builder
    encodeRequest !hdr bldr = prefixSize (encodeReqHeader hdr <> bldr)

The exact `Builder` type is not important here. There's one in
`bytestring`, and I have one in `small-bytearray-builder` that I
like as well. What is important is that this design clearly reflects
the protocol description in Kafka's documentation in an extremely
clear way. This makes it easy for users to understand. The only
clever trick here is that the leading `Size` field mandated by
`RequestOrResponse` is hidden from the user. I find this an
acceptable to hide since the user has no easy way to fill this
field in on their own. They would have to compute the length of
payload, which would be difficult and would add additional runtime
overhead to the encode functions. In a very general sense, a length
prefix is the only field I ever think should be hidden from the end
user when writing protocol serialization libraries. Another tempting
clever trick is automatically filling in the `api_key` and `api_version`
for the user. Consider:

    data PartialRequestHeader = PartialRequestHeader
      { correlation_id :: !Int32
      , client_id :: !MaybeText
      }

    encodeFCR' :: PartialRequestHeader -> FindCoordinatorRequest -> Builder

This makes it harder for the end user to make a mistake. There are
now two fewer opportunities for the wrong value to go in the wrong
place since both the `api_key` and `api_version` are filled in.
However, I think it's the wrong primitive to expose. It's trivial
to implement `encodeFCR'` using `encodeRequest` and `encodeFCR`, but
I want the user to have the flexibility that `encodeRequest` offers.
What flexibility? What if there's a Kafka request type that our
serialization library omits, either as an oversight or because it's
a new feature. With `encodeRequest`, a user could define their new
request type and an encoding function for it without needing to
fork the library. Or what if user wants to optimize the serialization
for a particularly common request type (possibly even caching the
serialized bytes) but doesn't want to reimplement header serialization?
But maybe `encodeReqHeader` even goes to far since it itself is
a convenince that saves but a few dozen keystrokes. The real primitives
here are `encodeReqHeader`, `encodeFCR`, and `prefixSize`, and
perhaps a serialization library should expose those as the primitives,
offering the others as convenience functions in another module.

What about responses? I am rather fond of:

    newtype ResponseHeader = ResponseHeader
      { correlation_id :: Int32
      }
    data Response = Response
      { header :: {-# UNPACK #-} !ResponseHeader
      , payload :: {-# UNPACK #-} !Bytes
      }
    -- This fields for this were given earlier.
    data FindCoordinatorResponse = FindCoordinatorResponse { ... }

    decodeResponse :: Bytes -> Maybe Response

    -- | Call decodeFCR on the response payload.
    decodeFCR :: Bytes -> Maybe FindCoordinatorResponse

So there's a `ResponseHeader` which is common to all `Response`s. But why
is the payload just `Bytes`? What are the alternatives? One possibility
is a giant algebraic sum with every possible response type in it.
Unfortunately, although I like this and think that it is generally
the best solution, the Kafka protocol makes this impossible. Since
the `api_key` is not part of the response, we have to use context
to know which decode function we should use. This means that decoding
is a two-step process in this scheme. We decode from `Bytes` to `Response`
to `FindCoordinatorResponse`.

Other protocols like postgresql prefix responses with not just a
length but [also a response type](https://www.postgresql.org/docs/current/protocol-message-formats.html).
In such a case, I find it preferrable to decode responses into
an algebraic sum that includes every possible response type. This
does not mean throwing out multi-step decoding entirely. Consider
that postgresql's `DataRow` should probably look something like: 

    newtype DataRow = DataRow
      { values :: SmallArray MaybeByteArray
      }

It's hard to say though. It might be better to do something like:

    data DataRow = DataRow
      { columns :: !Int16
      , payload :: !Bytes
      }

This second approach is probably better since you really need
to have the `RowDescription` available before you attempt to
parse a the payload of a `DataRow`. Preemptively splitting
up the row doesn't buy us very much, and it would lead to the
needless allocation of tons of tiny `ByteArray`s when then columns
being decoded were integers.
