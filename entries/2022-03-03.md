# Tracking Refinements in Low-Level IR

There are two issues I've been thinking about recently for a low-level IR:

1. I would like the compiler to produce code that throws uncatchable
   exceptions on integer overflow. However, there are many cases where
   it is easy to statically determine that overflow is not possible.
   How can we provide the compiler with this information?
2. How should the compiler reason about the lengths of arrays (or of
   array slices)? Sometimes, two arrays are known to have the same
   length. Sometimes, an array's length is statically known.

The goal in both cases is to provide the compiler with enough information
to help it to not produce unneeded assertions. Consider this example:

    sumTriple :: (Int,Int,Int) -> Int
    sumTriple (x,y,z) = x + y + z

Since nearly all computers do not allow signed numbers to exceed `2^63-1`,
we want to throw an exception when this happens. ARM and x86 make this
easy. Just check a flag register after addition, branching if the carry
bit was set. However, on a restricted set of integers, this is not
needed. Consider:

    sumTriple :: (Int{>=0,<1000},Int{>=0,<500},Int{>=0,<20000}) -> Int
    sumTriple (x,y,z) = x + y + z

Now, we don't need overflow/underflow checks. We did however, throw away
some information in the result type. It is even better to write:

    sumTriple :: (Int{>=0,<1000},Int{>=0,<500},Int{>=0,<20000}) -> Int{>=0,<21500}

And now the context that consumes the result has some nice additional
information that may help it, in turn, avoid assertions in a calling context.
Let's look at something that does not help us here:

    otherSum :: {N : Int} -> (Int{>=0,<N},Int{>=0,<N+5000},Int{>=0,<20000}) -> Int{>=0,<2*N+25000}

There's nothing wrong with this type, but it does not help us accomplish
our goal. We must assume that N may be arbitrarily large, which means
that overflow is possible. If we knew more about N, for example if N itself
had to be less than a billion, then we could avoid all the checks:

    otherSum :: {N : Int{>=0,<1_000_000_000}} -> (Int{>=0,<N},Int{>=0,<N+5000},Int{>=0,<20000}) -> Int{>=0,<2*N+25000}

As a detour, in this situation, the N parameter is *irrelevant*. The
[Dependent Haskell](https://www.cs.cornell.edu/~pamorim/papers/icfp17.pdf) paper
defines irrelevance as:

> Irrelevant quantification marks all terms (whether they are types or not) as
> erasable as long as they can be safely removed without changing the behavior
> of a program.

Tracking relevance matter a lot with full dependent types. Idris 1 was plagued
by this issue. It's less important in a system where the only truly dependent
types are numbers. Carrying around a number that you don't need isn't that
expensive. Here is a different way to right the above function:

    otherSum ::
         (n : Int, a : Int, b : Int, c : Int) | a>=0, a<n, b>=0, b<n+5000, c>=0, c<20000, n>=0, n<1_000_000_000
      -> (z : Int) | z >=0, z <2*N+25000

All of the proofs (greater than and less than) are irrelevant, and we expect
them to be erased by the compiler. This do actually need to be provided
to function calls, which is cumbersome, but this is an IR, so a compiler
should be able to fill in nearly all of this.

What about arrays of constrained elements? Maybe something like this:

    sumSmall ::
         (n : Int, xs : Array Int) | n>=0, n<1000, length(xs)=n, all(xs)=(>=0,<2000)
      -> (z : Int) | z >=0, z <2_000_000

This demonstrates two important features:

1. The length of the array is an argument to the function. Any function that
   takes an array argument must take its length as well. Otherwise, code
   generation is not possible. (GC must know the size of the array.)
2. The `all` predicate applies a proof to all elements in the array.

How does this all tie into code generation? The compiler can see the irrelevant
proof terms and make a decision, based on these terms, about whether or
not certain additions and multiplications have results that are guaranteed
to fit in a 64-bit register.

What's kind of nice is that the language can actually expose two different
addition functions. One for "crash if the result is too big" and one for
"fail to compile if the result might be too big". Compiler optimizations
could replace occurrences of the former with the latter when the right
proofs were in scope. Giving the user explicit access to both makes it
possible for users who want to write high-performance code (without any
assertions) to do so. The only caveat is that they need to be able to
prove the overflow does not happen.

# Arithmetic Proof Type

Just spitballing:

    -- Add and Multiply need some kind of normalizing magic built in.
    data Term
      = Var VarId
      | Lit Int
      | Add (Bag Term)
      | Multiply (Bag Term)
      | Min (Bag Term)
      | Max (Bag Term)
    data RelOp = Eq | Lt
    data Relation = Relation RelOp Term Term
    data Knowledge = Knowledge Relation Evidence
    data Evidence = Assumed | Derived Derivation
    data Derivation
      = Symmetic Knowledge -- op must be Eq, flips the terms
      | Transitive Knowledge Knowledge -- ops must match, all rel-ops are transitive
      | ReflexiveEq VarId
      | Plus Knowledge Knowledge -- ops must match, adds RHSs together and LHSs together
      | InjAdd Term Knowledge -- a+b=a+c ==> b=c
      | InjMult Term Term Term Knowledge Knowledge Knowledge Knowledge -- a>0,b>=0,c>=0,a*b=a*c ==> b=c
      | MinContains Term Term -- elem(e,es) ==> min es < e+1
      | MaxContains Term Term -- elem(e,es) ==> e-1 < max es
      | ...

This is a situation where it might be good to track `Eq` and `Lt` by making
`Relation` and everything built on top of it a GADT. Tracking `Min` and `Max`
this way helps avoid the need for an `or`. It's nice when we only have to deal
with `and`. In particular, we really want to have easy access to this kind of
information:

    e < k, elem(es,k) ==> min es < k

# Implementation

One thing that is not totally clear to me is how proofs should be passed
about in the low-level IR. For example, when returning a number from a
case statement:

    len :: (xs : List)
        -> (z : Int) | 0<z+1
    len(xs) = case x of
      Nil -> 0, 0<1
      Cons ->
        let z,zgt = len(x.tail)
         in 1+z, plus(zgt,0<1)

That worked out alright in that case, but only because we knew what we were
checking against. But I think that type synthesis will generally have
trouble with this kind of thing, at least when branching is involved.
For functions like add, multiply, min, max, and user-defined functions,
type synthesis is fine though. I guess this is related to the avoidance
of disjunction. Any time you branch, you just need to tell the compiler
which property it is (if any), that you're looking for in each case arm.
And then right there, at that point, the type checker will switch from
synthesis to checking.

Maybe that's not true. There might actually be a way to take the union
of two constrained values like that. A literal like zero is obvious.
For results with identifiers bound in the context, you need a way to
remove the binder. For example, above, the result of the `Cons` branch
was:

    exists z. k | k = z+1, 0 < z+1

If we eliminate z, we have:

    k | 0 < k

And then if we take the union of `0 < k` and `0 = k`, we get `0 < k + 1`.
With Z3, you can do this:

    (set-logic LIA)
    (declare-const a Int)
    (declare-const b Int)
    (assert (exists ((c Int)) (and (>= a c) (>= c b))))
    (apply qe)

This results in:

    (goals
    (goal
      (<= (+ b (* (- 1) a)) 0)
      :precision precise :depth 1)
    )

That is, it turned `exists c. a >=c && c >= b` into `a >= b`. CVC5 can do
the same with slightly different syntax:

    (get-qe (exists ((c Int)) (and (>= a c) (>= c b))))

Resulting in:

    (>= (+ a (* (- 1) b)) 0)

Using a solver like this, we can keep `or` around, which is nice. I do
like the idea of using quantifier eliminating when joining back up after
branching. Perhaps if it's a case-on-comparison, it's probably fine to
keep the predication in there, but if you're casing on an ADT, you probably
aren't going to case on it again later. And if you do, it seems odd to rely
on an association between that data constructors and the results from the
previous case. It makes the synthesized types much more complicated for what
I perceive to be no real gain.
