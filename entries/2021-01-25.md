# Merging Allocations

This is an idea for a way that static analysis could improve any automatic
memory management strategy. (Copying, nonmoving, and RC would all benefit.)
Let's say that you have something like this:

    data Foo = Foo { name : Text, age : Int, bar : Bar }
    data Bar = Bar { id : Int, description : Text }

    makeFoo : Text -> Int -> Int -> Text -> Foo
    makeFoo name age id descr = Foo
      { name = name
      , age = age
      , bar = Bar
        { id = id
        , description = descr
        }
      }


The function `makeFoo` creates a `Bar` and a `Foo` that points to the `Bar`.
Normally, we would expect something like this on the heap:

    FOO-LAYOUT | Ptr-Text | Ptr-Bar | Int
    BAR-LAYOUT | Ptr-Text | Int

That is, two separate objects, each managed separately by the GC.
If it is somehow possible to know that the inner `Bar` will not outlive
the `Foo`, then we could merge these two objects. The benefit of treating
them as a single object would be lower overhead for management of the
objects, especially if they end up aging into an old generation. The
single object would look like this:

    FOO-BAR-LAYOUT | Ptr-Text | Ptr-Bar | Int | PAD (0 bytes) | Ptr-Text | Int

What's going on? `PAD` is included for completeness even though no padding
is actually needed here. For data constructors with sub-word-sized fields,
`PAD` might be as large as 7 bytes. Also, notice that we cannot simply interleave
the fields of `Foo` and `Bar`. We need to be able to get a pointer to a `Bar`
so that we can hand it over to functions that take a `Bar` argument. There is
actually some trickery going on here. Pointers to objects would need to point
to right after the layout word. Or more likely, the layout word should go at
the end of the object. Also, the layout probably needs to be a bitmask
indicating which fields (where a field is 8 bytes) are pointers and which
fields are non-pointers. So you would end up with some low cap on the number
of words allowed in an object. Either that, or you would need a way to overflow
into more bitset words in the object header/trailer.

If types are erased for runtime, then the interesting thing is that you could
actually have `FOO-BAR-LAYOUT`, `FOO-LAYOUT`, and `BAR-LAYOUT` all show up
in the same program. The layout is not determined by the type. Rather, it is
determined on a per-allocation-site basis.

I have no idea if the analysis needed to perform this transformation is
practical. In the realm of GC, most ideas that rely heavily on static analysis
do not seem to pan out.

# Problems with Tries

Most trie-like data structures have one lousy property: pathologically bad
behavior on sets like this:

    1, 11, 111, 1111, 11111, ...

Of the high-performance tries that I'm aware of, I am certain that crit-bit
trees (and by extension qp tries) and masstrees suffer from this. There is
some good discussion of this issue on [Hacker News](https://news.ycombinator.com/item?id=3015246).
QP tries and masstrees both ameliorate this to some extent by increasing
fan out, but the issue is still there.

I wonder if there is a data structure somewhat like masstree but with adaptive
key chunking. Masstrees break strings into 8-byte chunks. That's cool, but
I wonder if instead you could have tiers:

* 1-byte chunking
* 2-byte chunking
* 4-byte chunking
* 8-byte chunking
* 16-byte chunking
* 32-byte chunking
* 64-byte chunking

Rather than storing B Trees at every layer, you could just store something
flat like an associative array with some upper bound on length. Of course,
since I gave a finite limit on chunk size, you could still construct input
that demonstrated the pathological behavior. With extremely large chunks,
this starts to look a lot more like an ordered map would (full key comparison
at every node).

What would the advantage of something like this be? If you can keep your
nodes small (less than or equal to 128 bytes), then you will be able to
make maximal use of cache on Intel. With 32-byte chunks, you could fit
3 key fragments and 4 four pointers into this space. Not great. Once you
hit 64 bytes, you're in a pretty bad spot concerning cache use. I think
that at that point, you would switch to something more traditional where
you just point to the key fragments. So maybe what would be best is:

* 1-byte chunking
* 2-byte chunking
* 4-byte chunking
* 8-byte chunking
* 16-byte chunking
* 32-byte chunking
* Large chunk (a two-child run of 33+ bytes)

As an example, let's say that you had these key-value pairs:

* farm: P0
* dogs: P1
* harm: P2
* beer: P3
* bath: P4
* scry: P5

The data structure would choose a single node with 4-byte chunking because
it would have size `6 * 8 + 6 * 4 = 72` bytes. With a smaller chunk size,
you would have to break the tree into more nodes. Although it may be
possible to save space this way, cache coherence is king. Once you
start having branch nodes, you would have `N` keys and `N+1` children
in each branch like a B tree does.

This is interesting. I don't know how it would perform terribly well in
practice. I think that you would encounter the worst case if your strings
all shared their first 33 bytes and then differed in their next byte,
and then you had more 33-byte runs, and so on. Something like this (but
with 33 As between each B

* AAAAAAAAAAB
* AAAAAAAAAABAAAAAAAAAB
* AAAAAAAAAABAAAAAAAAABAAAAAAAAAAAB
* AAAAAAAAAABAAAAAAAAABAAAAAAAAAAABAAAAAAAAAAAB

So really, we are no better off that any of the other tries described
earlier except that fan out might be much higher. Maybe this is inevitible
because tries are not height-balanced. But maybe you could take these large
chunks and make them 2-3 (or higher fan out) trees instead of degenerate 1-2
trees. That could probably lead to some kind of guarantee around balance.
