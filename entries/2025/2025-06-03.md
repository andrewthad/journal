# PL Features

Here are possible features for a programming language and how much
I value them:

* Static types: Extremely important.
* Join points: Extremely important.
* Polymorphism: Extremely important. The implementation of polymorphism
  must not box primitive values (like GHC). What I want is often called
  "intensional polymorphism", where there is some witness of an abstract
  type at runtime. This is needed so that the runtime can learn both
  the size of the type and the "pointerness" of the type. I am willing
  to conceed expressivity in order to have the implementation work this
  way. In particular, it is possible to restrict datatype definitions
  to a single field with any abstract type per constructor.
* Region types: Important. Without regions, there are a bunch of common
  build-then-consume patterns that are difficult to implement. Regions
  open up several questions. How does existential packing work? Can a
  region point to boxed data that lives outside of the region? Can a region
  be nested inside another region? And so on.
* Refinements: Nice to have. There are a bunch of simple invariants that
  are really nice to be able to track in a type system. Invariants like
  "this integer is not negative" and "this text only has lowercase ascii
  letters". This becomes more complicated when we abstract over it though.
  And we have to abstract over it to make it useful in certain critical
  cases. This is extremely helpful when we are dealing with arrays and
  indices into those arrays. There, the bound is not a compile-time constant.
  It's a natural number that we do not know until run time.
* Full dependent types: Not important. You can prove a lot of cool stuff
  with DT, but it is laborious and brittle when refactoring. I personally
  have moved further and further away from using DT over time. So I don't
  want this in a language simply because I wouldn't use it.
* Staging: Important. In Staged Compilation with Two-Level Type Theory,
  András Kovács describes a more complicated version of what I want. Strangely,
  this is the best description of staging that I can find. The feature this
  has that metaprogramming often does not have is that it is typed.
  Metaprogramming in Template Haskell involves building a syntax tree
  and then running the typechecker on the generated code at the very end.
  Error messages can appear far from the source of the error, and they
  present generated code (often nearly unreadable) to the user. Staging
  is less powerful, but I believe it would be more usable. Also, I want
  a variant of staging that includes writing the staged program as a DSL.
* Module systems (like SML): Not important. This is related to staging
  because most implementations of modules perform staged compilation.
  In practice, I do not end up needing forms of indirection like modules,
  typeclasses, or OOP interfaces.

Here are things that are not really language features but could be described
as "language adjacent":

* Large standard library: Important. It's really nice to have a bunch of stuff
  work together well. Languages like C# and Golang do well here. If a standard
  library include enough things, you can get a lot further without a complicated
  build tool that does package management.
* Good implementation of catenable builders: Nice to have. This is oddly
  specific, but I need this all the time. GHC Haskell doesn't do great here.
  It's possible to define catenable builders, but I want a version of these
  that the compiler knows more about so that the user isn't responsible for
  optimizing them.
* Good wrappers for POSIX functions: Important. Most languages invent their own
  "portable abstraction" that is lowered to either POSIX or Windows depending
  on what platform you are on. I don't need this. I want to be able to use
  the man pages on my system to figure out what syscalls are doing.
* Multithreading: Very important. This has to be supported. It's a pain to
  do this with reference counting. I'm still thinking about how to restrict
  sharing appropriately.
* A type for utf8-encoded text: Important. This needs to support refining the
  character set (probably only to ASCII subsets), and it needs to be possible
  to upcast this to bytes. The invariant that the byte sequence is utf8 encoded
  goes beyond what can be guaranteed with refinements because it deals with
  a relationship between consecutive bytes. The language needs to be able to
  provide this guarantee, and it needs to be impossible to thwart the
  guarantee. That means no "assume" and no "unsafe". This means that the
  language needs built-in functions to construct text in a way that is
  guaranteed to uphold the invariants. When an ASCII subset is used, this
  is all easy, and regular refinements do the job. But when we need to
  support arbitrary codepoints, we cannot safely paste into a buffer at
  any arbitrary position any more.
* Green threads (goroutines): Unwanted. This complicates the runtime, and
  it doesn't work for platform features that do not support polling. It is
  also not needed for most programs.
* Growable stacks: Unwanted. These are complicated, and they make profiling
  more difficult.
* It needs to be possible to implement textbook data structures (ordered
  maps, hash maps) in the language. If they have to be built into the
  compiler/runtime, it is too difficult to explore improvements.
* The runtime probably needs to be reference counted. Once you have regions,
  you have a way to accomplish, with greater predictability, what a GC nursery
  is supposed to accomplish. RC is more simple. The only frustrating part
  is the interaction between intensional polymorphism and RC. Inspecting
  the size at runtime is fine because there is still just one code path
  (size-polymorpic code is lowered to `memcpy` or to masked load/store),
  but inspecting the "pointerness" is more annoying. I do not want to
  have to litter a bunch of if-then-else statements everywhere when
  lowering polymorphic code. Another option is compiling a separate copy
  of the function. But I don't like this. Some kind of deferred scheme
  would probably help a little. It's probably possible to make a buffer
  pointer increment by zero when the data does not include pointers.
  And then once there are 16 or 32 changes in the buffer, evaluate
  them all. There is an easy straight-line path for this: Write the value,
  as a 64-bit word, to the update buffer, write the pointerness of it
  to the next address as another 64-bit word (or maybe blend them together
  in the first step), increment the buffer counter by the pointerness.
  This might be ok, but I should probably do something more simple first.
  Also, for array-range operations, it will still be necessary to perform
  the updates immidiately. The buffer is unusable in this case because
  it might not have enough space. Also, the buffer probably has to be
  flushed before dealing with the array. When processing the buffer, it
  is necessary to perform increments before performing decrements so that
  nothing live is freed. Using a buffer like this a riff on delayed RC.
  Typically, delayed RC scans the stack periodically. The strategy I'm
  suggesting is more prompt, but it also requires touching memory more
  often. Also, the updates can be accelerated by gather and scatter
  operations. The scattering part has to worry about conflicts, so it
  is necessary to sort and coalesce first. If we do use a buffer like
  this, we are free to perform increments promptly instead, but a decrement
  must always be buffered. The more I describe this, the more it seems like
  a bad optimization.

# Rethinking Lowering Refinements to Proofs

Lets think about proofs. Let's say that proofs are types that have no
representation at runtime. A refined type is just a type paired with
a proposition. For example:

    Type0  : Kind
    Type   : Kind
    Word8  : Type0
    Prop   : Type0 -> Type
    Refine : (t : Type0) -> Prop t -> Type
    Digit  : Prop Word8
    Ascii  : Prop Word8
    Refine Word8 Ascii : Type
    Array  : Type -> Type0
    Array (Refine Word8 Ascii) : Type0

We also have primitives for weakening refinements:

    Subprop         : (t : Type0) -> Prop t -> Prop t -> Type
    digitsAreAscii  : Subprop Word8 Digit Ascii
    weakenByProp    : (f : Type0 -> Type) -> (t : Type0) -> (a : Prop t) -> (b : Prop t) -> Subprop t a b -> Refine t a -> Refine t b
    
Because of some of the restrictions on where functions can appear, we can
also lift weakening over type constructors:

    weakenByPropF   : (t : Type0) -> (a : Prop t) -> (b : Prop t) -> Subprop t a b -> f (Refine t a) -> f (Refine t b)

This is not general enough though. We need to be able to lift weakening
through an arbitrary number of levels:

    Subtype         : Type -> Type -> Type
    subtypeFromProp : (t : Type0) -> (a : Prop t) -> (b : Prop t) -> Subprop t a b -> Subtype (Refine t a) (Refine t b)
    weaken          : (t : Type) -> (u : Type) -> Subtype t u -> t -> u
    lift            : (f : Type0 -> Type) -> (t : Type) -> (u : Type) -> Subtype t u -> Subtype (f t) (f u)

That's not bad. The proof terms are verbose, but a lot of common cases can be
solved predictably. We also need conjunction:

    And      : (t : Type0) -> (a : Prop t) -> (b : Prop t) -> Prop t
    ElimAnd0 : (t : Type0) -> (a : Prop t) -> (b : Prop t) -> Refine t (And t a b) -> Refine t a
    ElimAnd1 : (t : Type0) -> (a : Prop t) -> (b : Prop t) -> Refine t (And t a b) -> Refine t b

I've realized that this is similar to Lean's formulation of propositions. The
difference is that Lean defines `Prop` as a type, not a type constructor.
This makes it useful for describing a relationship between two or more
values instead of just for describing refinements.

# Contrasting Two Approaches

Let's see how both approaches to Prop look:

    addNatA : (Refine Int Gte0, Refine Int Gte0) -> Refine Int Gte0
    addNatB : (x : Int, Gte0 x, y : Int, Gte0 y) -> (z : Int, Gte0 z)

In the second form, `Gte0` takes an argument, and in the first form, it does
not. The second form lets us express things that the first form cannot:

    middle : (x : Int, y : Int, y >= x) -> (z : Int, z >= x, z <= y)

Is there an easy way to have both of these, maybe? What if we have something
like this:

    Type0  : Kind
    Nat    : Type
    Prop   : Type
    (>=)   : Nat -> Nat -> Prop
    Refine : (t : Type) -> (t -> Prop) -> Type
    Refine Nat (<=100) : Type

This works. Let's see if we can take the conjunction of several predicates:

    Conjunction : (t : Type) -> [t -> Prop] -> t -> Prop
    Conjunction Nat [>=10, <=100] : Nat -> Prop
    Refine Nat (Conjunction Nat [>=10, <=100]) : Type

This works out fine. It would be extremely helpful if we had subtyping so
that we could upcast from a more restrictive type to a less restrictive type.
There is a paper titled Subtyping Dependent Types (Aspinall and Compagnoni)
that talks about the difficulties of combining these two features. According
to the paper, the combination of the two is sound. In section 1.2, it points
out a "tangle". The traditional subsumption rule holds that typing depends
on subtypes. Systems with dependent types have a kinding rule that causes
kind checking to depend on type checking. And a rule checks that one type is a
subtype of another type must check that the kinds of the types agree. So,
typing depends on subtyping, which depends on kinding, which depends on
typing. This only seems to be a problem for metatheoric analysis of the
system though. I think it should not be difficult to implement this. What
subtyping rules do I want. Probably at least this:

      x2 <=? x1,    y1 <=? y2
    --------------------------
     (x1 <= y1) <: (x2 <= y2)

      x : Nat      x <=? y,   z : Nat
    ----------    --------------------
      x <=? x         x <=? y + z

There are two different "less than or equal to" constructs here. The one
with the question mark is not one that users have access to. During type
checking, it is introduced. The goal here is that, if someone needs to
pass a value with a type like `Nat | ? <= n+1` and they have a value of
type `Nat | ? <= n`, they should be able to do that, and it should just
work automatically. Let's see how this might work:

              p1 <: p2
    ----------------------------
     Refine t p1 <: Refine t p2

          y1 <=? y2
    ---------------------
     (<= y1) <: (<= y2)

So we need a rule for subtyping the partially applied LTE operator. It's
kind of weird that this rule is redundant. There is probably a better way
to write these rules that eliminates the redundancy. But I'm fine with
this. For conjunctions:

             isSubsetOf(props2,props1)
    ------------------------------------------------
      Conjunction t props1 <: Conjunction t props2

Again, we have a rule for something that is not fully applied. Maybe this
is the most general rule:

     f <: g,  performRule(lookupArgSubRule(f), a, b)
    -------------------------------------------------
      App f a <: App g b

    lookupArgSubRule(Gte) = GTE
    lookupArgSubRule(App Gte _) = LTE
    lookupArgSubRule(Conjunction t) = SUBSET
    lookupArgSubRule(_) = EXACT_MATCH // the fallthrough case

This is kind of weird. For every builtin proof type, we need to know how each
of the arguments are compared for a subtyping check. It's kind of like each
of the function arguments have an extra type. This probably should not be opened
up to users though. Only builtin predicates get special subtype checks like this.

If we treat conjunctions as sets, this gets pretty complicated. The resulting
system has quadratic behavior, but it's quadratic in the number of predicates,
which is likely to be very small.

I just noticed that Lean has refined types as the Subtype data type.
The [docs](https://lean-lang.org/doc/reference/latest/Basic-Types/Subtypes/)
have an example of a subtype being coerced to its base type.

If we apply subtyping to ordinary propositions, let's see:

    foo : (x : Nat) -> (z : Nat, z >= 5, z <= x)

If, in the function body, we have a proof that `z >= 7`, we could return that
as the second value in the triple, and that should work. But now I am reminded
of something that I do not like about dependent types. What if we have a
relationship between two values and one of them goes out of scope. Actually,
this should not be able to happen. Nevermind.

# User-Defined Refinements Without DT

Let's forget about DT for a moment. What if I want to enforce that two values
are related in some way? Can we write something like this:

    data Ty = Number | String | IllTyped
    data Field = Name | Age | Height
    data Atom = AtomField Field | Literal Integer
    fieldTy : Field -> Ty 
    fieldTy Name = String
    fieldTy Age = Number
    fieldTy Height = Number
    atomTy : Atom -> Ty
    atomTy (AtomField f) = fieldTy f
    atomTy (Literal _) = Number
    data Expr
      = Atomic Atom
      | Plus Expr Expr
    typesynth : Expr -> Ty
    typesynth (Atomic a) = atomTy a
    typesynth (Plus a b) = if (typesynth a == Number && typesynth b == Number) then Number else IllTyped

These functions are what LiquidHaskell calls a "measure". They are used in
refinements. When we have a term of type `Expr | typesynth == Number`, if
we scrutinize it and find that it is `Plus`, we learn additional information
about the fields. LiquidHaskell relies on an SMT solver. We might need
something different:

    wellTyped : Expr -> Bool
    wellTyped (Atomic _) = True
    wellTyped (Plus a b) = resultType a == Number && resultType b == Number
    resultType : Expr -> Ty
    resultType (Atomic a) = atomTy a
    resultType (Plus _ _) = Number

I think LH would actually make us split this up like this as well. The critical
restriction that we have is that a boolean RHS has to be a bunch of conjuncted
booleans propositions about the fields. And it cannot relate the fields to
one another. Alternatively, we could just dump the well typed invariant into the
`Expr` type:

    data Expr
      = Atomic Atom
      | Plus (Expr | resultType = Number) (Expr | resultType = Number)
    resultType : Expr -> Ty
    resultType (Atomic a) = atomTy a
    resultType (Plus _ _) = Number

I'm not sure which of these makes the most sense. It's nice to be able to
represent ill typed expressions with the same data type though. With
dependent fields, there is a clear winner:

    data Coordinates = Node
      (n : Nat)
      (Array U32 | len = n) -- x coordinates
      (Array U32 | len = n) -- y coordinates

So maybe the second form for `Expr` is better. The definition of `resultType`
has a clear bidirectional reading. And it does not scrutinize its fields.

At the end of all of this, I think I am settled on the idea of only using
propositions as refinements. I still have dependent types, so I could
always go back later and redo all the internals to regain things like:

    extractProp  : (x : Nat, y : (Nat | $0 <= x)) -> (y <= x)
    refineByProp : (x : Nat, y : Nat, y <= x) -> (Nat | $0 <= x)

Or more generally:

    extractProp  : (t1 : Type, t2 : Type, p : t1 -> t2 -> Prop, x : t1, y : (t2 | p x)) -> p x y
    refineByProp : (t1 : Type, t2 : Type, p : t1 -> t2 -> Prop, x : t1, y : t2, p x y) -> (t2 | p x)

I think the main thing that scares me about Lean-style propositions is that
I do not want to have to deal with partially applied functions. Maybe if
we had two separate predicates, it would be ok:

    PropLte   : Nat -> Nat -> Prop
    RefineLte : Nat -> Refinement

We could still convert between them:

    extractPropLte : (x : Nat, y : (Nat | RefineLte x)) -> PropLte y x

Then I wouldn't need partially applied things at the type level, which would be
nice. Well, I can just start with refinements. That should be easy.
