# More Thoughts About Refinements

As I think about this more, I'm realizing that I probably do not actually
want subtyping. But I do want commutative addition built into a language.
The compiler should understand that `a + 1 == 1 + a`. According to the
wikipedia page on unification, we can actually still get unification to work
even in the presence of an operator that is both associative and commutative.
I'm not sure that it is actually worth pursuing this, but it is nice
to know.

# Upcasting Refined Values

Let's use Lean's definition of `Subtype` for this (but without a universe
hierarchy):

    Subtype (a : *) (p : a -> Prop) : *
      val      : a
      property : p val

We can get the value of type `a` back by just projecting it. But how do
we weaken the condition:

    weaken : (p a -> q a) -> Subtype a p -> Subtype a q
    weaken f (Subtype val property) = Subtype val (f property)

What if our property is an inequality like `(x : Nat) -> x <= n` and we
want to weaken it to `(x : Nat) -> x <= n+1`? Let's try:

    p a ~ (x : Nat) -> x <= n   : Nat -> Prop
    q a ~ (x : Nat) -> x <= n+1 : Nat -> Prop
    implementation : ((x : Nat) -> x <= n) -> ((x : Nat) -> x <= n+1)
    implementation f = \m -> succRhs (f m)

This works, but it's tedious. All of the lambdas are a pain to deal with.
This is why I'm drawn to the approach where `Prop` is parameterized by
its argument type, which I discussed more on 2025-06-03. Let's see how
that approach works out:

    Prop : * -> *
    Subprop : (t : *). Prop t -> Prop t -> *
    Subtype (a : *) (p : Prop a) : * = [builtin]
    weaken : Subprop p q -> Subtype a p -> Subtype a q

This is exactly the same thing that I wrote on 2025-06-03, but there `weaken`
was named `weakenByProp`. For most of the arithmetic propositions that I
care about, we need a proposition of this form:

    SelfPlusTermLteTerm : Nat -> Nat -> Prop Nat
    SelfPlusTermLteTerm 0 n      ==>  $self <= n
    SelfPlusTermLteTerm 1 n      ==>  $self + 1 <= n
    SelfPlusTermLteTerm 1 (n+k)  ==>  $self + 1 <= n + k

Why am I drawn to this approach? I'm not totally sure. Here are a few things
that might be part of the answer:

* Generally, I don't like lambdas. They aren't data. Transforming them involves
  evaluating them. But we do not actually want evaluation to happen at runtime.
  It's difficult to make sure this doesn't happen though when a proof involves
  recursion.
* There are several notions of equality in Lean: syntactic, definitional, propositional.
  [This post](https://www.ma.imperial.ac.uk/~buzzard/xena/formalising-mathematics-2022/Part_B/equality.html)
  does a good job explaining them. I don't want to implement definitional equality.
* Deciding the definitional equality of types with lambdas is possible, but I
  don't want to have to do it.
* If you have to take proofs aparts, you really want some kind of implicit
  equality to get dumped into scope, and you also need definitional equality
  for this to work. But if you never take proofs apart, you don't need this.

None of these are compelling arguments for the system that I want. I think
I can take a considerable shortcut by building what I want. But then I could
go back and improve the dependent type system later if I needed to.

# Learning Things About Existing Data

What about functions like this:

    checkByteAbsent : (w : U8) -> Array (U8 | p) -> Option (Array (U8 | p and (? != w)))

This lets the user refine an array by checking that a certain byte does not
appear. This is useful, for example, to confirm that the NUL character is not
present in text. But how can this be implemented? We can only implement this
by working with the lower-level predicates. That is, we need to implement:

    checkByteAbsent : (w : U8) -> (xs : Array U8) -> All xs p -> Option (All xs (!= w))

And to do this, we need to be able to prove that some property holds for part
of an array:

    extendCoverage : AllRange a b xs p -> AllRange b c xs p -> AllRange a c xs p
    conv1 : AllRange 0 xs.len xs p -> All xs p
    conv2 : All xs p -> AllRange 0 xs.len xs p
    conv3 : (xs : Array a) -> All xs p -> Array (a | p)

Here, we have the length as a projected member of the array that is available
at the type level. This suggests that index be defined as:

    index : (xs : Array a) -> (Nat | ? + 1 <= xs.len) -> a

And when two arrays need to have the same length:

    dotProduct : (n : Nat) -> (Array S64 | .len = n) -> (Array S64 | .len = n) -> S64

Why not treat regions the same way? I can remember thinking about this before,
but I don't remember where it leads:

    mkBranch : {r} -> (Tree | .rgn = r) -> (Tree | .rgn = r) -> (Tree | .rgn = r)

This has no advantages over the traditional presentation of region annotations.
The only thing I can think of is that you could write a function that ignores
the annotation:

    ignoreTree : Tree -> Bool

But this function cannot inspect the tree. We do not have the region identifier
available, and it is expensive to recover it. But maybe it could just be passed
along with the object pointer as an implicit argument. We could recover it in
the function body, and recovery would be a no-op. This would let us write
functions like:

    sum : Array S64 -> S64

Without any information about the region or the length showing up in the type
signature.

Maybe treating reference-counted objects as existentially packed regions is
the right way forward.

A problem with the refinement-based region strategy is that the representation
of this type is not good:

    Array (Array U8 | .rgn = r)

The region is represented, redundantly, at every single array element. This
suggests that we really do need to do packing for these.
