# Programs That Last

I've seen this goal show up in several different contexts. The goal is
to make it so that programs last longer. Some people call this
"permacomputing", usually with the goal that a program can still be run
100 years after it is written. For example, the Hare programming language
explicitly has this as a goal. What kinds of features make success more
or less likely? 

* Tight coupling to a particular system: Bad. I'm using the term
  "system" broadly to refer to both an ISA, a kernel, a graphics card,
  and any critical libraries. In the long term, all time spent optimizating
  for a specific system is wasted since that physical hardware will
  stop existing. A dependency on a particular kernel or a set of libraries
  means that the loss of these causes the quit running.
* Tight coupling to a particular system: Good? In spite of the drawbacks
  that I mentioned above, it turns out of many of the old programs that
  people are able to run today are games designed to be run on a particular
  system. With better hardware, people have been able to virtualize these
  olds systems. Something that helps VM implementers is that the graphical
  interface is not really abstracted in any way (like it is with HTML or
  with any native gui toolkit). A system that lets programs set pixels
  directly is easier to implement that one that needs to implement a bunch
  of graphical abstractions (e.g. layout, buttons, inputs). I believe that
  Hundred Rabbits designed Uxn with this insight in mind, but I've not read
  enough about it to be certain.
* Specification and and high-level abstraction: Good, but oversold.
  High-level programming languages are vulnerable to specifications that
  nail down the semantics of the language but equip the user poorly with
  data structures needed to write programs. The result is that users have
  to rely on compiler-specific features to write programs, and these
  programs are consequently not portable. I know from experience that
  Haskell has this problem. The standard library from the Language Report
  has a bad interface for working with arrays, and it includes a interface
  for text that is guaranteed to perform poorly. GHC fixes these problems,
  but this requires the user to step outside of the standard for commonly
  needed data structures. Other standardized general-purpose languages that
  appear to have the same problem include SML and Scheme (although I have
  very little experience with either). The C programming does not have
  this problem, at least not nearly to the same extent. There really is
  a lot of portable C code that can be built with different compilers.
  However, C is in a somewhat unique position. It's ubiquity is propped up
  by POSIX, and the C world includes an uncommon cultural expectation that
  any application or library will implement all of its data structures
  (often by copy-and-pasting from an existing working implementation).
  Finally, we can consider super-high-level languages like SQL. SQL is
  standardized, but most implementations have a lot of extensions, which
  makes SQL unlikely to actually be portable. One of the difficulties is
  types. Some users need to work with arrays, ip addresses, dictionaries,
  json, and their SQL is no longer portable. Even timestamps, which are
  standardized, work differently in different implementations.

