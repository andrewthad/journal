# Compressing Arrays of Integers

I haven't thought about this in a while, but I was reminded yesterday
how disappointed I am with the Apache Arrow data format.

There are at least two common uses of integers:

1. Metrics. These are positive numbers that are typically distributed
   logarithmically. Small numbers are much more common than large numbers.
2. Identifiers. The magnitude of the number is unrelated to its meaning.

There are hybrid examples, like layer 4 ports, where a number roughly
identifies a protocol but sometimes has meaning as part of a range.

The best way to compress case 2 is with dictionaries. The best way to
compress case 1 is with an offset encoding. I am more concerned with
compression ratio than with decode performance, but I'm value ease
of implementation. Here's the strategy I'm thinking of.

# Strategy

Group array of 64-bit integers into blocks of 16 integers. Only a
16-integer block can be compressed. The scheme has a dictionary used to
initialize the block, and it has offsets that are applied to each element
separately. Here is an example (L4 destination ports):

    Values:
      443, 443, 443, 80, 53, 59475, 443, 443,
      80, 80, 443, 443, 443, 443, 443, 443
    Dictionary (4 values, 2 bits to represent index):
      Elements: 53, 80, 443, 59475
      Indices:
        2, 2, 2, 1, 0, 3, 2, 2,
        1, 1, 2, 2, 2, 2, 2, 2
    Offsets:
      Empty, no offsets needed.

Dictionary elements are each encoded with 2 bytes, so it takes 8 bytes
to encode the elements. And then we need 2 bits for each index, so we
need 32 bits for the indices, which is 4 bytes. So we need a total of
12 bytes. We need at least one additional byte as a way to communicate
how many bits were used for each dictionary element and also to communicate
that no offsets were used. So 13 bytes. These are 16-bit words, so it would
have taken 32 bytes to represent them uncompressed. That's not a great
compression factor, but it's hard to do much better when compressing
such a small amount of data. Let's look at something bimodal where both
the dictionary and the offsets are needed:

    Values:
      2000, 15, 2009, 60, 70, 2052, 85, 2100,
      2150, 100, 101, 112, 2185, 138, 2200, 140
    Dictionary (2 values, 1 bit to represent index):
      Elements: 0, 2000
      Indices:
        1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0
    Offsets (1 byte per offset):
      0, 15, 9, 60, 70, 52, 85, 100
      150, 100, 101, 112, 185, 138, 200, 140

The dictionary elements require 4 bytes. The dictionary indices require
2 bytes. And the offsets require 16 bytes. And then one more byte for
some of the metadata. So 23 bytes, and just representing as uncompressed
16-bit words would take 32 bytes.

# Concluding Thoughts

I would like to pursue this strategy further. Something I forgot to mention
earlier is that the bit-size of indices and of offsets should be restricted
to be a power of two. That is, 3-bit indices should be prohibited. They are
inconvenient to work with.

The other thought is about reusing dictionaries across blocks. It doesn't
really make sense to reuse offsets across blocks because the purpose of
offsets is to capture some of the jitter in metrics. But for dictionaries,
it might make sense to define up to 256 dictionaries (just the elements)
at the top of a column and then select a dictionary element set with 1 byte.
Or perhaps some kind of dictionary blend would be possible. I'm thinking
of the first example where the set of dictionary elements was: 53, 80,
443, 59475 (takes 8 bytes). The first three elements are reusable, but
the last one is probably not. It would be nice to select a base dictionary
and then override the element at a certain position. Or maybe just append.
Maybe the overrides always happen at the beginning (or the end) of the
dictionary element set. So if the original was [0, 53, 80, 443] and we
said "combine with [59475]", we would get [59475, 53, 80, 443]. Maybe
it takes one byte to select the base dictionary and two bytes to indicate
that [59475] is replacing the beginning of the dictionary elements. That
takes the dictionary element representation down from 8 bytes to 3 bytes,
and that would take the total down from 13 bytes to 8 bytes. This analysis
cheats by assuming that the dictionaries are free.

Something else worth pointing out is that Lemire's work on offest encodings
points out a need to represent "cheat elements" (that's not the language he
uses) where an element that isn't similar to the rest of the data in the
block is represented differently so that the number of bits used to represent
offsets can remain small. I'm trying to accomplish the same thing with
dictionaries. If we have a bunch of similar elements and one outlier, the
dictionary ends up with two elements, and then the offset will be small.

Other thought: It might be beneficial to include zero implicitly in every
dictionary. Maybe this would not matter as much if dictionaries were shared
though.

Other thought: Letting offsets be 1, 2, or 4 bits is not that bad. The reason
is that 16 times 4 is 64, so with a 16-element block, an array of any of these
sub-word-size offsets fits into a single 64-bit word. Technically, a 3-bit offset
would probably be fine, but then it would be necessary to communicate the size
a little differently (could not do powers of two anymore).
