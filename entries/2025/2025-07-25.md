# Something Like Apache Arrow

If I ever try to work out a reasonable data-frame format, here are some
things I would do differently from Apache Arrow:

* Arrow distinguishes between physical types and logical types, which is
  a good thing. I wish that it encoded the physical type and the logical
  type separately. Instead, it only encodes a logical type, and software
  must infer the physical type from the logical type. This makes it more
  difficult to add logical types. Any new logical type would result in
  all existing clients being unable to process the data. But if a client
  knew that the physical type was `u32`, there would still be a way to
  display it.
* Arrow has two ways to encode lists: `List` and `ListView`. Arrow's
  own documentation doesn't explain why this happened, but Meta has a
  [blog post](https://engineering.fb.com/2024/02/20/developer-tools/velox-apache-arrow-15-composable-data-management/)
  that provides a justification. There are at least two benefits. One
  is that elements can be written out of order. The other is that slice
  operations do not have to copy the buffer any more. Arrow's original
  formulation of lists seems unnecessary, and I would not include it.
* I would add a little more redundancy. It is impossible to know how
  the buffers in a record batch should be interpreted without having
  the schema available. It would nice if it were possible to decode
  the record batch to some kind of syntax tree. It wouldn't need to
  include logical types or field names. That's what the schema is for.
  But at least you could get a syntax tree. This would make it easier
  to write parts of the parser independently.
* Structs and unions are clearly important for some uses, but support
  for these in integrations is often lacking. This is particularly bad
  for unions, where there is no simple extension of SQL to handle them.
* Arrow lets you put null in a bunch of places where it doesn't make
  sense. For example, I'm not sure why dictionaries can have null.
  There are several of the "two places for null" situations that I think
  it would be best to avoid.
* Flatbuffers is awful to work with. I'm not sure what I would use instead,
  but it would be something else.
* I would put all of the metadata together instead of interleaving with
  the data buffers. Arrow does this to support two different access patterns.
  One is front-to-end access in a streaming fashion. The other is to start
  by reading all of the block metadata from the end of the file (which
  ends up pointing you to the beginning of the file to get the schema).
  This arrangement makes it possible to have a variant of the arrow file
  format without the trailing information, and this can be used to stream
  a large number of results. Dropping support for streams would make it
  possible to simplify the format considerably. Parquet puts all the
  metadata at the end and does not support streaming.
* Both parquet and arrow have batches of records within a single file.
  Parquet calls this "row groups" and arrow calls it "record batches".
  These batches may contain a million records. I don't know how I feel
  about batches. On one hand, they are somewhat redundant. After all,
  why not just split the data at the file level instead of within the
  file? The benefits of batches are: random access, parallel processing
  within a file, possibly better bounds on memory use. A benefit that
  I've never seen cited is that if a hard upper batch size was enforced,
  certain kinds of dictionary compression would have better guarantees
  about the size of indices (i.e. is a 32-bit index sufficient?).
* Now that I'm thinking about parquet, I'll mention that the record shredding
  strategy that it uses (Dremel) is confusing. I have a much easier time
  understand how list are encoded in arrow. Parquet's strategy is more
  general, and I think it's able to pack data better (not certain), but
  I never need this kind of generality.
* An advantage that arrow has over parquet is that arrow is easier to
  use as both an input format and an output format for queries. This
  happen in particular because arrow lets you mask out values so that
  buffer reuse is more likely.
* I want better compression, especially for integer fields, probably
  something based on Lemire's work. I'm looking for several characteristics.
  It needs to be possible to decompress as a stream with very small blocks,
  possibly with 16 or 32 elements each. (This is how some of Lemire's
  compression schemes work.) It also should be possible to just dump
  uncompressed data into the compressed format with a simple byte copy.
  This second requirement is not common for compression schemes, but
  I think it's a big deal. Let's say that we are adding two columns
  together to produce a new column where the value at each index is
  the sum of the value that the other two columns have at that index.
  There is no need to compress the result if it is going to be consumed
  immediately. So how do we deal with the uncompressed results? Parquet
  solves this by giving the user several choices of encodings (e.g.
  `PLAIN` vs `DELTA_BINARY_PACKED`). Arrow solves this by only doing
  buffer compression at the edges. I don't really like parquet's solution
  because it adds more options for "what to do when we consume this data".
  I don't like arrow's solution because the entire buffer must be decompressed
  before it is used (with parquet, it's likely that the same thing has to
  be done), which is bad for memory consumption.
* Dictionary compression is extremely important for text fields. Arrow does
  not require all entries in a dictionary to be unique, and I like this choice.
  If we just require dictionary compression for all text fields, we could
  simplify the format and the compute kernels.
* Dictionary compression can be helpful for integral types. It depends on
  the data. If the integers are used as a key or a code, their magnitude
  has no meaning. They may still be clustered around zero though. In this
  case, it's common for a single key to be at least half of the entries.
  It's possible to attach a little dictionary to each 16-element block,
  but with 32-bit or 64-bit integral elements, this still might consume
  more space than we want it to. The dictionaries need to be global, and
  they might need to encode multiple elements. Let's consider a simple
  dictionary strategy that includes a maximum of 256 elements. Each dictionary
  index is one byte. If there is a common integral key, it will show up
  in the dictionary. We might adapt the strategy from 2025-06-29 so that
  the dictionary included in a 16-byte block points to the global dictionary.
  This means that every element would only take up one byte. This brings
  the total from the first example in 2025-06-29 down from 13 bytes
  to 9 bytes. Kind of. The number 59475 probably would not appear in the
  global dictionary. So we would need to introduce it with an offset, which
  would increase memory consumption. We need a patching strategy that
  uses something like LEB128 to encode additional values. And each would be
  paired with an index (0-15). Or maybe only one patched value is allowed.
* It ought to be possible to compress dictionary indices for strings.
  It's probably worth using a variant of LEB128 so that they take up
  less space. Or we could adapt the dictionary encoding used by integers.
  There's probably not any point in messing with offset encoding in this
  context.

# Compression Strategies for Buffers

Perhaps it would be even better to make the entire set of dictionary elements
be loaded in from a global dictionary. The global dictionary would need to
encode a lot of elements multiple times, but then the element set would only
require a single byte. It probably makes the most sense to patch the block's
dictionary. Consider the input:

    Values:
      443, 443, 443, 80, 53, 59475, 443, 443,
      80, 80, 443, 443, 443, 443, 443, 443

And here's what was suggest on 2025-07-28:

    Dictionary (4 values, 2 bits to represent index):
      Elements: 53, 80, 443, 59475
      Indices:
        2, 2, 2, 1, 0, 3, 2, 2,
        1, 1, 2, 2, 2, 2, 2, 2
    Offsets:
      Empty, no offsets needed.

And here's what I am suggesting now:

    Global Dictionary: 22, 23, 53, 80, 443, 8000, 80, 22, ...
    Dictionary:
      Size: 4
      Global Index: 2
      Overrides:
        - Index: 3, Value: 59475
      Indices:
        2, 2, 2, 1, 0, 3, 2, 2,
        1, 1, 2, 2, 2, 2, 2, 2
    Offsets:
      Empty, no offsets needed.

How many bytes does this require?

* Global index: 1 byte
* Indices: 16 * 2 bits = 4 bytes
* Offset Size: 4 bits
* Dictionary size: 4 bits
* Overrides: 3 bytes (1 continuation bit, 3 size bits, 4 wasted bits, 2 byte payload)

This totals 9 bytes. This is better than before.

I think I should look at FastPFor instead. There's a good explanation of
it at https://ayende.com/blog/199523-C/integer-compression-understanding-fastpfor

With FastPFor (or some variant of it, I don't need it to be that fast, and
the repo doesn't guarantee a stable encoding), I could just break all data
up into 128-element blocks. This is pretty small, so it would be necessary
to arrange blocks contiguously and then point to the beginning of each (and
track the size) in the metadata. That's quite a bit of overhead, so it might
be good to delta encode all of the start offsets and encode them with LEB-128).
If 128 integers require 600 bytes, you'd end up needing just two bytes for the
offset and the length (since the length is implied by the difference of two
offsets). In an in-memory variant, it would be necessary to expand all of the
offsets and sizes instead of packing them, but I think 12-byte overhead would
be fine.

Actually, I've read Understanding FastPFor more closely, and it seems like
it groups blocks together so that they can share exceptions. There is a
limit on how many blocks can be stuck together since it must be possible
to index into an exception array with a `u8`.

We could probably do FastPFor on dictionary indices for strings as well. If
the dictionary is arranged so that the most common entries are at the front,
it should be possible to get some extremely high compression ratios.

Let's go back to the earlier example. Suppose that we have the same set
of values:

    443, 443, 443, 80, 53, 59475, 443, 443,
    80, 80, 443, 443, 443, 443, 443, 443

However, we have 128 of these instead (with a similar distribution), and the
ephemeral ports are all basically random. What do we end up with? Without
any frame of reference or delta coding, the algorithm should choose 9
as the "best bits". Then we would have 8 ephemeral ports represented as
exceptions, and each of these require 7 bits, and we need 8 bytes to point
to them. So we end up with about 159 bytes, about 1.25 bytes per integer.
What if we used delta coding? Let's say that this turned half of the values
into zero, and they could be represented by zero bits. We would have 64
exceptions, and each exception would be represented by 16 bits, and the
offsets would require an addition byte per exception. So, 192 bytes total.

Adding a dictionary would help a lot. With LEB128, you'd need 2 bytes
for each of the 3 common ports. The indices would take 2 bits each,
and then you'd patch with the 8 ephemeral ports. That would be about
54 bytes. That's less than half a byte per element. Maybe we could have
dictionaries, but we should just have 128-element blocks, and it would be
fine. We wouldn't need any global dictionaries either. One thing that's
a little weird about this is that FastPFOR likes to shift the patches
into the upper bits of a word. This doesn't work if you encode everything
as a dictionary first because it doesn't make sense to patch a dictionary
index. With a dictionary, it's better to just paste the unique values
at the end instead of patching anything.

This suggests that it would be helpful to have at least two different
strategies for compressing integers. I think that's okay. Each block
could have its own strategy. There's not a whole lot of benefit in
this, but it would make it possible to have a uncompressed block that
is being appended to at the end. And it makes it possible to have
blocks that use delta coding. Also, I think the blocks could probably
have 256 elements instead of 128. An importantant question to ask for
any compression scheme is "does this make it possible to search for
an exact match without decompressing into a buffer?". For dictionaries,
the answer is yes. In fact, the dictionary even makes sublinear time
possible in some cases. For PFOR, the answer is also yes, but it's
a little more complicated. If the value you are trying to match is in
the unpatched range, you walk through the low bits and figure out what
matches. Then you have clear all the bits corresponding to patched
entries. If the value you are trying to match is in the patched range,
however, you would need to walk over the patched entries instead,
and you would look up the low bits for each patch to construct the integer.
Combining delta coding with PFOR forces us to reconstruct the uncompressed
array. Combining delta coding with plain old FOR (no patching) makes it
possible to match without materializing. And honestly, situations where
delta coding is effective are unlikely to have serious outliers anyway.

I think it would be worth exploring a variant of PFOR that only
uses powers of two for bit sizes. This hurts the compression ratio,
but the implementation is much more simple and can be written in
portable C.
