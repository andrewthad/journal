# More Decompressor Thoughts

Just some more thoughts continuing 2025-07-29:

* I looked at Cap'n Proto again, and I think it's probably a good fit for
  the metadata. I don't need any of the "schema evolution" features though.
  Really, there are a lot of the features that I don't need, but it would
  be nice if a third-party tool could at least validate that the metadata
  was well formed. Actually, I'm reading more about how pointers are encoded,
  and maybe Cap'n Proto is too complicated for this. I should probably just
  start by directly serializing an array of C structs. I just tried this out,
  and it works very nicely.
* I need a format for chaining a bunch of 256-element blocks together so that
  I can compress larger arrays and compare the results against general-purpose
  compression algorithms.
* Dictionary decompression is slightly more simple when the dictionary size
  must be greater than 1.
* I need a special compression type for blocks where every element is the
  same. In this case, there should not be a block at all since the block
  would be tiny. This is a separate compression type but with no associated
  block. This would give us at least four compression types: uncompressed,
  PFOR, dictionary, constant.
* I started thinking about how string dictionaries should work, and I realized
  that it would be really helpful if all strings were hashed. If we limit
  blocks to 256 elements each, we can quickly build a vector of 32-bit random
  numbers (we actually only need 24-bit numbers) that can be used to make a
  hash function producing 16-bit hashes. We could store the vector and the
  hashes with the data.
* I've thought about moving more of the metadata out of the blocks. For a
  dictionary, you need these: index width (4 bits), element count (8 bits).
  Note: The index width is redundant. It's implied by the element count.
  From the index width alone, the position of both array (indices and elements)
  can be determined. The indices are at the beginning (position zero) and the
  elements start where the indices end. If the index width has to be a power
  of two, then `ixWidth * 256 / 64` gives the beginning of the elements.
  If not, it's a little more complicated because of the padding bits.
  For example: 2b => 8, 3b => 13, 4b => 16, 5b => 22, 6b => 26, 7b => 29, 8b => 32.
  This gives you 64-bit alignment of the elements. It's probably possible to
  save space by not requiring the element array to be 64-bit aligned when the
  elements are not 64-bit integers. Although then the arrays would overlap
  a little, which is not good. One advantage of moving the metadata out is
  that there is already unused space available in the metadata struct. The
  more important advantage is that it helps with alignment. It's nice to
  have everything in the block be 64-bit aligned. That makes it easier to
  write ordinary a portable C program that works on the data.
* I've completed a proof-of-concept implementation of dictionary compression,
  and I ran it on 1024 low-cardinality 32-bit words. It reduced the size from
  4096 bytes down to 1232 bytes. This is pretty good, but it's not great. The
  data has lots of repeated runs because of how I generated it, and LZ4 is able
  to bring it down to 546 bytes. One thing that I keep thinking about whether
  or not it makes sense to use Huffman coding. The values that show up very
  frequently should be represented by 3 or 4 bits, and the other ones could
  use a higher bit count. This would make decoding more complicated though
  since a variable number of bits are used for each word. It also makes random
  access impossible. But even PFOR doesn't have good random access, so this
  might not matter much.
