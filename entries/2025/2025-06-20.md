# Tiny Experience Report

Yesterday, I wrote a small library named `dtlight` and then played with
it in the repl. It's a programming language with dependent types but without
first-class functions. There is no lambda, and there is no partial
application. Here are my thoughts:

* We do not have to worry about types being Î· equivalent because there
  are no lambdas. Checking that two types are equal is a simple structural
  check that they match exactly.
* I use both debruijn indices and names, but I use them in different contexts.
  Names are the binders in let expressions. Debruijn indices get used in type
  signatures for top-level functions and type constructors. This way, it is
  trivially safe to apply a function to any arguments used dependently (like
  type variables). The type signature cannot reference names, so there is no
  possibility of shadowing.
* Terms and types are treated as the same thing. This was surprisingly easy.
  I've never done this before, and I thought it would be more difficult.
* Currently, types are not normalized before checking for equivalence. This
  is certainly a problem, and it means that any types that use let bindings
  will not work correctly. I don't think that it is difficult to fix this.
* I do not have any primitives that destructure values in such a way that
  additional equalities get brought into scope. I'm not sure if supporting
  this is a good idea or not. On one hand, it makes it difficult to know
  if a term can be hoisted past a boundary. On the other hand, I don't believe
  that hoisting term up past boundaries where scrutiny happens is ever a
  meaningful optimization. The ergonomic benefits of these equalities are
  huge.
* The system has a `PropOn` type constructor that imitates higher-order uses
  of `Prop`. For example, `PropOn Nat` is the type of propositions on natural
  numbers. (In Lean, this would be `Nat -> Prop`.) We can make propositions
  on naturals with `SelfPlusTermLteTerm : Nat -> Nat -> PropOn Nat`, where
  `SelfPlusTermLteTerm 1 k` means `self + 1 <= k`.
* We can refine types with `Subtype : (t : Type) -> PropOn t -> Type`. For
  example, `Subtype Nat (SelfPlusTermLteTerm 0 100)` is the type of natural
  numbers that are less than or equal to 100.
* Naturals and refined naturals (that use `Subtype`) are different types.
  Dependent uses of refined naturals are not possible. This is expected, and
  I do not think there is any way to avoid this. Because of this, there are
  two incompatible interpretation of literals. We might want 42 to have
  the type `Nat`, and we might want it to have the type `(Nat | >=42, <=42)`.
  These two types cannot not have a subtyping relationship though. One is
  kinded `Nat`, and the other is kinded `Type`. It might be necessary to
  have two notations for literals.
* The ergonomics of refined types are pretty bad. Inference would help some,
  but for refined types (i.e. `Subtype`), built-in subtyping would help a
  ton. In theory, it's possible to use tactics to fill in all the upcasts,
  but I want it to be possibly to write programs directly in the low-level
  language. Without subtyping, I'm concerned the it will be too difficult
  to do this.

Here are my next steps:

* Figure out what kinds of equality constraints are reasonable and then see
  how hard it is to support this. I think I already know the answer to the
  question. In GHC Haskell, it's possible to learn things like `a ~ Foo b`,
  and it's fine because you can just replace every occurrence of `a` with
  `Foo b`. For the same reason, it should be possible to propogate equalities
  like `n ~ m` or even `n ~ m+k` (replace all `n` with `m+k`). However,
  an equality like `n+j ~ m+k` cannot be propogated trivially. According
  to wikipedia, unification is decidable in this case, but it just seems
  too difficult.
* Figure out how difficult subtyping is.
