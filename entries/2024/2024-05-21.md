# Module System Notes

The system that I proposed on 2024-05-15 works pretty well. Here are some
implementation notes:

* General strategy for compilation:
    * Reduce type-level let bindings and most type-to-type functor
      application. The only remaining type-to-type functor application
      should be where the first argument is an indexed variable.
    * Typecheck.
    * Convert to an ANF-like form
    * Combination of floating up let bindings, CSE, and dead code elimination
    * Evaluate all functors, leaving us with just modules.
    * Perform the same slew of optimizations again.
    * Evaluate modules and module projection.
    * Final program should be a series of top-level functions and constants.
      It should be possible to evaluate this.
* I tried to share types across different stages. It did not work. Do not
  try to do this. It is possible to reuse the same implementations of Term
  and Type across different stages. But for component, it's better to just
  make a new definition for each stage. 
* Think about eliminating functors and modules more. This is more difficult
  than I originally thought it was.

# Just Some Harebrained Scheme

I was thinking (again) about how it would be nice to start by implementing the
monomorphic IR and writing programs in it before build the layer on top. What
always stops me from doing this is that I do not have access to the polymorphic
data types that are best defined in the high-level language.

But there's one data type I always have access to: Array. Arrays cannot be
defined inside of any programming language that I've ever used. They are
always built into the language. And so I build them into my languages as well.
In a monomorphic language, you have access to a slew of polymorphic operations
on arrays, but you just have to pick a concrete type every time you use them.
(This is how golang worked before generics were added.) What if I just
included more builtin polymorphic types (and associated operations)? Examples:

* Option type
* Result type
* Cons lists
* Map with integer keys
* Map with text keys
* Persistent catenable sequences

And then it becomes possible to do things like:

* Define a data type for JSON (this requires maps)
* Bind to syscalls for manipulating files and directories
* Bind to syscalls for networking
* Write a (very simple) web server
* Build lowerings from ultra-high-level languages (regex, BNF-like languages)
  to the monomorphic language.
* Decode HTTP requests (think about monomorphic effects like parsers)
* Talk to postgres

The advantage of taking this shortcut is that it becomes possible to do
more things faster. It also makes it a little easier to perform rewrites
since lots of operations become primops. The disadvantage is that we end
up with a bunch of special lowerings for stuff that really should be using
the same codegen that everything else uses. We need a special "case on
cons list" construct because a cons list is not defined inside the language
as a true ADT.

We end up needing some way to instantiate all the primitive operations for 
types at each element size class. This can be done, but we need a language
to write these in. Maybe C or maybe zig. Probably zig because it has
templates. It will be necessary to commit to a representation for each of
these, and changing that later will be difficult because the language in
which the primops are written will want things to be a certain way.

Examples of things that have to be decided:

1. How big are pointer representations? (4 bytes or 8 bytes)
2. Are pointers applied as an offset to a per-type base?
3. Can we perform GC in the middle of a primop? (If we can, we need to
   track live heap objects manually in the C-like language)
4. In objects that tag data constructors, how big are tags?
5. Should cons lists use tags?
