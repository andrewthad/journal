# Recursive Types, Identity, Lowering to C Structs (Again)

The term I forgot to search for yesterday was "equirecursive types". And there
is plenty of prior art on determining whether or not two equirecursive types
are equal.

From an [answer on reddit](https://www.reddit.com/r/ProgrammingLanguages/comments/134vg7h/recursive_type_checking/):

> Equality over μ types is a bit tricky, but much more tractable than trying
> to tame direct recursion. The core idea boils down to a single rule
> (although in a proper type checker you would need a variant with the
> arguments flipped as well): 
>
>      Γ, (μ x. τ1 ~ τ2) ⊢ τ1[x := μ x. τ1] ~ τ2
>     —————————————————————————————————————————— (μ-unfold)
>                 Γ ⊢ μ x. τ1 ~ τ2
>
> This essentially says that we can check equality of a μ type by unfolding
> a single layer of recursion and assuming that subsequent equalities between
> these types will hold. This is the step that is hard with implicit recursive
> synonyms.
>
> This is probably easier to understand with an example. Let's consider the
> following equality
>
>     μ x. (x, Int) ~ μ y. ((y, Int), Int)
>
> This would be checked as follows (I'm going to leave out unnecessary equality
> assumptions and trivial equality checks so they don't take up too much space) 
>
>     ∅  ⊢ μ x. (x, Int) ~ μ y. ((y, Int), Int)               | μ-unfold (left)
>     E  ⊢ (μ x. (x, Int), Int) ~ μ y. ((y, Int), Int)        | μ-unfold (right)
>     E  ⊢ (μ x. (x, Int), Int) ~ ((μ y. ((y, Int), Int), Int), Int)
>                                                             | (,)
>     E  ⊢ μ x. (x, Int) ~ (μ y. ((y, Int), Int), Int)        | μ-unfold (left)
>     E  ⊢ (μ x. (x, Int), Int) ~ (μ y. ((y, Int), Int), Int) | (,)
>     E  ⊢ μ x. (x, Int) ~ μ y. ((y, Int), Int)               | E
>     done
>     where E = μ x. (x, Int) ~ μ y. ((y, Int), Int)

And the post author concludes by linking to
[CS611 Lecture 33](https://www.cs.cornell.edu/courses/cs611/2001fa/scribe/lecture33.pdf).

The way I have been able to follow the example best is to read it backwards.
We start with the assumption E and then work our way toward the final unfold
that erases the context extension.

The Cornell lecture includes a nice description of an algorithm checking
equality of equirecursive types:

> Now we have to solve this problem: In a system with equi-recursive types,
> when are two recursive types equivalent? As we have stated above, two types
> are equivalent if their infinite unfoldings are identical. Another way
> to think of the infinite trees above is as graphs containing loops.
> The μ constructor tells us where back edges are.
>
> (Image of a graph is a omitted since I cannot reproduce it with text)
>
> According to this intuition, we need a graph algorithm. The idea of this
> algorithm is to see if all (finite or infinite) paths in each syntactic tree
> are possible in the other. The algorithm goes as follow:
>
> * Recursively walk both trees
> * On back edges, record that the two nodes were reached simultaneously (and
>   are presumably equivalent)
> * If comparing two nodes and they are already recorded as equivalent, return

This prodecure certainly terminates because it does not perform any unfolding.
I need to figure out how this can be incorporated into a type checker. It
would be nice to use the spirit of this idea without actually building a
graph.

# Hashing Equirecursive Types

When lowering to C, I like to take the hash of a type and use that as the
C struct name. Equirecursive types make this difficult. I think the only
way to do this would be to find a hash algorithm that has some notion of
a fixed point. A simple (and bad) hash algorithm could hash an object type
by unioning (bitwise or) the hashes of each field. That way, if an object
contained itself, we could pretend that the occurrence in field position
had a hash of zero. This would result in the correct hash still being
computed.

This is a terrible hash algorithm though, and objects with enough fields
would be almost certain to have the hash value be the string of all
ones (all bits set to one). But these properties were helpful:

    or(0,a) = a
    or(a,a) = a

Let's look at XOR and XNOR (a.k.a. logical biconditional):

    A | B | XOR(A,B) | XNOR(A,B)
    ----------------------------
    0 | 0 | 0        | 1
    0 | 1 | 1        | 0
    1 | 0 | 1        | 0
    1 | 1 | 0        | 1

Neither of these work because combining an argument with itself does not behave
the way we need it to. It is not actually necessary that the operation even
be commutative. Let's say that B is the bit of the recursive type being
mixed into the hash. Because we want `F(b,b) = B`, we need this truth table:

    A | B | ?
    ---------
    0 | 0 | 0
    0 | 1 | _
    1 | 0 | _
    1 | 1 | 1

But we also need to be able to pretend that the B is 0 initially and
not have that impact the result (`F(a,0) = a`):

    A | B | ?
    ---------
    0 | 0 | 0
    0 | 1 | 0
    1 | 0 | 1
    1 | 1 | 1

This is no good. It just ignores B entirely. The reason that XOR is
a useful way to combine hashes is that both bits contribute, and the
results do not tend toward zero or one (because the results are balanced).
I think that the property we actually need is:

    F(0,x) = F(x,x)

Which implies:

    A | B | ?
    ---------
    0 | 0 | y
    0 | 1 | z
    1 | 0 | w
    1 | 1 | z

The problem is that all the possible solutions are not XOR-like and
are unsuitable for hashing. The deficiencies have two forms:

1. Result column is not two ones and two zeros
2. Result column ignores an argument

This just isn't going to work. There is a paper
[Efficient Hash-Consing of Recursive Types](https://open.bu.edu/bitstream/handle/2144/1800/2000-006-hashconsing-recursive-types.pdf)
that describes a technique for hashing recursive types, but it has to
build a DFSA and minimize the DFSA first.

# A Different Idea

What if I just embrace the DFSA approach? But how far could I go with it?
What if that's what the compiler used even while typechecking? The tricky
thing is that before typechecking the application, it would be necessary
to build a giant DFSA representing every type that could occur. Whole program
compilation. That's fine. And I think that in a monomorphized language, this
is actually doable. If a type doesn't get written down at least once in the
source code, we cannot ever create values of that type. So we can start
by scanning the entire application to learn what types get used. Then we
build the DFSA. Then we typecheck the program, but when we synthesize a
type, the result is actually a number corresponding to a state in the DFSA.
In some places, we have to resolve the types as they were originally written
to DFSA states. We should probably do that before typechecking. Now
typechecking becomes very simple. Projection just follows a transition
to get to the state of the field. All type equality check just compare two
numbers. Easy.

An interesting properties of this approach is that optimization passes cannot
produce programs that use types that had not originally been used. I do not
think this is a bad thing. It's just interesting.

What about refinements and subtyping? This part does not fit neatly into
the framework. If types are states (and fields are transitions), then what
are refined types? They are like states inside of states. Perhaps upcasts
could be signified by a different kind of transition. When we ask "Can
expression x be used where type T is expected?", we could synthesize
the type of x and then check for paths to T. This works, but I do not
love it. It is weird to make transitions means two different things.
