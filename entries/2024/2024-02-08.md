# Experiences with Open Storage Formats

I've been working with parquet files, arrow files, polars, and DuckDB recently.
The allure of storing data as parquet files is that the data remains yours.
If you are dissatisfied with the tool that you are using to query the data,
you can use a different one. Or you can write your own. This one benefit is
a big deal. If you have 100TB of data in elasticsearch or postgres or sqlite
or wherever else you might keep the data, you've only got one way to browse
the data.

Parquet's first release was in 2013. So, we are about one decade into this.
There are still a surprising number of pain points that I have encountered.
First, the issues with the formats:

* A dearth of types in logical schemas. No IP addresses. No MAC addresses.
  UUIDs are specified in parquet but not in arrow. UUIDs are poorly supported
  by most tools.
* Strange types in logical schemas: timestamps with time zones, decimal
  numbers with limited precision.
* Poor documentation of data formats. Arrow uses flatbuffers for metadata,
  and parquet uses thrift. In both cases, the schema files are missing
  crucial information that someone reading them needs to understand them.
  It is not possible to read the spec and then write a program that generates
  a well formed data file. In both cases, I've had to read the spec while
  reverse engineering an existing data file to work out the details.

And then there are issues with tools. I've going to focus on polars since
that's the one that I have used most:

* Polars fails to decode arrow files
  [when optional compression is disabled](https://github.com/pola-rs/polars/issues/14215).
* Polars losses information by mapping arrow's fixed-length byte
  arrays to its `binary` type, which is a variable-length byte array.
  This is not part of polars's documentation. The user must figure this
  out by seeing what happens when polars loads an arrow file with this
  kind of data.

In the open data format ecosystem (and I'll count avro for this as well),
the happy path is where all of your data can be described as either a
signed 64-bit integer or a string. That's what works well everywhere,
but it has to. No one will use a tool that doesn't support those two
types. Once you veer off that happy path (ints and strings), you start
to encounter a lot of rough edges. I am disappointed by this. However,
I still think that this approach to storing data is better than the alternative
of feeding data into an application that assumes exclusive access to
the data.

# Integration of VFS and an Open Data Format

Imagine that there were a data format with these properties:

* A logical column is represented by one or more representational column.
  The representational columns include uncompressed, dictionary compressed,
  run-length compressed, delta-coding compressed, etc. This means that the
  data file is bloated. The same data is represented more than once.
* Each representational column is its own file.

This is a strange format because it wastes space. But, we could improve disk
consumption by building a VFS (virtual file system) that only stores the data
once on disk with whichever kind of compression makes sense. When an
application tries to load a particular representation of the data, the VFS
might need to build the representation, or it make already have it cached.
Now, many applications may read the same data. They will share pages of
memory, and they do not repeatedly decompress the data.

This means that a user could write an application that does not assume
that a VFS is used for acceleration, and it should work fine.
