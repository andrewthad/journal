# Fine-Grained Region Tracking

On 2024-09-10, I examined several strategies for supporting arrays with
elements in different regions. By extension, these strategies would apply
to any type constructor (e.g. ordered maps), not just arrays. Now I
am considering how important this feature is. Here is what I had at the end
of 2024-09-10:

    index     : (            Array@⊤ a@n, S64) -> a@n
    reverse   : (Handle n+x, Array@⊤ a@n) -> Array@n+x a@n
    append    : (Handle n+x, Array@⊤ a@n, Array@⊤ a@n) -> Array@n+x a@n
    replicate : (Handle n+x, S64, a@n) -> Array@n+x a@n
    sum       : (            Array@⊤ S64@⊤) -> S64@0
    fold      : ((b@m, a@n) -> b@m, b@m, Array@⊤ a@n) -> b@m

If the region annotation on the element is instead pushed down (i.e. it
matches the array's region annotation), we get this instead:

    index     : (          Array@n a, S64) -> a@n
    reverse   : (Handle n, Array@n a) -> Array@n a
    append    : (Handle n, Array@n a, Array@n a) -> Array@n a
    replicate : (Handle n, S64, a@n) -> Array@n a
    sum       : (        , Array@⊤ S64) -> S64
    fold      : ((b@m, a@n) -> b@m, b@m, Array@n a) -> b@m

What do we lose? Before, we could build an array in region 102 of
elements in region 100, and then we could reverse this, building a new
array in region 101, still with elements in region 100. But now we cannot.
Once we put the elements into an array in region 102, we lose track of
where the elements themselves live.

For primitive types, it is possible to work around this limitation.
For example:

    reverse64 : (Handle n, Array@⊤ S64) -> Array@n S64

But it is not possible to write a more general function unless we have
a special quantifier for primitive types. That suggests different kinds
for primitive and non-primitive types, and I don't like that.

Let's reframe this in light of the original question: Is it important
to be able to reverse an array into a lower region? And the answer is
probably no. Let's think of situations that arise more often in practice:

* Array builders. When building an array of unknown length (e.g. while parsing
  from serialized data that uses indefinite-length array encoding, like JSON),
  we might want to be able to populate a structure chunk by chunk and then copy
  into an array as a final step.
* Sorting. If we sort data that includes both a key component and a value
  component, we drag the values around without ever inspecting them. We may
  need a temporary region for intermediate results.

How might we handle these cases? Let's think of what an array builder could
look like (using asterisk for uniqueness):

    Builder : Type -> Type
    new     : (Handle n+1) -> *Builder@n+1 a
    push    : (Handle n+1, *Builder@n+1 a, a@n) -> *Builder@n+1 a
    freeze  : (Handle n, *Builder@n+1 a) -> Array@n a

There are several important things happening here:

1. Because of how builders are intended to be used, we encode a specific
   relationship between a builder's region and its elements' region.
2. Uniqueness types help ensure soundness

This is very similar to the strategy from 2024-09-10, but it has been tailored
more narrowly. Instead of supporting arbitrary region differences, we only
support a difference of one. This reduces the number of type variables. And
this is only supported for this one type. We could do the same for arrays:

    // Note: I do not think this is useful. It's just here for illustration.
    index     : (AltArray@n+1 a, S64) -> a@n
    replicate : (Handle n+1, S64, a@n) -> AltArray@n+1 a

And this can be understood as a specialization of the system shown at the
beginning of this blog entry. It is not usually desirable to use arrays
as ephemeral data structures, although it can be helpful sometimes. Let's
look at the sorting example. Here, we might not actually need anything
special. We can shuffle around an array of keys and indices (living in
a higher region) and then permute at the end.

Here is another way to think about the more simple system. In this system,
allocating functions that accept an array exist to create a temporary view
of the data in the array, and that view of the data does not outlive the array.
There are two views that the view could outlive the array:

1. The view itself is primitive. Examples: `sum`, `length`, `countSatisfying`
2. The array's elements are primitive. Anything that deals with strings
   falls into this category.

I suspect that handling the second case is important. String builders need
to be able to copy from any region:

    StringBuilder : Type
    new     : (Handle n) -> *StringBuilder
    push    : (Handle n, *StringBuilder@n, String@⊤) -> *StringBuilder@n
    freeze  : (Handle n, *StringBuilder@n) -> String@n

This is not quite right since the `freeze` operation leaks memory, but the
important operation here is `push`, which shows that the string we copy
from can live anywhere. We could attempt something more flexible:

    Builder : Type -> Type
    newU8   : (Handle n) -> *Builder U8
    newU16  : (Handle n) -> *Builder U16
    newU32  : (Handle n) -> *Builder U32
    newU64  : (Handle n) -> *Builder U64
    push    : (Handle n, *Builder@n a, Array@⊤ a) -> *Builder@n a
    freeze  : (Handle n, *Builder@n a) -> Array@n a

Here, we have a polymorphic `push`, but types like `Builder Person` are
uninhabited. This kind of trick makes it impossible to implement the builder
in library space though since we do not actually have a polymorphic copy
operation like that. Maybe we should just have two kinds. That would solve
this cleanly.

Having two kinds also solves other problems. Cross-thread communication
is difficult to do with regions. Most approaches to this are complicated.
They involve things like freezing a region and then passing it to another
thread (or maybe several other threads), and then some kind of reference
counting mechanism has to be used to deallocate. We can do something much
more simple (but that is less expressive) by just requiring all cross-thread
communication to be primitive types. Then there is no ownership problem
because the data is just copied from sender's stack into a data structure
that a parent thread owns and then into the recipient's stack. Any message
like this would be capped at around 64 bytes. For some stuff, this works
fine. For other stuff, it doesn't work at all. Let's consider the common
example of a worker thread that has exclusive access to important resources.
Some kind of cross-thread communication channel lets the worker receive
requests. Let's suppose that there is no acknowledgement mechanism because
that adds complication. This is simple and should work fine.

Let's revisit the original `Builder`. What if it were possible to customize
the number of levels:

    Builder : Type -> Type
    new     : (Handle n+c) -> *Builder@n+c a
    push    : (Handle n+c, *Builder@n+c a, a@n) -> *Builder@n+c a
    freeze  : (Handle n, *Builder@n+c a) -> Array@n a

Not a big difference, but we can now have the builder be any number of regions
up from its elements. But something is wrong. Given a value of type
`Builder@20 Foo`, we do not know how many levels the difference is
comprised of, which makes this unsound. We need this:

    Builder : Nat -> Type -> Type
    new     : (Handle n+c) -> *Builder@n+c c a
    push    : (Handle n+c, *Builder@n+c c a, a@n) -> *Builder@n+c c a
    freeze  : (Handle n, *Builder@n+c c a) -> Array@n a

The type `Builder 2 Foo` makes it clear that all the elements are two levels
down from the builder. What about tracking the level of the element instead of
the difference:

    Builder : Nat -> Type -> Type
    new     : (Handle n+c) -> *Builder@n+c n a
    push    : (Handle n+c, *Builder@n+c n a, a@n) -> *Builder@n+c n a
    freeze  : (Handle n, *Builder@n+c n a) -> Array@n a

What if arrays worked like this too:

    Array     : Nat -> Type -> Type
    index     : (Array@⊤ n a, S64) -> a@n
    append    : (Handle (n+c), Array@⊤ n a, Array@⊤ n a) -> Array@(n+c) n a
    replicate : (Handle (n+c), S64, a@n) -> Array@n+c n a
    sum       : (Array@⊤ n a) -> S64

One problem is that our top type (⊤) no longer works as well. The type of
`sum` requires an argument `n`, but it should not need one. This ends up
looking a lot like what I had at the end of 2024-09-10, but it is more
confusing.

I think I want to just go back to the 2024-09-10 strategy. It does a better
job expressing what I want. With the right syntactic defaulting, we could have:

    index     : ∀(n : Reg).           (            Array a@n, S64       ) -> a@n
    reverse   : ∀(n : Reg) (c : Nat). (Handle n+c, Array a@n            ) -> Array@n+c a@n
    append    : ∀(n : Reg) (c : Nat). (Handle n+c, Array a@n, Array a@n ) -> Array@n+c a@n
    replicate : ∀(n : Reg) (c : Nat). (Handle n+c, S64, a@n             ) -> Array@n+c a@n
    sum       :                       (            Array S64            ) -> S64
    fold      : ∀(n : Reg) (m : Reg). ((b@m, a@n) -> b@m, b@m, Array a@n) -> b@m

Above, any unspecified region in argument position is defaulted to ⊤,
and any unspecified region in result position is defaulted to 0.
This lets us omit region annotations for all arguments that are read from
and not aliased. This can be a problem for primitive types, which ideally
should be in the zero region. In some of the examples above, `n` and `c`
can be inferred from the value arguments. It is most likely to work for
`reverse` and `append` because an upcast of `n` is unlikely. If we pass
`n` explicitly, `c` is easily inferred in all of these. We could write:

    replicate{n}(myHandle, 42, 100u64)

And `100u64 : U64@0`, and region 0 is a subtype of region `n`, so this works
out fine.

In this system, we can define a more general builder type that supports
an arbitrary number of levels between the builder and the elements, but
we could define a specialization of each function that sets `c=1`, and
this variant would have superior type inference. If we have a handle
of type `Handle k`, then we unify `k ~ n+1`, and everything will work
out fine. The functions that accept a handle of type `Handle n+c` do not
provide enough information to recover `n` from the types of the arguments.

Syntactically, let's try blending this with refinements:

    index           : ∀(n : Reg). (Array a@n, S64[0..]) -> a@n
    safeIndex256    : ∀(n : Reg). (Array[255] a@n, S64[0..255]) -> a@n
    decodeAlphaNum  : ∀(n : Reg). (Array@n U8) -> Option (Array@n U8[a-zA-Z0-9])

Notice that in `decodeAlphaNum`, the argument array has `U8` defaulted to
the ⊤ region, but the result is in the zero region. That's ok. We have
an explicit coercion that lets us assume that any primitive type (even inside
of a type constructor) is in the zero region. Let's do a few more of these
with bounds checked array operations:

    indexVec : ∀(r : Reg) (n : Number). (Vector n a@r, Fin n) -> a@n

This works out fine as well. We need a little help:

    withFin : S32[0..] -> (∀(n : Number). Fin n -> a) -> a

Wait. Why am I using natural numbers for regions if I end up using rank-2
types for a different feature? My concern was with shadowing, but if I use
some variant of debruijn levels for the names (instead of letting the user
pick nice names), this should not be an issue. What do we end up with if
we do this:

    index     : ∀(r : Reg).           (          Array a@r, S64       ) -> a@r
    reverse   : ∀(r : Reg) (s : Reg). (Handle r, Array a@s            ) -> Array@r a@s
    append    : ∀(r : Reg) (s : Reg). (Handle r, Array a@s, Array a@s ) -> Array@r a@s
    replicate : ∀(r : Reg) (s : Reg). (Handle r, S64, a@s             ) -> Array@r a@s

And this is much more sensible. Also, we could a quantifier for "type at
region" so that all of the `a@s` would become more simple. One thing
that becomes more difficult is dealing with the top region. A clever
version of `with` can solve this:

    with : ∀(r : Reg). (∀(s : Reg). Handle s -> a@r) -> a@r

Now we cannot return anything in the top region. The top region still
poses difficulties. Casting to it lets us forget about `s`, which lets
values escape, which is bad. It's not essential, but without it, type
signatures pick up a bunch of baggage. Look at `append` with a top region:

    append : ∀(r : Reg) (s : Reg) (t : Reg) (u : Reg).
      (Handle r, Array@u a@s, Array@v a@s) -> Array@r a@s

No one wants to see that. The following restrictions are needed:

1. Region variables cannot be instantiated with ⊤ (as before)
2. Regions appearing in type arguments cannot be upcasted. Or maybe
   upcasts should be subject to restrictions but not prohibited entirely.

The second restriction means that

    v : Array@r S64@0
    typechecksAgainst(v, Array@⊤ S64@0) === true
    typechecksAgainst(v, Array@⊤ S64@r) === true

But

    v : Array@r a@s
    typechecksAgainst(v, Array@r a@⊤) === false

One question that remains:

    v : Array@r a@s
    typechecksAgainst(v, Array@⊤ a@⊤) === ???

It would be nice if this were accepted. It should be possible. I don't
know how annoying it will be to make a typechecker do this.

Let's look at an example:

    findSmall : (Handle r, Array S64) -> Array@r S64
    findSmall(h,src):
      result = withArray(src) => {0}(srcVec,len):
        finish(count : S32[0..]) -> (Array@r S64):
          compare count 0:
            EQ: emptyArray
            GT:
              result = withNat(count) => {1}(countNat):
                dst = initializeVector{r}{^1}(h,countNat,0s64)
                loop*(ix : Fin ^1) -> (Array@r S64):
                  ... // similar to the other loop, but we have no proof
                  ... // about whether or not the count actually matches
                jump loop(...)
              return result
        loop*(ix : Fin ^0, acc : S32[0..]) -> (Array@r S64):
          value = index{^0}(src,ix)
          accSucc = add(acc,1)
          accNext = ifLessThanThenElse(value,42s64,accSucc,acc)
          case-predecessor(ix):
            None: jump finish(accNext)
            Some ixNext: jump loop(ixNext,accNext)
        case-nat-to-fin(len):
          None:
            return emptyArray
          Some lenPred:
            jump loop(lenPred,0)
      return result

Notes:

* This scans the source array twice. The benefit of doing this is that we
  can figure out exactly how much memory needs to be allocated. Returning
  part of an overallocated array is not generally possible with bump
  allocators. Or at least it's not very reliable with regions. Another
  way to do this is to create the destination array in a temporary region,
  give it the same length as the source array, and then copy it into the
  source region handle when finished.
* The primitives `withNat` and `withArray` introduce control-flow join points.
  It should be possible to allow these in terminal positions instead of only
  as the RHS of a binder. 
* The debruijn level introduced by `withNat` is 1 because `withArray`
  took level zero. These variables representing natural numbers are used
  in types, and the user has no control over them. This means that shadowing
  cannot happen. In function definitions, I am not sure if I want to use
  numbers or names.
* The asterisk after a join point's name indicates that it is defined
  recursively.
