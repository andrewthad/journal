# Types of Array Ops with Uniqueness

Let's start with the system that I ended with on 2024-05-10:

    write    : Array u a @ 1 -> Int -> a @ u -> Array u a @ 1
    index    : Array * a @ * -> Int -> a @ *
    newArray : (Int, a @ *) => Array * a @ 1
    run      : (Unit => b @ *) -> b @ *

The array type constructor tracks both its element type and the uniqueness
of the elements. There are two drawbacks to this:

1. It is cumbersome to type this additional information
2. The type of nonunique arrays of unique elements (`Array 1 a @ *`) exists,
   but it cannot be used.

Problem 2 manifests itself when we try to freeze arrays. Contracting a unique
array (using it more than once) should be straightforward, but we end up
having to deal with the uniqueness annotation for the element as well. We
would like to be able to contract the whole thing all at once. Let's see if
plain old subtyping is safe here. Suppose we have:

    Array 1 a <: Array * a

This means that we can convert an array of unique elements to an array of
nonunique elements any time we want. Does this cause problems?

    foo(xs : Array 1 Foo @ 1):
      ys : (Array * Foo @ 1) = xs
      ...

Here, we can still use `xs` in the rest of the body of the array, but `xs`
is not unique, so we cannot use `write`. All functions that operate on
non-unique arrays are only able to work on nonunique elements. So it seems
like this is sound. It is an extension of the ability we usually have
to forget that something is unique, but it differs in two ways:

1. It is a "deep" operation, discarding information about more than just
   one value.
2. Use analysis is not used. This is plain old subtyping without any
   substructural rule.

It's strange that this works, but I really do think that it does.

# Unique Pairs

Let's try pairs quickly:

    Pair : Uniqueness -> Uniqueness -> BaseType -> BaseType -> BaseType
    makePair : a @ u -> b @ v -> Pair u v a b @ 1
    fst : Pair u v a b @ u -> a @ u
    snd : Pair u v a b @ v -> b @ v

I think this works, but it's awful to look at. Basically, we can always
create a unique pair from any two values. When we get either of them back
out, we require the uniqueness of the pair to match the uniqueness of
the element because a unique element inside a nonunique pair is not really
unique. Again, I think this is much too complicated.

# Cons Lists

Here's an overly restrictive attempt at cons lists:

    List : Uniqueness -> BaseType -> BaseType
    cons : a @ u -> List u a @ u -> List u a @ 1
    fold : (b @ v -> a @ u -> b @ v) -> b @ v -> List u a @ u -> b @ v

Two takeaways from this:

1. This is overly restrictive. It is not capable of representing a unique
   list of nonunique elements.  
2. If we weaken the result of `cons` from 1 to `u`, we can see that the
   uniqueness to `List` should always agree with its own annotation. This
   suggests that, especially for the benefit of tree-like structures, it
   should be possible to reference an "inherited" uniqueness.

Using this second idea, here is another presentation of `List`:

    List : BaseType -> BaseType
    cons : a @ u -> List a @ u -> List a @ u
    fold : (b @ v -> a @ u -> b @ v) -> b @ v -> List a @ u -> b @ v

This tightens things a little, and it will be useful for ordered maps.

Let's return to the problem of being overly restrictive. The current
presentation of lists prohibits us from having unique lists on nonunique
elements. For lists, the uniquenesses of the spine and the elements should
agree to some extent. Here are the possibilities:

* Good: Spine unique, elements unique
* Good: Spine unique, elements nonunique
* Good: Spine nonunique, elements nonunique
* Bad: Spine nonunique, elements unique

Arrays don't have spine uniqueness because their spine is fixed at the
time of their creation, so this probably didn't show up there. This
problem is well understood in literature that deals with uniqueness types.
Clean solves this by having attribute variable inequalities, and it manages
to keep this hidden from users in many cases by propagating attributes
(i.e. uniqueness information) upwards through type constructors. As evidence
of just how effective this can be, consider that StdList.dcl from Clean's
standard library does not have any attribute inequalities written out
explicitly. Still, this approach is heavy handed. It's exactly what the
authors of Uniqueness Typing Simplified were trying to avoid. Their
solution requires exposing operators for the conjunction (or maybe disjunction,
I've forgotten) of uniqueness variables to the end user. The theory feels
more simple that what Clean does, but I'm still not fond of it.

Another solution is to cram the information into the element type using
something like Linear Haskell's `Unrestricted` type constructor.
This lets us reuse the formulation that I suggested above. For example:

    Wrap : BaseType -> BaseType
    wrap   : a @ * -> Wrap a @ 1
    unwrap : Wrap a @ * -> a @ *
    consPerson : Person @ * -> List (Wrap Person) @ 1 -> List (Wrap Person) @ 1
    consPerson p ps = cons (wrap p) ps

This works (just like it does for Linear Haskell), but I do not like it for
all the same reasons that I do not like it in Linear Haskell. It's an
awkward construction, and `Wrap` has to be introduced and eliminated
every time we mess with the elements in the list. And on top of that,
if we ever decide to forget that a list of people is unique (and use it
in a nonunique fashion), now we've got a useless `Wrap` around our element.
 
A different solution would be to have types that simply ignore rules for
uniqueness checks. For example, if `Person` were defined this way, then
a unique reference to a person would not actually be unique. This is
a simple solution, and primitive numeric types already behave this way,
so that machinery would already exist in the compiler. But it would make
it so that we could not use uniqueness model a mutable person.

Let's try something else:

    List : Uniqueness -> BaseType -> BaseType
    consX : a @ 1 -> List 1 a @ 1 -> List 1 a @ 1
    consY : a @ * -> List * a @ * -> List * a @ *
    consZ : a @ * -> List * a @ 1 -> List * a @ 1

Maybe consolidate slightly but without any redundancy:

    List : Uniqueness -> BaseType -> BaseType
    consA : a @ u -> List u a @ 1 -> List u a @ 1
    consB : a @ * -> List * a @ * -> List * a @ *

The first one is for pushing an element onto the head of a unique list.
The elements may be unique or nonunique in this case. The second one is for
pushing an element onto the head of a nonunique list. The elements must all
be nonunique.

Having two different functions seems like an odd solution. The way to evaluate
whether or not this is a good idea is to examine functions for operating on
lists (and other tree-like structures) and figure out how common it is for
them to be polymorphic in the spine's uniqueness. Cons is, but it might be
an uncommon case. Consider insertion into an ordered map. Inserting into
a unique map and a nonunique map is simply not the same thing. They cannot
share an implementation. So we have:

    insertA : Key @ * -> a @ u -> Map u a @ 1 -> Map u a @ 1
    insertB : Key @ * -> a @ * -> Map * a @ * -> Map * a @ *

Notice that we can still freeze a mutable map at no cost whenever we want to.
That is:

    Map 1 a <: Map * a // Subtype captures freezing the elements

Let's look at a few other cons-list operations:

    fold : (b @ w -> a @ v -> b @ w) -> b @ w -> List v a @ u -> b @ w

Notice that the list argument has polymorphic uniqueness. We have to use 
a type variable here (rather than star) because the uniqueness is inherited
by the spine. Also notice that a single implementation is sufficent for
all lists. When we have `u ~ *` and `v ~ 1`, the type for the argument list
has no inhabitants. So the function could just crash in this case because
it is not possible to even call it. Anything that consumes a list without
returning a new one will work similarly:

    and : List * Bool @ u -> Bool @ 1
    sum : List * Int  @ u -> Int  @ 1
    -- Thinking about this more on 2024-05-14, I would suggest that the
    -- uniqueness variable be removed. The list should just be nonunique.

In both of these, primitive types appear, so in argument position, we make
them nonunique, and the result position, we make them unique. This is not
required but it reduces the number of type variables present. Also notice
than in all of the reductions, when `u ~ 1`, the function deallocates the list
as it traverses it. Another list function:

    filterA : (a @ * -> Bool @ *) -> List * a @ 1 -> List * a @ 1
    filterB : (a @ * -> Bool @ *) -> List * a @ * -> List * a @ *

Filtering does not allow the element to be unique. This is because the
element is passed to the predicate and may also appear in the result.
Even Clean has this restriction:

    // Note: this is Clean's syntax
    filter :: (a -> .Bool) !.[a] -> .[a]

Critically, `filter` has to be broken into two functions. This is because
it uses `cons` in its implementation. Or at least, for nonunique (immutable)
lists, it does. For unique lists, `filter` can just reuse the cons cells
that were part of the argument. In this case, even if we could have had a
unified implementation, we wouldn't want it. The function just needs to
be written differently. I think this is decent evidence that spine-polymorphic
construction is not a critical feature.

Let's try going slightly further. Haskell has a `mapMaybe` function that
looks like this:

    mapMaybe : (a -> Maybe b) -> [a] -> [b]

With uniqueness, we have a few options:

    // First implementation cannot reuse cons cells because the output
    // element might have a different type
    mapMaybeA : (a @ u -> Option b @ v) -> List u a @ 1 -> List v b @ 1
    mapMaybeB : (a @ * -> Option b @ v) -> List * a @ * -> List v b @ 1
    // Third implementation is monomorphic and does not allocate cons cells
    mapMaybeC : (a @ u -> Option a @ u) -> List u a @ 1 -> List u a @ 1

This is similar to the variants of `insert` defined for maps earlier. There
are two differences worth highlighting:

1. The third variant is neat. This additional opportunity for reuse appears
   when we construct a new list with the exact same element type.
2. In options A and B, we get to choose the elements output type. This choice
   is free since we are allocating a brand new (unique) list.

# Note About Uniqueness Typing Simplified

I was reading Uniqueness Typing Simplified again, and this stood out:

> But there is a catch. As we saw in Sect. 4.2, functions with unique elements
> in their closure must be unique, and must remain unique: they should only be
> applied once. In Clean, this is accomplished by regarding unique functions as
> necessarily unique, and the subtyping is adjusted to deal with this third notion
> of uniqueness: a necessarily unique type is not a subtype of a non-unique type.
> Hence, we cannot pass functions with unique elements in their closure to dup.

There's even a footnote in Linearity and Uniqueness An Entente Cordiale about the
same problem:

> A similar problem arises from the application of unique functions, and this has been
> a thorn in the side of developers of uniqueness type systems for some time. The
> solution applied in Clean is that any function with unique elements in its closure is
> "necessarily unique", meaning it cannot be subtyped into a non-unique function and
> applied multiple times. Handily, this coincides with the notion of a linear function,
> which is why our calculus having a linear base also avoids this problem.

There appears to be a commonly encountered problem in uniqueness type systems
where lambdas closing over unique elements have to get some kind of special
treatment. Intuitively, this makes sense, if a lambda closes over anything that
is unique, then applying that lambda twice would cause each application to
believe that it possessed a unique reference to something, but this belief
would be mistaken. So, instead, we say that the lambda can only be applied
at most once, and everything is fine.

Fortunately, in the language I'm thinking of, functions are not even first
class, so having them be treated differently is fine.
