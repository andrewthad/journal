# Regions in a Late-Stage IR

What about adding region types to an IR in a very late stage? That is,
post monomorphization. If we are doing prompt reference counting, this
could be a pretty helpful optimization. It gives us a principled way to:

1. Bulk deallocate (no fiddling with free lists)
2. Avoid touching reference counts when we do not need to 

When might this be useful? Consider:

    foo = function(...):
      x = makeBigStructure(...)
      result = analyzeTheData(x)
      result

The value returned by `makeBigStructure` is probably unique, but
`analyzeTheData` does not require uniqueness. Even without regions,
this can profitably be rewritten as:

    foo = function(...):
      x = makeBigStructure(...)
      result = borrowThenDiscard (x as y):
        analyzeTheData(y)
      result

How helpful is this? Possibly only a little helpful. Since `borrowThenDiscard`
knows that the unique value is discarded after the computation, it can
deallocate without looking at reference counts. The original presentation
of the function was less clear about this. Effectively, `x` was upcasted
to a nonunique value, so deallocation has to check reference counts first.

Since we are playing the "keep the whole structure live" game, regions feel
like a nice addition. Since `makeBigStructure` returns everything that it
allocates, it is possible to build a variant of it that allocates into a
region:

    foo = function(...):
      withRegion r: 
        x = makeBigStructureIntoRegion(r, ...)
        result = analyzeTheDataInRegion(r, x)
        result

There is no longer any benefit in `borrowThenDiscard` because deallocation
works the same way for both unique and nonunique objects in a region. We
have to build a special version of `makeBigStructure` and of `analyzeTheData`
for this to work. This feels tricky. Maybe there is a way to just have
one implementation of `analyzeTheData`. After all, we do not actually want
to have reference counts updated by it ever.

For that to be possible, we would need to do something like this:

    analyzeTheData : r -> BigStructure{r} -> MyResultType

And `r` can be either a region or the global heap. The trickiest part of
this is that when something in the global heap goes out of scope, we want
to decrement a reference count, and when something in a region goes out
of scope, we do not want to do anything. Because of this, we need completely
different allocation strategies for in-global-heap types and in-region types.

It is, in certain cases, possible to treat the global heap and a region
as though they were the same thing. That's what `analyzeTheData` is in the
above example. Even if `BigStructure` is on the heap, we know that the RC
for it cannot drop to zero until the function exits. If nothing from it
escapes, then we should pretend that the existing global heap was actually
a region and that all of the objects in the existing global heap are objects
in that region. This transforms the program into one where the types make
it apparent that RC adjustments should not be happening.

But what does this actually look like. We need something like this:

    analyzeTheDataWorker : r -> BigStructure{r} -> MyResultType
    analyzeTheDataWrapper : BigStructure -> MyResultType
    analyzeTheDataWrapper(s):
      pretendGlobalHeapObjectInRegion(s)(r):
        analyzeTheDataWorker(r,s)
    foo = function(...):
      // Object x lives on the global heap
      x = makeBigStructure(...)
      analyzeTheDataWorker(x)
      result

And then the worker inlines:

    foo = function(...):
      // Object x lives on the global heap
      x = makeBigStructure(...)
      pretendGlobalHeapObjectInRegion(x)(r):
        analyzeTheDataWorker(r,x)
      result

The function `analyzeTheDataWorker` does not need to mess with RCs on any of
the objects in the region. So that gets us halfway there. Getting a compiler
to figure out when to upgrade `makeBigStructure` is more difficult though.
This is because reading is more uniform than allocating. So we really do just
need to end up with two versions of the function. We could fix this by
increasing the uniformity. We need region allocation to set the RC to one
(just like heap allocation would do), and we need region allocation to
use the same free-list strategy as heap allocation (instead of bump allocation).
Then, we could have:

    makeBigStructure : (r : Region) -> ... -> BigStructure{r}{Unique}

This does not result in region allocations with the best performance, but
it does give us the best code reuse. Now we would have:

    makeBigStructureWorker : r -> ... -> MyResultType{r}
    makeBigStructureWorker : ... -> MyResultType
    makeBigStructureWrapper(...):
      makeBigStructure(GlobalHeap, ...)

Now inline where we left off:

    foo = function(...):
      x = makeBigStructureWrapper(GlobalHeap, ...)
      pretendGlobalHeapObjectInRegion(x)(r):
        analyzeTheDataWorker(r,x)
      result

I'm still not able to see a simple mechanical transformation to close the
remaining gap. We want to end up with:

    foo = function(...):
      withRegion(r):
        x = makeBigStructureWrapper(r, ...)
        result = analyzeTheDataWorker(r,x)
        result

Maybe if we think of applying a function to a region argument as `applyRegion`,
then we could say:

    pretendGlobalHeapObjectInRegion(applyRegion(GlobalHeap, ARGS) as x)(r) { BODY }

Can be transformed to:

    withRegion(r):
      x = applyRegion(r, ARGS)
      BODY

This does not always work. It only works if the function only returns objects
in the heap that it was given. But that's a somewhat reasonable restriction
to place on functions. We could just make every function that needs to allocate
accept a region argument. This is not quite right though because some functions
should only be allocating into the global heap (because the objects end up
going out of scope and if that happens in a region, we would leak memory).

# Conclusions

I think that it is acceptable keep regions out of the high-level language
that users typically write code in. Regions could be part of a monomorphic
late-stage IR as a way to ensure that compiler optimizations are sound.
Additionally, this late-stage IR could be targeted by code-generation tools
(decoders for json, protobuf, etc.) that have complete knowledge of the
life cycle of objects in a small part of a program.

I do not have a plan for converting programs that use global-heap objects into
programs that use a mix of global-heap objects and in-region objects.
Here are the tensions that I have identified:

* It is common to bump allocate into regions because this is the fastest
  way to allocate. However, allocating this way makes code reuse impossible
  in functions that allocate.
* We do not need to track RCs of region-allocated objects. In the right
  situations, we can take advantage of this by pretending that
  global-heap-allocated objects are in a region. This is an odd thing
  to do though. And we cannot allocate into the fake region.

I think there is some hope for this idea. But it needs more thought.
Possible simplifications of the idea:

* Objects are still reference counted when they are in a region. The
  only performance advantage we get is that when the region goes out
  of scope, we get to bulk deallocate instead of walking the children.
* Do not update RCs promptly. This causes everything to be dealt with
  more uniformly. For example, we can bump allocate everywhere.
  Deferred RC is more complicated to implement though.
* Do not support compiler optimizations that convert global-heap-allocated
  objects to region-allocated objects. Only support code that uses regions
  in tools that compile DSLs. If we do this, we can ignore all the
  code-reuse problems.
