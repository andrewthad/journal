# Creating and Reusing Regions

I started thinking about how region allocation should be implemented.
There are several types of regions:

* Regions that are not frozen. There are two possible contexts:
  * Reentrant. Recursion (or mutual recursion) may cause the same region
    allocation point to be reached twice.
  * Nonreentrant.
* Regions that will be frozen (and reference counted)

Regions that are not frozen behave more predictably. Let's focus on them
first. If every function were nonreentrant, things would be simple. Every
region could just be its own global variable (or a thread-local global
variable if we are worrying about threads) that communicated what address
range the region used. And that's it. To get a mild improvement, we could
statically analyze regions and figure out whether or not they overlap.
If two regions do not overlap, they could share the same global variable.

Reusing the same virtual memory every time the same region is allocated has
one big advantage. The regions are likely to end up sized correctly. If
a region has a statically known bound on its size, it should just live on
the stack. But if it doesn't, we want to have a giant contiguous area outside
of the stack. Most of the pages in this area are unmapped. As we allocate
objects into a region, we want to back the pages one-by-one with a userfaultfd
handler. But when a region is freed, we do not want to unmap all of these
pages. Unmapping has a cost, and we are likely to need to remap them again
in the future. Since we want to leave the mapped pages as they are, we should
choose a computation that is like to require that same number of pages. And
the best candidate is the same computation that ran last time.

There is a question of whether or not we should ever give memory back
to the kernel. We can, but it's hard for users to reason about this because
it's somewhat unpredictable. If we backed each region by noncontiguous blocks,
this would be much easier. But then the allocator would have a larger code
footprint in each function. The advantage of the userfaultfd approach is that
the hot path is a bump allocator that assumes that it has unlimited memory
available. The disadvantage, it seems, is that the pages used for one region
cannot be cheaply redistributed to other regions. I think that I need to just
accept this as a disadvantage. It seems impossible to mitigate. At safe points,
where we know that a region is not in use (like when a thread finished),
we could reclaim memory. But this should probably be directed by the user.
Having the runtime make guesses about this is going to lead to bad things.

Back to the next cases: recursive (and mutually recursive) functions. Now we
are in trouble. Nonrecursive functions can be handled as they were before.
Maybe each recursive function should track how many times it has been entered,
bounded at 64 or 128. Each entry is for a separate contiguous virtual address
range. The disadvantage of this is that it would be easy to run out of virtual
address space. We could instead just have virtual address space be based on
depth in the call stack, capped at 128. The disadvantage is that,
depending on how many times a function recurses, all the regions created
further down in the call stack (even by non-recursive functions) will be
backed by different virtual address ranges.

There is an even more simple strategy that does not involve tracking call-stack
depth. Just have an array of 128 pointers virtual address spaces. Every time
you created a region, taking the pointer for your virtual address space and
then and then increment the index. Every time you free a region, decrement
the index. We could even give all non-recursive functions there own little
special regions as global (or thread-local) variables.

I guess that, in a function that can recurse, the depth is unlikely to matter.
Ideally, you shouldn't create a region and then recurse. If you do, it's
probably just to give the immidiate child a place to allocate into. It probably
does not feed it further down. And if it doesn't, then at that point, you could
sort of "pause" the region and reuse it. This is interesting idea. In fact,
it even works pretty well for nonrecursive functions. Let's pursue this further.
Consider a function:

    myFunc : Foo -> Bar
    myFunc(foo):
      {r}, hnd = region
      p = allocate{r}(hnd, Person)
      result = otherFunc{r}(p)
      q = allocate{r}(hnd, Person)
      free{r}
      ... do something with result

When we call `otherFunc`, suppose that it needs to create a fresh region.
Since there are no handles to region `r` in scope, we can trivially
reuse it at the current position. The real question is how is this supposed
to actually work? Meaning, how does this opportunity get discovered? I think
there needs to be some static analysis. It might even be possible to use
the type system. We want to end up with something like this:

    myFunc : Foo -> Bar
    myFunc(foo):
      {r}, hnd = region
      p = allocate{r}(hnd, Person)
      readonly{r}
      result = otherFunc{r}(p)
      readwrite{r}
      q = allocate{r}(hnd, Person)
      free{r}
      ... do something with result

That is, we change the capability to make `r` readonly before making the
function call, and then we make it restore it to read-write. What is happening
at runtime? The regions need to be stored in some kind of set (maybe a simple
bitset of 64 bits works). When we mark a region as readonly, we put it back
in the set. No, this does not work. Because then we could create a new region
after calling `readonly`, and if it outlived the call to `readwrite`, we would
be in trouble. We need to establish that a region dominates another region.
This is actually similar to the parent-region-child-region thing I've explored
(where the parent has to outlive the child) on 2024-10-14, but it differs in
these ways:

* We must not allocate into the parent region while the child region
  is live. In theory, this could be accomplished by revoking the parent
  capability, but that would prevent reading from the parent.
* We do not need to be able to upcast objects from the parent region into
  the child region.

It's not difficult to create primitive that accomplish these goals. It looks
like this:

    sleep : Function()(Sleep r {1}) {-r}
    wake  : Function(Sleep r {1})() {+r}

This is overly restrictive since it makes it impossible to read from `r`,
but it's ok. What if, instead, every region was in a "can allocation happen"
state at every point in time. Now we would have:

    sleep : Function()(Sleep r {1}) {r => RW}{r => RO}
    wake  : Function(Sleep r {1})() {r => RO}{r => RW}

We might not even have to thread the `Sleep` token through since there is
no longer any danger of destroying a region through misuse. Now let's
think about the type of `otherFunc`:

    otherFunc : {r : Region}[r => RO](Person@r) -> Result

Now the function communicates, through its type signature, that it cannot
allocate. We may be able to do away with handles entirely. Now, how can we
stack the next region to the right of all the data in the read-only region:

    adjacent : Function()(exists c. Adj p c {1}) {r => RO}{r => RO, c => RW}
    freeAdj  : Function(Adj p c {1})() {r => RO, c => RW}{r => RO}

The intent is that we have to put a region to sleep, request an adjacent
region, use the adjacent region, free the adjacent region, and finally
wake up the original region. But we cannot actually enforce this ordering.
It's possible to wake the original region before freeing the adjacent region.
It feels like I'm getting closer to something that works though. One idea
that I'd like to return to is the original formulation of `sleep` and `wake`.
This is very simple, and it works. It's only drawback is that it makes it
impossible to read from the object while the region is asleep. Originally,
I had thought that this flaw was too serious for this to be used. After
all, it prevents `otherFunc` from reusing the region allocated by `myFunc`.
But it doesn't prevent that region from being reused entirely. If `otherFunc`
calls some other function that doesn't require the capability for `r`, it's
possible to put `r` to sleep before that function call, and it might get
reused there.

Let's consider the most serious problem: recursive parsers and decoders.
The simple formulation of `sleep` function might fix this. Let's consider
a JSON parser:

    fn decode{r,s}[r,s](Handle(r), Array(U8)@s)(Array(U8)@s, Object@r)
    as (hnd, input):
      token = head input
      when token -> 't':
        input' = dropLeadingOrCrash(input,"true")
        return (input', Json.True)
      when token -> '[':
        t, thnd = region
        buffer = allocate List{t, Object@r}(thnd).Nil
        loop*(input0, buffer0):
          listElemToken = head input0
          when listElemToken -> ']':
            input1 = drop(input0, 1)
            result = reverseListIntoJsonArray{t,r}(hnd, buffer0)
            return result
          when listElemToken -> ',':
            # Sorry, this is incorrect for the first element, but I'm
            # just trying to illustrate.
            input1 = drop(input0, 1)
            proof = sleep(t)
            input2, obj = decode{r,s}(hnd, input1)
            wake(t, proof)
            buffer1 = allocate List{t, Object@r}(thnd).Cons{tail = buffer0, head = obj}
            jump loop(input2, buffer1)
        jump loop(input, buffer, 0)

Here, we use region `t` to hold a single value, a buffer. In the example, this
buffer is a cons list. But we could make a special region that holds a single
growable array. Such a region would have no associated handle, and the array
itself would support just two operations: push and freeze. This special region
could, however, still be put to sleep.

So this does resolve a problem with recursive decoding. But it's still not
clean enough. The huge issue I still see is that we still cannot guarantee
that a thread that reuses a subregion will get destroyed before the parent
is woken back up. So putting a region to sleep doesn't really entitle us
to reuse it.

I know how to fix this:

    sleep : Function()(Sleep caps r {1}) {-r, caps}
    wake  : Function(Sleep caps r {1})() {+r, caps}

The sleep token captures the entire capability context at the time that the
region was put to sleep. This means that waking the region requires that
there are no new regions available. Let's see if we can cheat and break
this system:

    prf0 = sleep r
    x, xhnd = region
    prf1 = sleep x
    wake{r}(prf0) // wake region r back up, this succeeds
    wake{x}(prf1) // wake region x back up, this will fail

We cannot wake `x` back up because `r` is in scope. If we try
to destroy `r` and then wake up `x`, that will also fail to
typecheck because the `Sleep` proof did not get consumed linearly.
As we would expect, we have to destroy region `x`, and then we can
wake `r` back up.

# Thing That I Realized About Child Regions

I had forgotten about a tricky interaction between child regions and
compressed pointer representations. If we want to use only 32 bits for
pointers, we have to make sure that child regions allocate into the same
32GB virtual memory as the parent. I think the easiest way to get this
guarantee is to cap parent regions at 24GB and child regions at 1GB.
That way you could chain up to 8 child regions, and then they would
fail after that. But I don't think that child regions are needed very
often anyway. My example from 2024-10-14 was that I wanted to be able
to reference slices of the input when decoding a byte array to a JSON
object. Ultimately, we want to turn the JSON syntax tree into something
else. When we do, we are probably ready to get rid of the string, but
maybe not. If we do it this way, the two regions can each have handles,
and the handles will not interfere with each other. And we can still
free the child region. One drawback is that it's kind of difficult to
reuse the space in the child region. That is, it only gets reused as
another child region for the same parent region.

Or maybe I should just make pointers cost 8 bytes instead of 4 bytes.
That would also solve this problem.
