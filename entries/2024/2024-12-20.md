# Allocator Prior Art

The zig programming language explores the idea of functions parameterized
by allocators. This is also possible in C and rust, but it is a less common
pattern in their ecosystems. Roughly, you have something like this:

    data Allocator
      alloc : (u64) -> ptr
      free  : (ptr, u64) -> ()

You can pass an allocator around to functions, allocate memory, and then free
that memory, but you have to use the `free` from the same allocator that you
used `alloc` from. For example:

    fn foo(a : Allocator):
      x = a.alloc(100)
      a.free(x,100)

Something like that. Note that, the way I've illustrated this here, the user
is responsible for tracking the size of an allocation. Also, in this example,
the implementations of `alloc` and `free` are almost certainly not just
top-level functions that close over nothing. The close over some common
context, which makes them compatible with one another. It's possible to
disallow closures by adding existential types:

    data Allocator
      env   : Type
      alloc : (env, u64) -> ptr
      free  : (env, ptr, u64) -> ()

Basically just closure conversion.

Zig's standard library neatly illustrates that allocators can be composed
somewhat. The most simple allocator is `std.heap.page_allocator`, which just
calls `mmap` to request memory from the kernel. The bump allocator
`std.heap.ArenaAllocator` takes another allocator as an argument, which makes
sense. A bump allocator needs something to hand it larger blocks of memory,
and then it can cut these up by bumping into them.

# Analysis

This approach to allocation seems cool, but it has some drawbacks.

One drawback is performance. Consider that a bump allocator is typically super
cheap. It's so cheap that you can just inline the code because it's only
a few instructions. Even for the slow path, it's possible to make the callee
responsible for preserving all the registers so that you do not bloat the
calling context. I explored this on 2024-11-08. If allocation is hidden
behind an indirection, it's not possible to inline the common case, which
reduces performance. You also lose the ability to coalesce bumping.

The other drawback is allocator-agnostic code cannot assume anything about
the allocator. A function that assumes an arena is written differently from
a function that assumes some kind of fine-grained memory tracking. The biggest
difference is that arena's cannot free individual allocations.

So I don't feel like this is a good path to pursue.

# Related Strategy

I'd like to combine two insights:

1. The only way to allocate memory without making a function depend on some
   kind of system is with `mmap`.
2. All allocators are built on top of `mmap`.

This suggests that a function should just ask the kernel for the memory it
needs directly and then implement its own memory management scheme on that
memory. This works, but we end up messing with virtual memory a lot.

I think the only thing that really works is to build a user-space `mmap`
variant that everything uses. But how is something like this any different
from the global allocator `malloc`? There are at least two differences:

1. The API for malloc requires that all memory returned by a `malloc` call
   be freed at the same time.
2. Malloc has to support allocations of any size.

The `mmap` wrapper that I imagine is much more like `mmap`. All allocations
are multiples of 4KB, and it's possible to free subsets of them. In theory,
this might make the bookkeeping more simple, but we still have all the
same difficulties with needing to support multi-page allocations (e.g.
a 12KB contiguous block of memory). We could set an upper bound on what
can be requested (e.g. 256MB). If we did this, it would be possible to
bound the number of bits used to track the status of allocations in each
region. For example, if we require 1 bit for each 4KB page:

* 1B => 32KB
* 64B => 2MB
* 4096B => 128MB

So an upper bound on the size of an allocation makes the simple "scan a
list of bits" strategy feasible. If we broke it apart into two tiers
instead, we could do 2MB megablocks (this is nice because 64B is exactly
one cache line), and then the top tier could track whether or not the
megablock was available. This does not work well for larger allocations
though. Because just knowing that we have "1 or more pages" available
in a megablock is not going to help us figure out whether or not we
have enough contiguous pages. So then we are back to scanning.

We could treat requests of different sizes differently, or we could
require that all requested sizes be a power of two. I don't love this
idea because it could waste a lot of memory.


