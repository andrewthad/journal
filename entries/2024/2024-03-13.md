# First-Class Mutable Arrays

First-Class mutable arrays would have these characteristics:

* they are never frozen
* they may appear as object fields
* they may appear at the top level (but not inside a module functor) but
  may only be accessed with io primops. This suggests that the type itself
  needs to track locality
* they cannot be used as type constructors because they are invariant
  in their argument

The difficultly with first-class mutable arrays is that we need to hide
them from `withArray`:

    withArray : (Int, a, MutableArray* a => b) -> b

In the callback, we do not want any outside mutable arrays (regardless of class)
to be available. But the function is first class, which means that second-class
values are stripped from the context, but first-class values stay.

We are in a conundrum. We also do not want any objects that contain mutable
arrays to be available in the callback.

GHC solves this neatly with a phantom type `s` on ST that gets associated with
each mutable array.

The paper Gentrification Gone Too Far discusses privilege lattices in
section 4.2. These seem insufficient for what I want to accomplish though.
We want something like this:

    withMutation : (Unit => b) -> b

Where the callback cannot close over any existing mutable values. But
within the callback, we need to be able to call functions that return
mutable values. For example:

    withMutation
      { m0 <= makeEmptyMap()
      ; m1 <= insert(m0,10,42)
      ; m2 <= insert(m1,10,42)
      ; ...
      }

Here, `insert` returns a mutable value. But maybe such a thing should be
allowed.

One problem is that I am trying to overload the meaning of class. I want
it to be used for several different things:

* scoping mutable arrays to be frozen
* scoping borrowed immutable arrays

The paper thoughtfully applies class to all types, even functions. But I've
already decided that I don't want class applied to integral types. And I
don't even want functions as a first-class construct. So, what if functions
were more flexible:

* First:  Close over mutable values: no,  Return mutable values: no
* Second: Close over mutable values: yes, Return mutable values: no
* Third:  Close over mutable values: yes, Return mutable values: yes

We could be even more fine grained and describe functions that cannot close
over mutable values but can return them. I am not certain that there is any
value in tracking this separately from third-class functions, so I have
omitted it.

With this system in mind, both `makeEmptyMap` and `insert` are third-class
functions. But neither of them close over mutable values (or over anything
for that matter). Maybe returning mutable arrays isn't a problem. Let's
trying tracking mutability and class separately. Second-class values are
always associated with a scope.

This isn't going anywhere. Let's try a different approach. What if functions
are annotated with information about whether or not they close over mutable
values:

    FunTy =
      { args : [Type]
      , res : [Type]
      , closeOverMut : Closure // +mut, -mut
      , eff : Effectfulness // pure (->), mut (=>), io (!=>)
      }

Now let's think about the functions from earlier (we usually skip the
`closeOverMut` annotation on top-level functions):

    withArray         : (Int, a, MutableArray{2} a =>{-mut} b) -> b
    withMutation      : (Unit =>{-mut} b) -> b
    borrow            : (MutableArray{2} a, Array{2} a ->{+mut} b) => b
    sum               : Array{2} Int -> Int
    readFromGlobalVar : Unit =>{+mut} Foo

We still have the rule that nothing can ever return a second-class value,
but we have tweaked functions. Instead of tracking whether they close over
second-class values, we track whether they close over mutable value.
Functions themselves are effectively always second class. A function cannot
return another function.

This system does something similar to what GHC does, but it does it by pruning
the context rather than by unifying phantom type variables. Here are some
other functions:

    newMutableArray : (Int, a) => MutableArray{1} a
    write           : (MutableArray{2} a, Int, a) => Unit
    read            : (MutableArray{2} a, Int) => a
    copy            : MutableArray{2} a => MutableArray{1} a
    pushAndGrow     : (MutableArray{1} a, a) => MutableArray{1} a // might alias the argument

This is pretty nice. Now, let's think about objects. Objects can be mutable
in more than one sense. One kind of mutable object is one that is being
constructed and is to be frozen (second class). Another kind of mutable
object is one that will never be frozen (first class). And a third kind of
mutable object is one that references (even transitively) first-class mutable
data. How do we want to treat objects that are not themselves mutable but
might reference mutable data? Concerning context pruning, we have to treat them
the same way we treat mutable data.

So, for both arrays and objects, we need to track:

* Mutability of top level (not used for context pruning)
* Transitive mutability (used for context pruning)
* Class (used to prevent escape) 

Especially for objects, this system is a burden for users. They will need
to decide, for each function argument, what type they want. I think that
the syntactic default for everything should be first class even though
functions that accept arrays can typically accept second-class arrays.

# The Core Idea

Just to reiterate, the core idea here is to complicate the system from
Gentrification Gone Too Far. We have class as before, but it only
controls if something can be used as a return value. So it models
scope well. Then we have mutability, which is used to decide when
certain values cannot be closed over.

It is necessary to track whether or not a lambda closes over mutable data.
If a lambda closes over mutable data, it could possible to smuggle
mutable data across a `withMutation` boundary. So closing over mutable
data must "infect" the lambda. Which is actually surprisingly similar
to the original system.

Why does this still work? The original system in the paper is more true
to functional programming. It has both first-class and second-class
functions. The system I propose here lacks first-class functions.
Arguments can be functions, but returned values cannot be functions.
Since a lambda cannot ever be returned, we do not have to worry about
it closing over other second-class constructs.

# Simplification: Only Track Mutation

We could simplify this system to only track mutation. This new system
would have no way to freeze mutable data. All of the class annotations
would disappear. Crucially, `withMutation` would still require that
its argument not close over mutable data.

# Complication: Track Mutation and Class for Functions 

I don't actually want to do this, but I think it is possible to track,
in function types, both closure over mutable values and closure over
second-class values. This would be useful if I wanted to actually be
able to pass functions around and store them in objects, but I don't.

# Restrictions with Polymorphism

What about type variables? I don't want to have multiple classes of
type variables. So what types can they be instantiated with? Certainly
not second-class types. But what about mutable types? If we allow
mutable types, then we end up with a restriction that lambdas cannot
close over any value with a variable type. We would also end up with
an odd situation with type constructors where the mutability of the
argument type would bubble up into the type of the type application.
The most simple thing is to say that type variables can only be
instantiated with first-class, immutable types.

What expressivity is lost?

* We cannot use the general-purpose ordered map type to store mutable
  cells. I do not think this works particularly well even in Haskell.
  I do not consider this a huge loss.
* Arrays with mutable elements do not work. This is a big problem.

Maybe we could have two ways to introduce a type variable? One way would
indicate that the type variable might instantiated with a mutable type, and
the other way would indicate that it is certainly not instantiated with a
mutable type. I do not like the cognitive burden this puts on the user.

We could go the other direction and say that type variables must be
assumed to be mutable. This means that if we have a value of type
`OrderedMap FileDescriptor` (where `FileDescriptor` is opaque), we cannot
close over it with `withMutation`. But there are other functions like

    modify : (Foo -> Foo, Key, Map Foo) -> Map Foo

where the callback can close over mutable values (because it cannot
perform mutation). Even this variant allows us to close over mutable
values in the callback:

    modify : (Foo => (), Key, MutableFooMap) => ()

As I think about it more, it is actually very uncommon to need to prevent
closure over mutable values. It's really just things like `withArray` and
`withMutation` where we use embed a computation that requires mutation into
a pure context.

With this in mind, it is more clear where this system is outdone by GHC's
approach. GHC's approach lets us do this:

    makeDoubleton :: MutVar s Int -> MutVar s Int -> Array (MutVar s Int)
    makeDoubleton a b = runST $ do
      dst <- newArray 2 uninitialized
      write dst 0 a
      write dst 1 b
      unsafeFreeze dst

Although not explicit in the example, there is a type variable for state
being used in `runST`. In the system I am considering, we cannot write this
function because `withMutation` (my equivalent of `runST`) removes everything
mutable from the scope. In fact, in the extension I propose to support
polymorphism, we cannot even implement

    makeDoubletonPolymorphic : (a,a) -> Array a

This is a problem. We need to be able to create arrays of abstract types.

So maybe the first approach does make more sense. The biggest difficulty
was supporting arrays of mutable elements. We could just prohibit mutable
elements in arrays. Or we could have a way to bind "possibly mutable"
types. This would only be used in function polymorphism and would not be
allowed for abstract types in modules. It could be allowed in both places,
but I am not convinced that this would be useful.

If objects can have fields that are mutable, then it only makes sense for
array elements to be mutable as well. Maybe several builtin array primitives
should support polymorphism involving mutable types, but users should not
have any way to do the same.

But for things like disjoint sets, we actual want to be able to say "this type
is (possibly) mutable and abstract".

So maybe going back to the "all abstract types might be mutable" approach
makes sense. The inability to write `makeDoubletonPolymorphic` can be
addressed by having a primitive with that behavior. In what other settings
will problems arise? Any time we create an object using mutation. This
means that for a data type with any polymorphic fields, we cannot assign
the polymorphic field inside the scoped block. No values of the correct
type will be in scope.

I think this is all fine. Before closing, I want to consider another
question? Is this, in any way, better than GHC's system. It is clear
that it is, in some ways, less expressive. Advantages of this system
include:

* Implementation advantage: No possibility of use-after-freeze. This
  eliminates undefined behavior.
* Ergonomics advantage: Abstract mutable types do not require finnicky
  plumbing of phantom type variables. In GHC, we have
  `type AbstractMutableMap : Type -> Type` where the type itself is only
  a type family so that it can accept a state token `s`.

I still have to think about IO and an equivalent of a top-level `IORef`,
but I'll worry about that later.
