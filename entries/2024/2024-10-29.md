# Who Benefits from Programming Language Design Choices?

Programming language authors like to present design choices as something
that were done with the user in mind. People are reluctant to say "I did
something because makes it easier to write the compiler". But I think this
second reason is somewhat common. Here are some design choices that make
it easier to write a compiler:

* Not supporting polymorphism. (Golang before 1.18, Hare)
* Not supporting higher-order functions.
* Not supporting higher-kinded types.
* Not supporting a module system.
* Not supporting higher-order functors.
* Not supporting type-directed overloading.
* Only supporting separate compilation. Not supporting any form of inlining.
* Not supporting namespacing. (Although I've never heard anyone argue that
  a lack of namespaces benefits the user)

It's possible to present most of these as improving the user experience
by making things more simple or more predictable. And such a presentation
is true. In particular, it tends to be easiest to reason about performance
when working with a language that is less expressive. Reasoning about
behavior is a little fuzzier (contrast dealing with strings in C against
dealing with strings in almost any other language). But a language without
any expressive features is also easier to implement than a language with
some expressive features.

I don't think PL authors try to mislead people, but there is a cognitive bias
working against them (or maybe against us, if I ever finish writing a
compiler). There is a pressure to provide a reason that isn't the real reason.
Language specifications are necessarily abstract. They should not include
information about the compiler's implementation or about the runtime.
Similarly, a compiler, used as an application, is abstract. It's an opaque marvel
that converts human-written source code into something an operating system
can run. But because it's abstract, authors don't like to give
feature-set explanations that involve "it's too hard to implement if we make
this different". It's easy to lean on "user's don't really need this" or
"it makes it hard to reason about a program's behavior" as a crutch, especially
because those things might also be true. But they might not be the real reason.

I started to think about this more after several years of on-and-off writing
hobby compilers. Doing this has made me appreciate the implementation cost
of certain features, and it's given me a great deal of symapthy for language
authors who omit expressive features.

# Benefits of Separate Compilation and Linking

Separate compilation is an old compilation technique. Decades ago, it made
it possible to compile programs with the limited memory that computers had.
Since then, the resource limitations have been lifted, but separate
compilation is still popular. It's so common that research papers sometimes
list it as a desirable feature without even justifying it. I believe that
there are two reasons to design a compiler that performs separate compilation.

The first reason, a weak reason, is that builds can be done incrementally.
The opposite of separate compilation is whole-program compilation.
A whole-program compiler rebuilds everything when a single function changes.
This matters for large programs, but it doesn't matter as much for small
programs.

The second reason, a compelling reason, is that Linux has a common format
for object files (ELF), and there are tools for working with these files.
One tool is called a linker. On Linux, we have at least three
linkers available: `ld`, `gold`, and `mold`. They have similar interfaces,
and in many cases, they can be used as drop-in replacements for one another.
What linkers offer is a target language where the positions of functions
and data are abstracted away. Commonly used linkers provide additional
useful optimizations like `--gc-sections`. There are other tools like
`readelf` and `objdump`. These give users a way to inspect an object file. 

# Uniform Representations of Data

Working on a little PL project, I was able to get this code to compile:

    fn doubleton{r : Region, a : Star}[r](Handle(r), a)(Array(2,a)@r)
    as (hnd : Handle(r), val : a):
      dst = array.new{r,2,a}(hnd,val)
      return dst

Assume we have this header file:

    struct array {
      int32_t length;
      unsigned char* payload;
    };
    
    struct opaque {
      unsigned char content[32];
    };
    
    struct handle {
      uint32_t index;
    };

It produces this C code:

    #include <stddef.h>
    #include <string.h>
    #include <stdlib.h>
    #include "runtime/lithty.h"
    struct array doubleton(size_t a, struct handle hnd, struct opaque val) {
      struct array const dst = {.length = INT32_C(2), .payload = calloc(2, a)};
      for (size_t i = 0; i < 2; i++) {
        memcpy(dst.payload, val.content, a);
      }
      return dst;
    }

As an aside, this completely ignores the region and allocates into the global
heap with `calloc`. But it works. Now, if I specialize the input program so
that the element type is `U32`, I get this code (skipping include directives):

    struct array doubletonU32(struct handle hnd, uint32_t val) {
      struct array const dst = {.length = INT32_C(2), .payload = calloc(2, 4)};
      for (size_t i = 0; i < 2; i++) {
        memcpy(dst.payload, val.content, 4);
      }
      return dst;
    }

And this does not compile. We cannot project a `content` member out of a
value of type `uint32_t`. There are two ways to fix this:

1. Lower `array.new` differently depending on the element type
2. Use `struct opaque` as the type of everything almost everywhere, and
   extract the value whenever we call a primop that requires a concrete
   type.

The second strategy results in strange C code that is difficult to read
(since the type of most identifiers becomes `struct opaque`), but it's
easier to implement. The first strategy results in more readable C code
that a compiler might optimize better, and it lets us have the type of `val`
be `uint32_t` in the function declaration. A disadvantage of this is that
it is difficult to pass closures around. Consider a function that modifies
an element of an array:

    // Aside: This only works on primitive types because the closure
    // has no capabilities. That is, it cannot allocate or modify data.
    fn modifyAt{r : Region, a : Star, n : Nat, i : Nat}[r](Array a, i < n, fn(a)(a))(Array a)

Regardless of whether we use closure conversion or function pointers, we
have to make a decision about what the function's argument and result
look like. The only option is `struct opaque`.

So what we actually need to do is, at the boundary of a function, turn
all of the value arguments into `struct opaque`. Inside the function body,
we can immidiately turn them into their correct types. We don't have to
do this same transformation for join points, just functions.

I need to think hard about how much I actually care about being able to pass
a closure to a function. I think I care, but I need to think harder.

# Bad Argument Order for Array Functions

It is already apparent to me that all of the functions that work on arrays
look strange when they are called. The problem is that the index into the
array is a type (a relevant type), not a value. The index is not likely
to be inferred, but the "less than" proof is likely to be inferred (if any
value of type `i < n` is in scope, it can be used). So this
"types then values" system is not great. Consider writing to arrays:

    fn write{r : Region, a : Star, n : Nat, i : Nat}[r](Array a @r, i < n, a)(Array a @r)

I really want to be able to do this instead:

    fn write{r : Region, a : Star, n : Nat}[r](Array a @r, i : Nat, a){i < n}(Array a @r)

And then `write` has the normal argument order. To do this, I need an
"inferred type argument" set and a separate "inferred value argument"
set at the end. And I have to be able to blend non-inferred type and
value arguments.
