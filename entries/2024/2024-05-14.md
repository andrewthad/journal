# Top-Level Unique Data

Entente Cordiale includes a rule called Necessitation that looks like
this:

      ∅ |- t : A
    -------------- NEC
    [Γ] |- *t : *A

The paper's explanation of this rule is that "values can be assumed unique
as long as they have no dependencies". Intuitively, this makes sense.

## Application: Primitive Numeric Types

The most straightforward application of this idea is that numeric literals
should be considered unique. That is, when the user writes 42, this should
be a unique S64, not a nonunique S64. The optimizer must not consolidate
duplicate occurrences of a numeric literal, but there is no reason for
the optimizer to do this anyway.

## Application: Boxed Types

We cannot create top-level unique objects because the use site would need to
inspect the entire program to figure out if it was used anywhere else. But
we can create nonunique top-level data and call a special `copy` function
to get a unique copy of it when we need it. Like numeric literals, applications
of `copy` would not be eligible for CSE.

# Why Have Unique Numbers

One question that I come back to from time to time is why uniqueness should
be a property of all types. There is another way to do things:

    Uniqueness = Unique | Nonunique
    Type   : Uniqueness -> Kind
    Int    : Type Nonunique
    Person : (u : Uniqueness) -> Type u
    List   : {u : Uniqueness} -> {v : Uniqueness} -> (a : Type u) -> Type v

In this system, types like `Int` are never unique. We could also have types
that are only ever unique. For example, a built-in mutable hash map type
might only ever be unique. Here are the advantages of doing things this way:

* Inference works better. We can write `List Int`, `List Array`, and
  `List Array*`, and `u` gets inferred in every case.
* For primitive numeric types, the unique and nonunique variant are
  indistinguishable. Giving users two ways to represent them is silly.

Here are the disadvantages:

* It becomes more difficult to use integers to implement an abstraction
  that exposes an abstract unique type.
* Upcasting to forget uniqueness is more complicated. We cannot
  turn just any unique type into a nonunique type, so we need a freeze-like
  operation (`freeze : Array* a -> Array a`). Deep freezing is a problem.
  Just to clarify, the problem is that we cannot freely take an expression
  E of type T with kind `Type Unique` and upcast it to a type with kind
  `Type Nonunique` because we do not know that a corresponding type exists.

Let's think about solutions to the second problem. Suppose that we
make this change to the system:

    Uniqueness = Unique | Nonunique
    Type     : Uniqueness -> Kind
    Int      : Type Nonunique
    Person   : Object
    Array    : {u : Uniqueness} -> Type u -> Object
    Annotate : (u : Uniqueness) -> Object -> Type u

Now we can say:

    Annotate 1 a <: Annotate * a

Which means that

   Annotate 1 (Array (Annotate 1 (Array S32)))
   <:
   Annotate 1 (Array (Annotate * (Array S32)))
   <:
   Annotate * (Array (Annotate * (Array S32)))

So that part can be dealt with.

What about the problem with abstraction? Let's add a bit a magic to help
us pretend that numbers are unique:

    Int : Type Unique // instead of nonunique

And then:

    signature Unix {
      type Fd : Type Unique
      -- | Takes a path and a callback as arguments.
      withFile : (Annotate * (Array U8), Fd => a) => a
    }
    linux_x86_64_withFile : Function(Annotate * (Array U8), S32 => Unit)(Unit)
    module UnixImpl {
      type Fd = S32
      withFile = linux_x86_64_withFile
    }

The trick to making this work is to rig the uniqueness check for primitive
numeric types. For any such type, we can just not perform use analysis
and allow multiple uses to be counted as unique. This doesn't quite work
because when we have a list of S32, we still want to be able to use functions
that work with nonunique elements.

There's got to be some way to do this. There's generally no value in thinking
of integral types as unique. It's only when we are hiding the representation
of things that this is useful. So maybe the answer is:

    Uniqueness = Unique | Nonunique | Omniunique
    Int : Type Omniunique
    // Subkinds
    Type Omniunique <: Type Unique
    Type Omniunique <: Type Nonunique

Notice that we do **not** have `Type Unique <: Type Nonunique`. We still
have `Annotate 1 a <: Annotate * a` so that we can freeze objects. Even
that's a bit strange though because it implies a subkinding relationship
that we do not actually have. Maybe the correct way to express that
is more like:

    x => Annotate 1 a
    -----------------
    x <= Annotate * a

But it seems like we still need subtyping (and subkinding) to really make
this work. This just does not work well.

# Two List Types

Here's an idea:

    ListUnique    = Functor(u : Uniqueness)(a : Base) = constructed { Cons (a @ u) (^0 @ 1), Nil }
    ListNonunique = Functor(a : Base)                 = constructed { Cons (a @ *) (^0 @ *), Nil }

The syntax is bizarre, but what this does is defines three types of lists.
These give rise to:

    consUnique    : a @ u -> ListUnique  u a @ 1 -> ListUnique  u a @ 1
    consNonunique : a @ * -> ListNonunique a @ * -> ListNonunique a @ 1

So far, we have avoided dealing with the problem of unique data inside of
nonunique data. The paper Uniqueness Typing Simplified would handle this
by writing:

    ListUnique = Functor(u : Uniqueness)(a : Base) = constructed { Cons (a @ (u + !)) (^0 @ (1 + !)), Nil }

I'm using bang for the inherited uniqueness. And plus is a semigroup operation
that combines uniqueness. Basically, this bounds the uniqueness. We do not
need syntax for this because this is always how uniqueness works on fields.
So it's implied when a data type is written. This means that if we have a
value of type

    constructed { Cons (a @ (u + !)) (^0 @ (1 + !)), Nil } @ 1

And we duplicate it, the nonunique expressions have type

    constructed { Cons (a @ (u + !)) (^0 @ (1 + !)), Nil } @ *
    ==>
    constructed { Cons (a @ *) (^0 @ *), Nil } @ *

Which is the same as the type of `ListNonunique`. The user could instead
define a single type:

    List = Functor(u : Uniqueness)(a : Base) = constructed { Cons (a @ u+!) (^0 @ !), Nil }

But this is odd because `u` is unused when the list is used immutably.

I've still not fully convinced about what to do. With builtin types like
arrays, it's a little easier because the data is guarded by primops. This
let's us sculpt the interface with primops. So we can let the user upcast
from `Array 1 Person @ 1` to `Array 1 Person @ *` and then they are just
stuck. There's nothing they can do with a value of the second type because
we do not have a primop that accepts it as an argument.
But with ADTs, users have to be able to deconstruct them. This unfettered
access to the data makes it harder. The equivalent of the array solution
would be to reject a case statement that scrutinized a value with an
unsound nesting of unique data. For exmaple:

    myList : constructed { Cons (a @ u) (^0 @ v), Nil } @ u
    case myList of { ... }

The typechecker would reject this because the combination `u ~ *` and
`v ~ 1` would create an uninhabited type. Such a restriction would mean 
that I could not define the `fold` that I suggested on 2024-05-10.
Instead, I would need to have two fold functions:

    fold : (b @ w -> a @ 1 -> b @ w) -> b @ w -> List 1 a @ 1 -> b @ w
    fold : (b @ w -> a @ * -> b @ w) -> b @ w -> List * a @ * -> b @ w

We only need these two because we only need to consider what the reducer
needs. If reduction requires a unique value, then we have to preserve
uniqueness for it. Consider another way to split this function:

    fold : (b @ w -> a @ u -> b @ w) -> b @ w -> List u a @ 1 -> b @ w
    fold : (b @ w -> a @ * -> b @ w) -> b @ w -> List * a @ * -> b @ w

The works, but there is redundancy. In this formulation, if the reducer
did not require uniqueness and the list was of type `List * a @ 1`, we
could use either of these.

In the case where there is no overlap, it's conceivable that some kind
of type-directed overloading could be made to work. Resolution would
be driven by the type of the reducer though, so this might not be that
useful.

I am beginning to believe that the scrutinee check is a reasonable solution
to my problem.

There are still issues with the list API being fragmented. It's not very
clear where the different pieces belong. For any container that's polymorphic
in its element, there will be three interesting cases:

1. Persistent (immutable) container, immutable data
2. Mutable container, immutable data
3. Mutable container, mutable (or unique) data

Sometimes, the same operation happens to work on two of these. For example,
we can write a `fold` that works on 2 and 3. And we can write a `fold` that
works on 1 and 2. But we cannot write a fold that works on all three.
At least, not with the rules that I'm considering. But I think this is ok.
If you are using a data structure to do anything, then at the minimum, you
absolutely have to know if your elements are unique (even if type of
the element is abstract).

# Refinements

I think we can get refinements into this system with:

    Refine : (b : Base) -> Refinement b -> Refined
    Ann    : Uniqueness -> Refined -> Type
