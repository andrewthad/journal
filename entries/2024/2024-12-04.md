# Reflections on Automatic Memory Management

I thought through ASAP again yesterday, and I'm convinced that it does not
adequately solve the problem that it sets out to solve. But now I'm thinking
more generally about automatic memory management, and I have some high-level
observations that I wanted to write down. First, I want to list several
dimensions along which automatic memory management schemes can be evaluated:

* Promptness: Are things deallocated as soon as they become unreachable? One
  practical consequence of this is that a prompt scheme can be used to manage
  things other than memory (like file descriptors). Prompt schemes include
  RC and ASAP. Non-prompt schemes include semispace collectors and
  mark-and-sweep collectors. Regions are somewhere in between. The user
  has to put in more effort to make them more prompt. Prompt systems tend
  to have performance issues when copying large slices of arrays. This issue
  is not explored often in the literature.
* Cycles: Are cyclic data structures supported? RC does not support them. Nearly
  everything else does. Dropping support for cycles simplifies the implementations
  of most of the others.
* Type System: Does the scheme use a type system to prevent problematic cases
  from appearing? This is what region systems do, and it's what rust (which
  uses an ownership type system) does. This is a very effective solution, but
  it has some drawbacks. It reduces the expressivity of the language. It
  burdens the user with memory management. It forces the language author
  (and the compiler author) to deal with the type system as well. It reduces
  the abstraction level offered by the language. That is, a particular model
  underlying the language is assumed. If you transpile a language to JavaScript
  or to Chez Scheme, you end up getting nothing from all the work you did to
  satisfy the type system.
* Are interior pointers supported? Specifically, can we take a pointer into
  an object (usually pointing to an arary that is inlined into the object)?
  And can we slice an array to get another array? When we slice, does it keep
  the whole array live? Again, this is typically not dealt with in the
  literature. Region systems let you slice into anything, and the resulting
  array is in the same region as the parent object. Most systems based on
  semispace collectors botch this, failing to reclaim unreachable memory
  and having separate types for slices and arrays (GHC, JavaScript, Java).
  Golang, which uses a mark-and-sweep collector lets you slice into things,
  but unreachable memory in a parent object/array is never reclaimed.

Let's consider a contrived example. It's contrived, but scenarios like this 
show up in practice:

1. We allocate a very large map and bind it to `m`
2. We pluck a value of type `T` out of the map `m` and bind it to `x`.
3. We perform a series of allocating operations resulting in a binder `y`,
   also with type `T`. It is possible that `y` aliases `x`, but we cannot
   determine statically whether or not this is the case.
4. The map `m` goes out of scope. The binder `x` goes out of scope.
   The binder `y` stays in scope.

How is this handled by various schemes:

* Semispace: In the best case, nearly every heap allocation that comprises `m`
  happens in the same from-space, and the rest of the steps do not cause enough
  memory pressure to trigger a copy. Whether or not this works out well depends
  on the size of the from-space and on the number of allocations performed in
  steps 2 and 3. It's temperamental. We might end up performing any number of
  full scans of `m` even though its certain to stay live until step 4.
* Mark-and-sweep: Similar to semispace except that we must always scan the
  map to deallocate it.
* ASAP: We have to scan the map to deallocate it. If we performed steps 2 and 3
  in a loop, we would have to scan the map every iteration of the loop. This
  allows us to promptly deallocate the memory allocated in step 3. Neither
  semispace nor mark-and-sweep collectors exhibit this kind of pathologically
  bad behavior in this case.
* RC: This has the most predictable performance. When the map goes out of
  scope, it must be scanned in full. When to kind the value in the map that
  `x` was bound to, it either has a reference count of 1 or 2 (depending on
  whether `y` aliased `x`).
* Regions: We must change the program to describe it in a language with a
  region type system. One possible transformation is to put everything in a
  single region and make step 4 copy the object that `y` is bound to out of
  the region into another region with a greater lifetime. Deallocating the
  map does never requires walking it. This is like the best-case scenario
  for the semispace collector, but it's guaranteed instead.

Although RC comes out ahead in this scenario, it has considerable overhead.
Having to walk everything in order to deallocate it isn't the worst thing
imaginable. After all, you have to visit all of those memory location just
to build the data structure, so the overhead is a constant factor.

There are several ways to defer RC. One strategy that I've explored before
is to combine RC with a nursery and then trace the nursery once it fills
up and update the reference counts. The hope is that the "most objects die
young" heuristic works out and you get your deallocations for free. It's
somewhat unsatisfying because you don't know that it's going to work, but
it might.

# Slightly Better Deferred RC

I want something a little more sophisticated than that. If we're just slowly
building up a giant ordered map that we are planning on freeing at the end,
it would be great if we never scanned it. Uniqueness types (or linear types)
seem like they ought to help us here, but I've had problems with those.
If unique data can reference non-unique data, then you need to scan anyway.
If data structure is fully unique all the way down, you get a nice no-scan
guarantee. But it's hard to deal with lookups when the values are all unique.
You have to do something weird like this:

    // The map is unique, and a is unique
    lookup : (a -> (a, b)) -> Map a -> (Map a, b)

This is terrible. No one wants to use a system like that.

Perhaps referencing a non-unique object from a unique object should pin it.
The most natural way to pin an object is to increment its reference count
immidiately. Looking at it this way, it all works naturally if we give up
on deferring and go back to a vanilla RC. Unique data just implies that
the RC because is 1. Degradation to a non-unique value works because the
reference count is already correct.

What if we could statically figure out that a group of objects should all
be deallocated together? Expressing a problem in ANF makes this kind of
analysis more difficult than it would otherwise be. I think it is still
possible though. Suppose an allocation is bound to an identifier. The binder
is written into another object and immidiately goes out of scope. We know
with certainly that the only way to reach this first object is through the
second object. But how can we use this information? Especially because
it's possible for this to stop being true. That is, this remains true at
certain program points, but it expires if we ever project the field out.

If something goes out of scope and it's the sole means of accessing a bunch
of other heap-allocated data, we could reclaim all of that data promptly.
This kind of prompt reclamation would have to trace the data structure
in order to reclaim it. Still, it seems like a pretty easy anaysis to do.
If a function returns a data structure that doesn't alias any of the
function's arguments, then we ought to be able to just free everything
immidiately when it goes out of scope. This recovers some of the promptness
of traditional RC.

But what about freeing them all without having to walk the data structure?
It's difficult to do this. The data needs to be all allocated contiguously.
This can be done, but its a tricky. We need a way for an object to indicate,
at runtime, that everything reachable through it belongs to a certain range
of memory addresses. And that address range also cannot contain anything else.
That's a tall order.

# RC and Second-Class Values

Second-class values have a nice interpretation in an RC runtime. Since a
second-class value cannot escape a function, it does not need to be
reference counted. Or at least, in the body of the function, the counters
for any second-class value can be left alone. However, since a second-class
value cannot be stored in an object on the heap, I believe that traditional
static analysis could figure out the same thing.

Second-class values are so limited that I have a hard time seeing how they
can be usefully incorporated into a language.

# Coarse-Grained Object Graph

This is an idea that I've considered in the context of RC, but it would work
in other settings too. Suppose that we allocate into blocks. Each block might
be 4KB. As we allocate into the block, we fill a shadow block that only
includes pointers. After a block is completely full, the runtime scans
all of the pointers in the shadow block and builds one or more coarse-grained
arrays of outbound references. Any internal references are not tracked in
a coarse-grained reference array. Suppose that we track outbound references
with 4KB granularity, 8KB granularity, etc. We keep going up all the way to
something like 4GB. Eventually, every block would reach a granularity where
there would be no outbound references (when the heap is viewed as a series
of 4GB blocks, there is usually only one block).

What is the purpose of this excercise? With a mark-and-sweep collector,
even assuming that we only used a single course-grained array, this would
give us a way to deallocate in bulk. If we built up an AST that all went
out of scope at the same time, their would be no course-grained references
into it. For this to work, we would need to trace the course-grained
reference graph first and then trace the fine-grained reference graph.

This only works with deferred variants of RC, not vanilla RC. In addition
to per-object counts, we would have a per-block count. The per-block count
would count the number of blocks that refer to the block. Internal references
would not be tracked. When an entire block died, all of its outbound references
would be followed and decremented. If this caused a block's inbound reference
count to drop to zero, that entire block could be deallocated without walking
all of its objects individually. When the last object in a block dies, we
have to make sure that we process the block's course-grained references
before we do follow the fine-grained references in the object. Other thoughts:

* If blocks are homogeneous, it is easier to scan them for pointers.
  We could even use gather instructions.
* If blocks are homogeneous, the course-grained reference graph can be
  cyclic, which is bad.
* If blocks are homogeneous, it might improve the effectiveness of
  coarse-grained reference tracking. Certain data types cannot refer to
  other data types, so keeping them in different arenas would help.
* This strategy does not work well if we reuse partially empty blocks.
  We need for a block to be entirely empty before we reuse it. If we
  start dumping more stuff into it, we will end up with inbound references
  who's lifetimes are extremely unlikely to coincide. We could keep wasted
  memory down with something like MESH to coalesce low-density pages.

A different idea is to pair course-grained RC on the level of blocks with a
semispace collector, but I don't actually see how this would work.

# Speedwalking

It dawned on me that if we are scanning something to perform a deep free
on it, it might be possible to speed it up if the CPU support a gather
instruction. It should be possible to look up 8 or 16 reference counts
in parallel this way. But you'd need some kind of buffer that you pushed
object pointers into, and then you could get them all at the same time.
I don't think this is a good idea. But I've also never heard it described,
so I wanted to write it down before I forgot it.
