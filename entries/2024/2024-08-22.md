# An Effect for Mutation

This is an attempt to try to improve the system from 2024-08-21
a little bit. Suppose we split functions into two types:

* `->`: Pure function
* `=>`: Functions that can mutate arguments
* `~>`: Functions that perform IO

A function that can perform IO can never be called from a pure context.
But a function that mutates things can. We just do not want the mutation
to escape. This is what GHC enforces with its `ST` type. But in the system
I'm thinking of, we have an additional way to mutate things using linear
types, and this kind of mutation is actually a pure function.

But how do we run, in a pure context, a subprogram that makes use
of the nonlinear version of mutation? Something kind of like this:

    run : (a => b) -> a -> b

But the types `a` and `b` cannot include anything mutable. How can we
enforce this? One option is by performing a dynamic check (like the
one suggested on 2024-08-21 to handle concurrency). I do not really
like that option here though.

Another option is to prohibit the callback passed to `run` from closing
over anything. This would make it so that the callback could not modify
the value of type `a`. There are no functions that can in-place update
a value with an abstract type. This does not stop us from returning
something mutable though. So it's not a solution. We could do something
extremely restrictive:

    run : (a@C => b@C) -> a@C -> b@C

Now `a` and `b` both have to live in the "constant region". This means
that if they are primitive integral types, they can take on any value,
but if they are object types, they can only have a compile-time constant
value. The constant region cannot contain mutable data. This works, but
it's way too restrictive.

We could combine these strategies:

    run : (a@r => b@C) -> a@r -> b@C

Now we can feed in lots of types as `a` but `b` is still very restricted.
We could try using the rank-2-types trick on the regions to stop the
callback from closing over things:

    run : forall {r} {t}. (forall {s}. a@s => b@t) -> a@r -> b@t

This is getting closer. What if we reintroduce the `Write` capability:

    run : forall {r} {t}. (forall {s-w}. a@s => b@t) -> a@r -> b@t

Now the callback cannot write to region `s`. And the callback cannot
smuggle in a `Write` capability for region `r` because it does not
see that value of type `a` as being in region `r`. But we could still
close over a copy of the original value in the callback and mutate
it that way. The function arrow itself needs to communicate which
regions can be modified. I've seen this in a paper before, maybe one
about Cyclone, but I cannot find it now. Here's the idea:

    run : forall {r} {t}. (forall {s}. a@r {s}=> b@t) -> a@r -> b@t

What this says is that the callback may read from any region, but it
may only write to the newly introduced region `s`. It is free to allocate
(but not to write) in any of the regions. So we end up with the ability
to create `MutableArray`s and `MutableMap`s in region `s`, but we
cannot return any of these because `s` cannot unify with `t`.

We are still somewhat limited in what we can do. We cannot store anything
from region `r` in `s`. The precludes, for example, building a mutable
map containing nonprimitive values from region `r`. We might could reference
immutable data from `r`, but this doesn't work when `a` is itself a type
constructor applied to an abstract type because we do not know if the
abstract type is mutable or immutable. This is somewhat limiting.

I need to think more about what problem this is intended to solve.
That is, how often do a need a mutable data structure (that cannot
be handled with the linear approach) in the middle of a pure computation?
Not often. In the example scenario from 2024-08-20, none of the pure
logic needed this kind of mutable types. Even decoding JSON, which needs
a map, does not need a heavy-duty mutable map. It would even be wasteful
to do that. For JSON, it makes more sense to load object key-pair pairs
into an array, sort the array by key, freeze the array, build an
immutable map from the sorted array, and then deallocate the region
that the array was in. Sorting an array of key-value pairs using the
linear-types approach is not difficult. It is a little difficult, and
this is difficult either way, to recover references that point to the
right region. To accomplish this, we have to sort something that includes
indices into the original array. When we finish sorting, we must index
into the original array to get the right references. Even slightly better,
we could just sort the original key-value array in place since it's not
yet frozen. Then we could build an immutable map from it.

Basically, what I'm actually trying to suggest here is, what if traditional
mutable data structures were only available in the IO effect? This would
let us build things like caches that live near the top level in an application.
We could not use mutable data in pure code (no read or write, although we
could pass around references). 

# Applications 

Note: I wrote this section before I decided that I don't actually want to
be able to scope mutation.

Here are some HOFs whose meanings become more clear:

    mapInPlace    : {r  }(a@r ->  a@r, MutableArray a @r) => Unit
    updateInPlace : {r  }(a@r => Unit, MutableArray a @r) => Unit
    map           : {r,s}(a@r ->  b@s, Array a @r) => Array b @s
    foldImm       : {r,s}((b@s, a@r)    -> b@s, b@s, Array a @r)    -> b@s
    foldMut       : {r,s}((b@s, a@r) {s}=> b@s, b@s, Array a @r) {s}=> b@s

This makes several functions explodes into lots of little variants. But
I think that being able to know if a function mutates an argument or not
is pretty useful.

# Conclusion

I've gotten back to a truly direct style where there are no pseudo-control
structures in the language. And all my pure functions should actually be
pure, so the compiler should have decent opportunities to improve programs.
It should be possible to group let bindings into blocks and have the order
of these not matter. (None of the RHSs can reference any of the other bound
identifiers.) Optimizations involving regions themselves are harder. Initially,
I would probably try to avoid messing with regions (leaving them anchored to
their introduction sites). Maybe first, I could try removing an unused region.

