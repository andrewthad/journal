# Advantages of RC (And of Nonmoving GC)

Consider two ways in which we can measure the amount of memory consumed
by a program:

1. Total number of bytes
2. Total number of pointers

And let's define a related metric "pointer density" (I made this term up)
to mean pointers divided by bytes. A general technique for improving the
performance of a program that uses a GC is to reduce the pointer density.
This might be done by swapping out language-native objects/structs for
a byte array. The resulting code might use indices into the byte array
where pointers had previously been used.

This program transformation results in large byte arrays. Copying collectors
are at an enormous disadvantage when working with large byte array.
A nonmoving collector needs only to visit the byte array, and the cost
of visiting is constant. Visiting a 16-byte array has the same cost
as visiting a 1GB array. But copying collectors pay much more for the
large array because the have to copy it. So the cost scales up linearly
with the size of the array. For this reason, GHC allocates large objects
differently. They are pinned, and they do not get copied during GC.
I suspect that most practical copying collector would need to be equipped
with a nonmoving collector used for large objects (and probably for the
old generation if the collector is generational).

Here are two ways to think about this:

* A purely copying collector does not see the full benefit of non-native data
  structures in the same way that a nonmoving collector does.
* Purely copying collectors will struggle to scale to enormous (64GB+) heaps.
  This means that only hybrid copying collectors will scale up, which means
  that practical copying collectors are necessarily more complicated than
  nonmoving collectors (because a practical copying collector will include
  a nonmoving collector).

# Context

For slightly more context, what I'm really thinking about is what kind of
automatic memory management I want for a programming language. Ideally,
there would be a small number (maybe a thousand) of objects managed by
the collector. Each object would be sort of like a region. It would be
responsible for managing itself. Here are the properties of such a system:

* We lose some reuse. For example, if we have a json syntax tree as a single
  object, we cannot cheaply extract a subtree from it. This defect is shared
  with any system that has regions.
* Tracing is super cheap because there are not that many objects.
* Objects can reference other objects.
* Most objects have a single "root" for access.

How can we represent maps? If the keys and elements are some primitive type,
it's not too bad. Each map is its own region, and when we finish with the
map, we free the region. Empty maps and small maps are tricky, but I think
that we just need to make sure that we have a way to create small regions
that get a lot larger over time.

But if the elements are pointers (not primitive types), things get more
complicated. We need something like RC to track references to objects from
maps. Also, freeing the map is more expensive. It requires scanning the
entire map to decrement all of the reference counts.

And then there is another case where the elements are boxed, but they are
all supposed to live together in the same region with the map's spine. This
means that we can deallocate the whole thing cheaply.

I feel like I'm making my way back to regions. Maybe what I want is a
particular flavor of regions:

* Existentially quantified regions are essential. Any existentially quantified
  region should be reference counted.
* In the past, I've worked on an uncommon type system to try to get everything
  to ANF. The paper Linear Regions Are All You Need explores another approach
  to achieving ANF, but it relies on linear types in a way that overly
  sequentializes things. Maybe full ANF just isn't worth it. I'm much more
  willing to accept this than I previously was.
* In theory, it's possible to lower a strict, immutable source language to
  something with existentially quantified regions. The performance of the
  resulting system would be abysmal though. To improve performance, we need
  a way to cheaply create anonymous regions with a single member of known size.
  Doing this still effectively degrades the memory management strategy to RC,
  which is known to be bad, but at least we can make this much less awful than
  it might otherwise be. Any region created this way could not be grown.
  That is, the handle to the region (a capability for allocation) would be
  unavailable. I think that we need to do this to existentially quantified
  regions anyway, so it's not a big deal. But we want a special construct
  for creating these regions.
* If we are accepting defeat in the battle over ANF, maybe it makes sense to
  embrace GHC's `runST` strategy for mutation.
