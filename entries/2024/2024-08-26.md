# Closures (And More) as Second-Class Values

On 2024-08-23, I described a scheme for dealing with closures. To work
around the difficulty of having true second-class values, I made it so that
closure could not be bound to an identifier. However, this comes with
drawbacks. We cannot share the same closure in two function calls. We also
have to keep all of the individual values that constitute the environment
live longer than we might need to, which may cause register spills. 

As I thought about this more, I realized that we already have another
somewhat second-class construction in the system: regions. Regions cannot
be returned from functions. Going beyond the restrictions from the
Gentrification paper, regions cannot be passed as arguments to join
points. This restriction is to make region lifetime analysis tractable.
Stack-allocated data (of any type, not just closure) needs nearly
the same restrictions. It cannot be returned from a function. Join points
can close over stack allocated data, just like they can close over regions.
A nonrecursive join point can accept a stack-allocated argument, but
a recursive join point cannot. Why? If a recursive join point feeds
its own stack-allocated data to itself, it can make the stack grow
without bound. In theory, this could work (it's the intended
use of x86's SP register), but users have a hard time reasoning
about stack use in such a system. It's also nice during code generation
if every bound variable corresponds to a single address (or a single
offset from the stack pointer). 

Regions are somewhat more naturally second class. There's no reason
to alias a region like this:

    let r2 = r1 in ...

And there is no reason to return a region from a function. There's only
one way to create a fresh region since all regions are indistinguishable.
So all that is required to handle regions is a special let-like construct
just for regions:

    withreg r { let foo = allocate @r Foo in ... }

This works even in an ANF-style language. The user does not have a ton
of control over when region deallocation happens, but since we prohibit
regions from being passed to join points, we end up with reasonably
tight bounds on lifetimes.

With stack-allocated data, the most important difference is that we sometimes
need to be able to return stack-allocated data. Consider a data structure that
is allocated (with zero or zero-like initialization) and then initialized
slowly. Something like this is used linearly, so the updates must all return
a fresh reference to the object. But this means returning a second-class
value, which is not allowed. So we need a special RHS for let bindings
that means "update second-class value", and our "returned" object should
also be second class.

How does this impact everything else? It should be possible to pass
second-class values, even initializer that must be consumed linearly,
to functions. In the case of passing an initializer, the function could
freeze the initializer and then read from the frozen object.

There is a conceptual overlap between stack allocation and regions. Each stack
frame can be understood as a region, especially since regions are not allowed
to escape. So why do we need to treat the stack as something special? Maybe
we don't. For each region, we could perform an analysis to see whether or not
there number of allocations into the region is bounded and small (less than 8KiB
or something like that). If it is, we could mark this region as being a
"stack region", and all of the objects in the region would be allocated at
fixed offsets from the stack pointer.

What about closures? Let's take a look at the `ReplicateN` example
from 2024-08-23:

    clsr ReplicateN : Closure{a}(a)(List a) = closure
      envr
        count : S64
      eval(e : a) = ...

In this particular example, the `count`, an `S64`, would be in the "primitive"
region, but let's pretend that it's some other type that is actually heap
allocated. So the closure itself is allocated into a region, and the
environment must live in that same region. This is fine. We can upcast
read-only data (like the data in the environment) to pretend that it lives
in the closure's region (regions are a FIFO stack). Notice that only the
environment lives in the closure's region. The argument and result types
may live elsewhere. So, we actually need this:

    // The plus means that we need the allocate capability for the region
    clsr ReplicateN : Closure{r+}{a}(a@r)(List a @r) = closure
      envr
        count : S64
      eval(e : a@r) = ...

Allocating closures into regions appears to work. Here are some thoughts:

* We do not have to make closures completely first class. In particular,
  I do not want to be able to instantiate type variables with closure types.
  Being able to do that forces us to track polarity (variance?) in the
  type system, which I do not want to be able to do.
* We don't even have to allow closures as fields to objects, even though
  this simple concession would not require type-variable polarity tracking.
* We cannot prevent closures from being returned from functions. The closure
  could be returned into an existing region. I would need to go out of my way
  to prohibit this, so I think it's fine to support this. I don't know why
  anyone would want to do this though.

# Closures and IO and Mutation

Consider the system for mutation/IO that I ended on in the 2024-08-22 entry.
Is it possible to extend closures to support this? Yes. This should work
just fine. So we can have this function for modifying an existing value
in a mutable map:

    // Third argument is a closure
    modify : {r}{s}{a}(Map a @r, Key, (a@r => a@r)@s) => Unit

One note. Here, we don't want `modify` to require the allocate capabality on
region `r` (since it doesn't allocate anything), but we need the closure
to be able to close over the allocate capability. This should be fine.
I just need to spend more time thinking about what can get stored in a
closure. Type variables are similar. We need a type environment for each
closure that tracks how each of the type arguments were instantiated.
This is fine because we don't erase type variables. So even in a polymorphic
function, we can allocate and return a closure, and we get a correctly
monomorphized closure. This has to be done this way because we can use
an instantiated polymorphic closure where a monomorphic one is expected.
The application site, where a monomorphic closure is used, does not pass
any type arguments to the closure. So these arguments need to be stored
in a type environment.

Regions arguments to closures are a little different. Type arguments are
never erased. We have to pass the size of the type to `memcpy` in a bunch
of places. But the erasure of regions is less clear. Depending on how
regions are implemented, every single load and store might require
a "base address" for the region. And in that case, we would need to handle
regions exactly the same way. The allocate capability certainly
has to be closed over no matter what because there is no possible
implementation of regions that erases it. So I think it makes the
most sense to take the pessimistic assumption that we do need to close
over all of these things.

Going back to the `modify` example, the closure argument might include
a representation of `a` and a representation of `r`. This feels redundant
because we already have this information available at the closure's
application site. But the application site cannot feed those arguments
to the closure. Why? Because the closure might not need them. If we
perform this application:

    modify{R1}{R2}{Foo} : (Map Foo @R1, Key, (Foo@R1 => Foo@R1)@R2) => Unit
    modify{R1}{R2}{Foo}(map, key, func)

We can see that the closure might be fully monomorphic. Section 4 of the paper
Typed Closure Conversion (1996) does a good job explaining the issue that
I am dealing with. The paper is interested in supporting partial application.
I'm not concerned about partial application. So I think I can ignore all the
stuff with translucency. I should be able to use the paper's more simple
treatment of a closure that involves free type variables. That treatment is:

* Give it a closed type (no free type variables)
* Make it accept all of the types that were formerly free as
  type arguments (i.e. a rank-two type)
* Pair the body of the closure with a type environment and
  a value environment.

Which is exactly the same thing I was thinking of going. This makes me feel
good about this approach.

# Closures and Allocation Capabilities

It's a little weird to allow a closure to close over an allocation capability.
Being able to see allocation capabilities explicitly helps users reason about
what a function can do. If we do not allow this, we bifurcate APIs:

    modify : {r}{s}{a}         (Map a @r, Key,           (a@r  => a@r)@s) => Unit
    modify : {r}{s}{a}{Alloc r}(Map a @r, Key, ({Alloc r}(a@r) => a@r)@s) => Unit

Now, it is fully clear that the first `modify` cannot allocate anything.
But I do not think that this is worth doing. I think it is reasonable for
users to just have to keep in mind that a function that applying a closure
might result in allocation.

# Closures Escaping Regions

Closure conversion in a typed setting involves existential quantification, but
existential quantification interacts poorly with stacked regions. So I need
to be careful here. I think that the rule just needs to be that a closure's
entire type environment and region environment is all part of its type. 
It would actually be ok to allow closures to have existentially quantified
types, just not existentially quantified regions.

# Closures and Forking Threads

Since closures can hide allocation capabilities, they cannot be shared
between threads. This could cause two threads to be able to allocate into
the same region at the same time, which would have disasterous consquences.

# Tags for Search

tag:secondclass, tag:closures, tag:regions

