# Revisiting ASAP

I spent a week studying ASAP a year ago, and then I gave up on it and pursued
other strategies for memory management, especially regions. I revisited ASAP
again recently, and it really occupies a unique position in the landscape
of techniques for automatic memory management. It's difficult to understand,
and I wanted to write down some notes in case I burn out on it again. First,
some terms:

* Program point: A position in the program. Often π in the papers.
* Memory location: An object on the heap. This is sometimes called a "location"
  for short. It is also called an execution-time location.
* Location: Ambiguous. From ASAP: "This overloading is more than purely
  aesthetic: in the shorter form we use the compile-time identifier for
  the scanned value (i.e., the variable x) rather than the execution-time
  location (l)." So, sometimes a location means a compile-time binder,
  but sometimes it means a memory location.
* Path: An access pattern for a location. Paths are specific to types.
  That is, they use data constructors to drill into the type. They are
  similar to regex, and they support repetition.
* Zone: A location and a path paired together.
* Z: A function that takes a zone (location + path) as its argument and
  returns a set of all locations reachable from the argument zone.
* Shape: Here's the definition from Practical Static Memory Management (PSMM).
  "a relation over zones such that for all zones `(x, p), (y, q) ∈ Zone` ...
  the set of locations reachable from (x, p) and (y, q) overlap (external
  aliasing)."
* Share: Same as shape but for internal aliasing.
* Access: Again, from PSMM: "The Access decoration at π is simply the set of
  zones that may be accessed at any program point π 0 reachable from π."

# ASAP Example

Here is an example of how I think ASAP might work. Suppose we have a
primitive for selecting an element based on a boolean. The result
might alias either of the arguments. Now lets consider a fragment
of a program:

    a = makeFoo(...)
    b = makeFoo(...)
    s = makeFoo(...)
    // P1
    c = choose(myBool, a, b)
    // P2
    ... // Neither a nor b is referenced again, but c and s are used

All of the data is opaque, so paths don't matter here. Loosely, we have:

    Access(P1) = {a, b}
    Access(P2) = {c}

We ignore `myBool` since it is a primitive type. Now lets compute the
matter and antimatter sets:

    Matter(P1, P2) = {c, s} / {s} = {c}
    Antimatter(P1, P2) = {a, b}

Since s does not alias a or b, we are able to remove it from the matter
set. To evaluate the CLEAN prodedure:

1. Mark zones in the matter set {c} as safe.
2. Scan through the antimatter set {a, b} and free everything that was not
   marked as safe. Either a or b will get freed.
3. Reset all the marks.

Notice that we are starting from the stack and basically running a mark
and sweep GC. Also notice that we could have performed the same operation
without any of ASAP's analyses. The purpose of the analyses is to help us
scan less.

# Ideas

## Program Points

ASAP talks about share and shape as properties of a program that are
true at a certain program point. However, ASAP does this to accomate mutation.
If we don't care about mutation, we might be able to improve this. Without
mutation, share can be thought of a property of a binder. Shape is a little
more complicated because it involves two binders. Neither of these properties
need to be associated with program points (ignoring that scrutiny refines
them). They are both constants once their arguments are in scope.

## Functions

Both ASAP and PSMM support an interprocedural analysis that results in
"amalgamated call contexts" and "summaries". I don't understand this well,
but I am also skeptical that this is worth doing. Instead, I would do this:

* A function assumes that all of its arguments (and everything they can
  reach) are live for the entire body. That is, a function never deallocates
  any of its arguments.
* A function assumes that all of its argument may alias one another extensively.
* Functions are annotated with something that indicates in which ways
  a result may alias arguments. This is useful information for the caller.
  Without this, every calling context would need to assume that its result
  aliased its arguments.

## Stack Allocations

ASAP is compatible with allocating non-escaping data onto the stack. In fact,
any allocation cannot appear in the return value can be stack allocated.
It can be stack allocated even if it appears is stored in heap-allocated
objects. We would need a way to figure out if a pointer pointed into
the heap or the stack, which is not difficult. If the runtime attempted
to free stack-allocated data, it would instead perform a no-op.

This is a little more tricky with recursive join points. We would need to
check that a recursive join point did not pass the object back to itself
and that it did not return it. But this is not an insurmountable problem.

## Uniqueness

All of this seems compatible with uniqueness types used for mutation.
Editing a mutable array could trigger bad scanning behavior, so I need
to think about arrays specifically. It's like how RC is terrible for
bulk operation on arrays (copy, clone, etc.) because it's too prompt.
ASAP has that same problem.

## Defer Deallocation

Section 5.7, Alternative approximation of waste, of ASAP discusses the
possibility of coalescing CLEAN procedures. It suggests that the most
extreme version of this would be to CLEAN only right before a function
returns and no where else in the function body. With recursive join points,
this does not work, but the idea is still interesting.

There is another approach to deferring deallocation that does not
directly coalesce CLEAN. The other approach is to push the antimatter
zones onto a stack of zones that we want to attempt to deallocate later.

The paper makes this claim:

> Taking this batching strategy to the extreme, we can push all the
> deallocations to the return points of each function. This extreme
> approximation of waste is based on scope: values are deallocated when
> the function returns and they fall out of scope. It is not possible
> to deallocate values later than this because, when a function returns,
> its local values are popped oﬀ the stack and their memory blocks become
> unreachable.

But if we pushed the addresses onto a stack of "things to attempt to
deallocate later", everything should be fine. Or it would be fine
as long as we did not support stack-allocated data.

## Regions

I am not sure how to get regions to play nicely with ASAP. Consider
a value that does not alias anything else (like a syntax tree returned
after parsing something). It seems like it should be possible to release
all of this memory at once. It might be possible to just have multiple
managed-memory heaps, but you can free an entire heap at once after you
finish with it. This would still make it possible to allocate temporary
data into that heap. But the understanding would be that when you were
supposed to build something into it that you wanted freed all at once.
I don't know. I should probably quit worrying about this feature.

## Scanning Big Things

Let's say we have a large ordered map where the keys and integers and
the values are text. We perform a lookup, do something with the value,
and then let that value go out of scope. When the value goes out of
scope, we will end up scanning the map to look for it. Why? We know
that the value appears in the map, but our analyses are treated as
approximate. So the compile-time analysis decided that the value may
appear in the map, not that it is certain to appear in the map.
We could try to bring back the 3VL from the ASAP paper, but we are
still going to run into other situations where we end up scanning
big data structures to look for tiny things. We need a better plan for
dealing with this.

## Pointer Ordering

Can we improve the performance of scanning by enforcing that objects
can only point backwards into the heap? No, because it's only possible
to enforce that with semispace collectors, not mark-and-sweep collectors.

# Conclusions

ASAP is complicated. It seems difficult to implement the required static
analysis to even drive the runtime. It also doesn't work well with semispace
collection, regions, or RC. Its promptness is appealing, but that very
promptness causes it to have pathologically bad performance in scenarios
that might happen in real code.

Traditional GCs amortize the cost of their full-heap scans over the time
it takes to allocate several (or many) megabytes of data. If we push
ASAP to the extreme with deferred deallocation, we just end up with
a complicated mark-and-sweep collector that is unlikely to actually
perform any better.

The only way I can think of fixing the critical flaw in ASAP is to somehow
arrange the heap in such a way that it's possible to quickly answer the
question "does object x contain y transitively?" Even the backward-pointing
guarantee that you get from a semispace collector does not provide tools
for answering this question.
