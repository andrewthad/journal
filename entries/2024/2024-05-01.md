# Linear Consumption of ADTs, Continued

Continuing from 2024-04-29, I've had another thought. In the system that I
want to build, it's not actually possible to scrutinize a single-data-constructor
type. This differs from Haskell and most SML derivatives. What I mean is that
we cannot write:

    case p of
      Person name age -> ... name ... age ...

We instead write:

    ... p.name ... p.age ...

The interaction with linear types is interesting. We cannot scrutinize `Unit`
(the 0-tuple type), so we cannot consume `Unit` linearly except by passing
it through. For example:

    foo : Unit -o Unit
    foo x = x

This is allowed. Let's consider a type with on linear field and one unrestricted
(nonlinear) field:

    type Output = record
      { Output =
        { descriptor<1> : Fd 
        , bytes<*> : Array U8 
        }
      }

This has a single data constructor. The expectation is that the file descriptor
is consumed linearly and the bytes are not. Here is how we might use this:

    read : Fd -o Output
    readTwice : Fd -o Output
    readTwice fd0 =
      r0 = read(fd0)
      r1 = read(r0.descriptor)
      allBytes = concatenate(r0.bytes, r1.bytes)
      Output{descriptor=r1.descriptor, bytes=allBytes}

Roughly, the rule is that a value is consumed linearly when its linear
components are consumed linearly. Additionally, there must be at least one
linear field. What happens with a multiplicity polymorphic data type?

    type Pair m n a b = record 
      { Pair =
        { fst<m> : a
        , snd<n> : b
        }
      }

And now let's use it:

    combine : R -o R -o R
    consumePairLinearly : (a -o R) -> (b -o R) -> Pair m n a b -o R
    consumePairLinearly f g p = combine (f p.fst) (g p.snd)

But this doesn't really work. When we have `m ~ Many` and `n ~ Many`,
then according to the rule I made up, this should not count as linear
consumption.

There might be ways to work around this. Banning multiplicity polymorphism
helps. Or maybe it is possible to ban multiplicity polymorphism in functions
but not in data types. Then every function has to pick a concrete multiplicity
instantiation for the data type.

Every with that workaround in place, this does not work correctly for cons
lists. There are not any linear fields in `Nil`. But maybe this is alright.
Maybe the rule for linear consumption of data types is:

* Data type must have at least one linear field in one of its data constructors
* If all the constructors with linear fields are masked out, the data type
  cannot be consumed linearly.
* Linear consumption happens when scrutiny eliminates all possible data
  constructors with linear fields (and consumes those fields linearly).
* The remaining data constructors (those without linear fields) may be handled
  in a non-linear fashion.

With lists, for example, we have:

    cons        : a -o UList a -o UList a
    fold        : (b -o a -o b) -> b -o UList a -o b
    traverse_   : (a {M}-o Unit) -> UList a {M}-o Unit
    traverseIO_ : (a !-o Unit) -> UList a !-o Unit

The type `A -o Unit` is not inhabited by anything interesting (although
when `A ~ Unit`, it's inhabited by the identity function). But if we use
modules to introduce an abstraction that prevents file descriptors from
escaping, then the traversals become more interesting:

    Posix = signature
      { Fd : Type
      , M : Effect
      , with  : Function{M}(String{*}, Function{M}(Fd{1})(a)){1})(a)
      , read  : Function{M}(Fd{1})(Fd, Array U8)
      , close : Function{M}(Fd{1})(Unit)
      }

I don't like this API. The LH paper instead adapts the monadic presentation
of IO:

    type IOL p a
    returnIOL :: a ->p IOL p a
    bindIOL :: IOL p a -o (a ->p IOL q b) -o IOL q b

Since monadic bind CPSes the computation, we are effectively able to cheat and
get a linearity annotation on the result. We might want all effects to support
linearity on the return type. Maybe even noneffectful functions too:

    Function (Type,Multiplicity) (Type, Multiplicity)
    ==> // lowers to
    Function (Type,Multiplicity) Type 

How does this lowering work?

    foo : MutableArray Int {1} -> MutableArray Int {1}
    bar : Int
    bar = withMutableArray $ \m0 ->
      m1 = foo(m0)
      m2 = foo(m1)
      m2

The function `foo` is lowered to:

    fooCps : MutableArray Int {1} -> (MutableArray Int {1} -> a) {1} -> a
    bar : Int
    bar = withMutableArray $ \m0 ->
      fooCps(m0, \m1 -> fooCps(m1, \m2 -> m2))

Another way to incorporate this is to not actually perform CPS. The system
would have functions that return linear values be treated differently. And
there would be a different kind of let binding binding the results of these
functions. A normal user would only see one kind of let binding and one kind
of function. But internally, they would becomes different things. Notably,
we might want things like:

    newArray : Int -> a -> MutableArray a {1}

We cannot freeze this array, and also, it's not allowed to escape. Because
as far as type checking is concerning, we'd like to pretend that

    let myArr = newArray 100 Foo
     in ...

Is lowered to

    // The type of withArray
    withArray : Int -> a -> (MutableArray a {1} -> r) -o r
    withArray 100 Foo (\myArr -> ...)

That's not quite right. This allows the array to escape. Maybe that's ok though.
Suppose we have a function that allows such an array to escape:

    makeBigFooArray : Unit -> MutableArray a {1}
    makeBigFooArray = newArray 100 Foo

Then, as far as typechecking as concerned, we are actually dealing with

    makeBigFooArrayCps : (MutableArray a {1} -> r) -> r
    makeBigFooArrayCps f = withArray 100 Foo f

So, escape is actually fine because it's not really escape. There's just more
CPS preventing true escape. We just have to be careful about how optimizations
treat linear let. Here are some thoughts:

* Application of linear functions is not eligible for common subexpression
  elimination.

Useful things that can be done with this:

* Building up a larger data structure that is mutable. (How do
  we free the data structure?)
* Providing a better abstraction for opening and closing files.
  Having `withFile` is ok, but it is more natural to have
  `openFile` and `closeFile` and let the type system force the
  user to call `closeFile`.

I feel stuck. I prefer direct style to CPS. But the extension of linear
types to support direct style feels strange. Maybe uniqueness types are
the better path forward. Let's try the IO example:

    open  : Function{M}(String)(Fd*)
    read  : Function{M}(Fd*)(Fd*, Array U8)
    close : Function{M}(Fd*)(Unit)

So that seems like it works fine. And for arrays, uniqueness types work well,
but there is one issue that always bothers me. The paper Uniqueness Typing
Simplified suggests:

    newArray : Int -> Array*

But this has always struck me as strange because it's not really a function
like most functions are. If we have:

    let a = newArray 100
    let b = newArray 100
     in ...

Then mindless application of CSE gives us:

    let a = newArray 100
    let b = a
     in ...

But this is wrong because we need `a` and `b` to actually be different arrays.
What if we handled this issue with the effects system? That is, we have an
"allocate unique" effect. Let's represent this with the big arrow (=>).

    run         : (a => b) -> a -> b // Note: both a and b are non-unique here.
    newArray    : Int           => MutableArray*
    writeArray  : MutableArray* -> Int -> Element -> MutableArray*
    freezeArray : MutableArray* => Array

Notice that `writeArray` does not need the big arrow. We only use this
sequencing effect to plug the gaps in uniqueness types.

Although maybe this same solution would work for linear types. Let's try it:

    run          : (a => b){1} -> a -> b
    newArray     : Int             1=> MutableArray
    writeArray   : MutableArray{1} -> Int -> Element -> MutableArray
    freezeArray  : MutableArray{1} => Array
    discardArray : MutableArray{1} => Unit

The type of `newArray` is interesting. It uses the same `IOL` trick from the
Linear Haskell paper, so the caller must do something with the mutable array.
(They cannot just let it go out of scope.) Thinking about abstracting file
descriptors again:

    Posix = signature
      { Fd : Type
      , M : Effect
      , open : String M1=> Fd
      , ...
      }

This suggests that, in the user-facing language at least, the "binding" arrow
should be the only one available. In a lower-level IR, this can be split into
the truly pure arrow and an arrow is a CPS monad in disguise.

Let's try building an array in the high-level language:

    // Sequenced
    copy   : Array a            -> MutableArray a {1}
    freeze : MutableArray a {1} -> Array a
    // Pure
    index : (Array a, Int) -> a
    write : (MutableArray a, Int, a) -> MutableArray a
    // Our code
    reverseTripleton :: Array a -> Array a
    reverseTripleton(src):
      dst0 = copy(src)
      dst1 = write(dst0,0,index(src,2))
      dst2 = write(dst1,1,index(src,1))
      dst3 = write(dst3,2,index(src,0))
      res = freeze(dst3)
      res

The functions `copy` and `freeze` both rely on the CPS monad to typecheck.
Let's examine their CPS forms:

    copy   : (MutableArray a {1} -> r) {1} -> Array a -> r
    // Note: My disussion of freeze below is confused and wrong.
    // I ended up recreating the same thing that I said was wrong.
    freeze : (Array a -> r) -> MutableArray a {1} -> r
    freeze : MArray a {1} -> (Array a -> r) {1} -> r

The CPS version of `copy` looks fine, but the one for `freeze` strangely
suggests that the result `r` must be consumed exactly once for the mutable
array to be linearly consumed. But this is not right. Actually, `freeze`
does not even need a CPS form. The Linear Haskell paper has `freeze` as:

    freeze :: MArray a ⊸ Unrestricted (Array a)

But I am loath to introduce an `Unrestricted` type. What about this:

    freeze :: MArray a ⊸ (Array a -> r) ⊸ r

We're still in nearly the same situation, but now we at least get to
use the immutable freely. Whatever calls `freeze` has to consume the
result of type `r` linearly though. But maybe this is sufficient
because `freeze`'s actually closes over everything else. It's the final
statement in the block. Consider how the CPS forms of `copy` and `freeze`
might be combined:

    copy (\buf -> freeze buf (\imm -> imm)) src

So, we end up with several ways of lowering functions:

    plain      | A {*} -> B {*}    | (B {*} -> r) {1} -> A {*} -> r
    allocating | A {*} -> B {1}    | (B {1} -> r) {1} -> A {*} -> r
    freezing   | A {1} -> B {*}    | (B {*} -> r) {1} -> A {1} -> r
    deallocate | A {1} -> Unit {*} | (         r) {1} -> A {1} -> r

The allocating and freezing cases are actually the same thing. And, as shown
above, we can even CPS the plain case if we want to. But the goal is just to
move the annotations off of the RHS types.

This seems like it might work. But there is something that still bothers
me. In the original example, we have

    dst0 = copy(src)
    dst1 = write(dst0,0,index(src,2))

But should this typecheck:

    dst1 = write(copy(src),0,index(src,2))

Intuitively, yes, it sould. Allocation effects can be reordered without
changing the behavior of a program, so we could even have floated this down
through through other bindings if needed. We are just in a weird universe.
It has these properties:

* CSE is not allowed
* DCE is allowed

In the low-language language, we start by lifting all the truly
pure functions. We also need to introduce bindings for functions that allocate
and freeze, but those already had names in this example:

    reverseTripleton :: Array a => Array a
    reverseTripleton(src):
      dst0 <= copy(src)
      dst1 <= lift write(dst0,0,index src 2)
      dst2 <= lift write(dst1,1,index src 1)
      dst3 <= lift write(dst3,2,index src 0)
      res <= freeze(dst3)
      res

And then we can group the lifted write operations:

    reverseTripleton :: Array a => Array a
    reverseTripleton(src):
      dst0 <= copy(src)
      dst4 <= lift:
        dst1 = write(dst0,0,index(src,2))
        dst2 = write(dst1,1,index(src,1))
        dst3 = write(dst3,2,index(src,0))
        dst3
      res <= freeze(dst4)
      res

Then, finally, we can recognize that this does not need the
allocating/sequencing effect:

    reverseTripleton :: Array a{*} => Array a{*}
    reverseTripleton(src): lift reverseTripleton0(src)

    reverseTripleton0 :: Array a{*} -> Array a
    reverseTripleton0(src): run:
      dst0 <= copy(src)
      dst4 <= lift:
        dst1 = write(dst0,0,index(src,2))
        dst2 = write(dst1,1,index(src,1))
        dst3 = write(dst3,2,index(src,0))
        dst3
      res <= freeze(dst4)
      res

    run : (a{*} => b{*}){1} -> a{*} -> b

But what is the algorithm for figuring that out? I'm not sure. In this case,
we are guided by a simple heuristic that anything with nonlinear arguments
and a nonlinear return type is definitely safe to duplicate. This means
that if we had this in the high-level language (HLL):

    myTripleton = ...
    x = reverseTripleton(myTripleton) 
    y = reverseTripleton(myTripleton) 
    ...

In the HLL, we might actually be able to do CSE here because the type of
`reverseTripleton` makes it clear that we are not relying mutating the
result, but even if we didn't do CSE in the HLL, this would get lowered
to:

    myTripleton = ...
    x = lift reverseTripleton0(myTripleton) 
    y = lift reverseTripleton0(myTripleton) 
    ...

And then

    myTripleton <= ...
    x <= lift reverseTripleton0(myTripleton) 
    y <= lift reverseTripleton0(myTripleton) 
    ...

And CSE under `lift` is sound.

I think this addresses everything that has been bothering me. Discarding
linear values is handled by an effect/monad, not by scrutinizing an
empty tuple. Even dealing with integers is better. Suppose we want to
implement posix `read`:

    readSyscall : Function{IO}(I32)(Array U8)
    liberate : I32{1} -> I32*
    constrain : I32{*} -> I32{1}
    read : Function{IO}(I32*)(I32*, Array U8)
    read(fd):
      fdx = liberate(fd)
      r = readSyscall(fdx)
      fdy = constrain(fdx)
      pure (fdy, r)

Maybe I should distingush in the high-level language. We only even need
the CPSed thing when we are dealing with allocation of mutable data,
deallocation, and freezing.
