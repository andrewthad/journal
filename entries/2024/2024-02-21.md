# Allocate on Stack

I started thinking about this again. This time, I am not thinking about
trying to track regions explicitly. Consider:

    // Implied (arguments and result may only be from previous stack frames):
    // allocation(a) <= s
    // allocation(b) <= s
    // allocation(c) <= s
    // Implied (primitive integral types do not live in stack frames):
    // allocation(a) <= 0
    function[s](a : Int, b : Map)(c : Map) 
    { // Allocate into global heap, which is the the lower bound, like zero 
      x = allocate Heap { ... } 
      // Allocate on stack, stack frame implied syntactically to be: s + 1
      // We should not actually need to write s+1 in brackets.
      // This means that we cannot return c from this function.
    ; y = allocate Stack[s+1] { ... }
      // Again, s+1 is implied and does not actually need to be written.
      // Also implied: allocation(z) <= s+1
    ; z = call foo[s+1](x,y)
      // Implied in recursively defined labels:
      // allocation(k) <= s+1
      // allocation(j) <= s+1
    ; rec[s+1] // bracketed part not actually needed
      { label bip(k : Map)
        { // in here, stack allocations are at: s + 2
        } 
      , label bop(j : Map)
        { // in here, stack allocations are at: s + 2
        } 
      }
    }

One thing that is interesting about this approach is that allocation frame
is tracked separately from types. That is, for some expression `e`, we have:

* `allocation(e)`
* `type(e)`

This means that information about allocation frame is not a part of types.
The advantage is that polymorphically typed expressions can still have
bounds for their allocation frames. The disadvantage is that primitive
integral types have allocation frame bounds, even though this doesn't
really make any sense. They are always on frame 0 because they are passed
by value, not by referrence. The same is true of any pass-by-value type.

The other way to do this is to put the bound in the type. We would have:

    Type
      = Boxed Frame ConstructorFieldMap
      | Integral Signedness Width
      | Bool
    typeFrame : Type -> Frame

This becomes cumbersome because we now have to list out the frames explicitly
in more places:

    function[s](a : Int, b : Boxed s Map)(c : Boxed s Map) 

With this strategy, every time we have a boxed type as a function argument,
we will end up having to explicitly mention the frame. We cannot rely on
a syntax-driven solution like we did before. We do not really even want
these to be different types since it complicates monomorphization.

# Worker Wrapper for Stack Allocation

Here's an idea for a cool transformation:

    function(x : Int)(Person)
    { y : Int = ...
    ; z : String = ...
    ; p0 = allocate Heap Person
    ; p1 = write p0 age y
    ; p2 = write p1 name z
    ; p = freeze p2
    ; p
    }
    ==>
    { worker = function[s](t : Frame)(p0 : MutablePerson@t, x : Int)(Person@t)
      { y : Int = ...
      ; z : String = ...
      ; p1 = write p0 age y
      ; p2 = write p1 name z
      ; p = freeze p2
      ; p
      }
    ; function(x : Int)(Person@0)
      { p0 = allocate Heap Person
      ; worker(Heap)(p0,x)
      }
    }

If we call this function in a context where the result does not escape,
we can inline the wrapper, and the a separate optimization could change
the heap allocation to a stack allocation.

One think that is becoming more clear is that frame tracking on function
results is more rigid. Saying that a function's result is in a frame that
is less than or equal to `s` is no good. There is a reasonable theory
behind it, but is there is a way to producing working code?

    { worker = function[s](p0 : MutablePerson, x : Int)(Person)
      { y : Int = ...
      ; z : String = ...
      ; p1 = write p0 age y
      ; p2 = write p1 name z
      ; p = freeze p2
      ; p
      }
    ; ...
    }

Now, the frame of `Person` is inferred to be less than or equal to `s`, which
is true even though it is weaker than the strongest possible claim. But the
function still clearly does not allocate. We just have to figure out at
run time, which objects live on the stack and which objects live on the heap.
Can we do this? Yes. We just inspect the pointer, and we can tell where
it lives. I am bothered by the imprecision, but we already have to
deal with such imprecision if we want functions permit a read-only argument
to be placed on either the stack or the heap. Inside the body of such a
function, we simply do not know where the object lives. And that's ok.

I see a situation where this causes a problem. What if we want to embed
the `Person` in a heap-allocated object. We cannot do it because the type
system does not tell us that the person lives on the heap. In fact, the
wrapper from the original example won't even typecheck anymore.

So, it turns out that we need to support explicit frame arguments.

# Two Types of Types

When types appear as field types, they are different from types that
appear on binders in functions. Here are some differences:

* Field types cannot be mutable
* Field types do not need to refer to frames
* Fields cannot be pointers

And we have similar restrictions on type variables since a type variable
can be given as an argument to a type constructor.  I propose having two
different definitions of type, one for fields (used in data constructor
declarations) and one for expressions. Before going into more detail, I'll
provide a motivating example of an issue I ran into recently:

    { @ListS64 = constructed
      { Cons =
        { head = S64
        , tail = ^0
        }
      , Nil = {}
      }
    ; @MutableListS64 = constructed.mutable
      { Cons =
        { head = S64
        , tail = @List64
        }
      , Nil = {}
      }
    ; ...
    }

In this example, I had to define a list type twice because `tail` recurses
in the immutable variant but not the mutable variant. It would make more
sense to write:

    { @ListS64Construction = construction
      { Cons =
        { head = S64
        , tail = ^0
        }
      , Nil = {}
      }
    ; @ImmutableList64 = constructed immutable @ListS64Construction
    ; @MutableList64 = constructed mutable @ListS64Construction
    ; ...
    }

The general idea here is that properties like mutability, frame tracking,
and even regions would be pushed down onto constructed types from the top.
During projection, the field types would be enriched with additional
information based on an object's properties. Here is the system:

    ExprType
      = Boxed Mutability Frame Construction
      | Array Mutability Frame (FieldType 0)
        // we definitely do not want mutable array elements,
        // and the upper bound on the frame is just pushed down
      | Integral Signedness Width
      | Bool
      | Ptr
          ExprType -- base object
          ExprType -- element
    exprTypeMutability : ExprType -> Mutability
    exprTypeFrame : ExprType -> Frame
    FieldType (n :: Nat)
      = Boxed Construction
      | Array ExprType
      | Integral Signedness Width
      | Bool
      | Recurse (Fin n) // debruijn index
    type SimpleType = FieldType 0
    // Upcast the type, claiming that it in the global heap and is immutable.
    simpleTypeToExprType1 :: FieldType 0 -> ExprType
    // Upcast the type to an immutable ExprType. If possible, designate
    // an upper bound for the frame. Pass-by-value types only live
    // in the global heap, so we cannot use an exact bound here or
    // the function would be partial.
    simpleTypeToExprType2 :: Frame -> FieldType 0 -> ExprType

We need to think about what `Frame` is here. Sometimes, we want an upper
bound, like in the case of a function argument. But sometimes, we want
an exact match. We never want a lower bound. So I think that `Frame`
should be:

    Frame
      = Eq FrameVal // Note: After reflection, I think that Eq is not necessary 
      | Lte FrameVal
    FrameVal = Zero | Variable FrameVar
    FrameVar = ... // opaque, probably a string or a number

There a quotienting problem in this representation where `Eq Zero` and
`Lte Zero` are actually the same thing, but I'm not worried about that.

It would be nice if type variables could be instantiated with types that
included frame annotations, but I do not think there is a good way to
support this. For example:

    identity[a : *](x : a)(a)

If we can use a frame variable (in our examples, typically `s`) as part of
the instantiation of `a`, then we do not have a finite set of types that
`identity` has as instantiations. We have two options for supporting
polymorphic functions:

    identity[s][a : SimpleType](x : simpleTypeToExprType1 a)(simpleTypeToExprType1 a)
    identity[s][a : SimpleType](x : simpleTypeToExprType2 s a)(simpleTypeToExprType2 s a)

The first variant accepts fewer types, but it provides a guarantee that
the result in on the global heap. The second variant accepts more types,
but we lose information about where the result lives. I think that
the first variant is better. The additional information about where
the result lives means that the result can be placed in an object.
Generally, I don't find these these kinds of functions particularly
useful, so I think that lose flexibility here is ok. Other impacted
functions include:

    identity   : a -> a 
    bool       : a -> a -> Bool -> a
    fromOption : a -> Option a -> a

Nevermind, there is a way:

    identity[s][t : Frame][a : SimpleType](x : simpleTypeToExprType2 t a)(simpleTypeToExprType2 t a)

It's kind of silly because you have to introduce an extra type variable,
but it works. You cannot use a mutable type or a pointer. What happens
with type constructors? Let's try it:

    head[s][t : Frame](a : SimpleType)(xs : Array Immutable t a, ix : Int)(simpleTypeToExprType2 t a)

The array itself includes an upper bound on its frame, and the elements
all share this bound. There is something else that we know here, in this
case and in similar cases, but it is not captured by the type system.
We know that s >= t. If we stack allocate something inside of the body of
the function `head` (not that doing that makes any sense), we ought to
be able to put `xs` inside it. We could have frames themselves be bounded:

    head[s][t : Frame s]

Maybe this would work. I'll try this out in the Ordering Frame Bounds section.

# Intersecting Frame Bounds

The intersection of frame bounds happens when we do this:

    if x then a else b

Here is how intersecting bounds works:

    intersection(0,0) = 0
    intersection(0,n) = n
    intersection(n,0) = n
    intersection(n,n+1) = n+1
    intersection(n,m) = max(n,m)

That last case is unfortunate, but I think it is inevitable.

# Ordering Frame Bounds

First, let's reformulate frames:

    Frame
      = Zero
      | Succ Frame
      | Variable FrameVar
    parent : Context -> Frame -> Frame
    parent _ Zero = Zero
    parent _ (Succ f) f
    parent ctx (Variable v) = lookup v ctx

Examples:

    parent 0 = 0
    in ctx (s : Frame 0) (t : Frame s):
      parent s = 0
      parent t = s
      parent (Succ t) = t

Now let's rethink intersection:

    intersection(0,0) = 0
    intersection(0,n) = n
    intersection(n,0) = n
    intersection(n,Succ n) = n+1
    intersection(n,m)
      | parent n == m = n
      | parent m == n = m
      | parent (parent m) == m
      | parent (parent (parent m)) == m
      | ...
      | otherwise max({n,m})

This needs to be written recursively to be correct, but this at least
illustrates what needs to be done. There is still the possibility that
the ordering of the frames is unclear, and that's fine. So we may now
write:

    bool[s : Frame 0][t : Frame s][r : Frame s]
      (a : SimpleType)
      (simpleTypeToExprType2 t a, simpleTypeToExprType2 r a, Bool)
      (simpleTypeToExprType2 max({t,r}) a)

Such a type is possibile, but it is also not necessary. We would more
likely define `bool` as:

    bool[s : Frame 0][t : Frame s]
      (a : SimpleType)
      (simpleTypeToExprType2 t a, simpleTypeToExprType2 t a, Bool)
      (simpleTypeToExprType2 t a)

The arguments are likely to have a known stack frame ordering. Maybe
all frames should always have ordering required. Then we would not
have goofy stuff like `max()`. We could also write bool like this:

    bool[s,t] // implies that s >= t
      (a : SimpleType)
      (simpleTypeToExprType2 t a, simpleTypeToExprType2 t a, Bool)
      (simpleTypeToExprType2 t a)

Is there a use for unordered frames? In the case of read-only data,
you're just going to want to choose the greatest possible stack frame:

    isEmpty = function[s](m : Map@s)(Bool)

There are a bunch of weird cases with ordered maps that I'm noticing now.
Consider these:

    union  : function[s]  (a : Map@0, b : Map@0)(Map@0)
    lookup : function[s,t](m : Map@t, k : Key@s)(Value@t)
    insert : function[s]  (m : Map@0, k : Key@0, v : Value@0)(Map@0)

Anything that might reuse the map in the result needs the map to be on
the global heap. This is because the function has to allocate on the
global heap to create a result, and this new map cannot reference
the stack. So `union` and `replace` require that their arguments (even
the keys and values) live on the global heap. By contrast, `lookup` is
more flexible with where its argument lives. We could instead define
lookup without `t`:

    lookup : function[s](m : Map@s, k : Key@s)(Value@s)

But `t` is useful because if we set it to 0, then we know that the result
lives in the global heap.

# Concluding Thoughts

This system is useful, but how cumbersome is it? Is this the kind of thing
that it makes sense to expose to end users, or is does it serve better as
an IR that ensures that escape analysis is done correctly? Let's say we
have some function in a higher level language without explicit stack allocation
that looks like this:

    makeDecision = function(a : Environment)(Decision)

When translating, the first step is to lower everything in such a way that
everything always lives on the global heap. 

    makeDecision = function[s](a : Environment@0)(Decision@0)

Just from looking at the function body (not any call sites), an optimization
pass can see that `Environment` is used in a read-only way, and neither it
nor any of its boxed fields are aliased in decision. So, the optimizer can
change the signature:

    makeDecision = function[s](a : Environment@s)(Decision@0)

We can do this because

    Environment@0 :< Environment@s
    ==>
    Function(Environment@s)(X) :< Function(Environment@0)(X)

Later, at any call sites, the optimizer could do either of these depending
on the context:

* Propogate the weakening of the frame requirement upward
* Decide the allocate the environment on the stack instead of the heap
  (if the environment was allocated just prior to calling `makeDecision`)

Useful takeaways:

* It is possible for the GC to discover, at runtime, whether an object
  lives on the stack or the heap. We can take advantage of this by
  allowing heap-allocated types to be subtypes of stack-allocated types.
  This makes code sharing easier.
* It is not necessary to track exact equality of frames. We can just track
  an upper bound and have frame variables be ordered.
* We can track frames without causing problems for a monomorphization pass.
  Frame variables can remain variables. The have no representation at runtime.
* Types of fields, instantiations, and expressions should be slightly
  different. The fields of constructed objects should start out vague
  in the definition of the types and then had additional information pushed
  into them from the top down. This same strategy could be useful for dealing
  with region types as well. It is hard to say though. Region types and
  explicity stack allocation intersection somewhat. That is, fragments of the
  stack used to house bounded-lifetime objects can be understood as regions.
  But regions are different. We cannot enforce a total ordering like we can
  with stack frames. And we do not want to be able to weaken a
  global-heap-allocated type to a region-allocated type. With our stack
  allocations, it's fine if the GC just scans them all the way through.
  (There cannot be that many of them or the stack would overflow.) But
  one of the purposes of regions is to avoid having to do these scans at
  all. So, to incorporate regions into this, we would need to make a
  considerable concession. But, the regions would still be useful for
  prompt reclamation of smaller-than-arena data (esp. intermediate
  representations like syntax trees). This idea might be worth exploring more.
  I think you would end up with a pseudoregion where anything that was physically
  in the region could point to other things that were physically in the region,
  but they could point to things on the global heap as well. Anything in the
  global heap could be promoted to look like it was a part of any region, but
  nothing in the global heap could point to anything that was physically in
  a region.

# Region Idea

Let's explore this a little more. For example, with json, we could do something
like this:

    { json = construction
      { Null, True, False, String Text
      , Array (Array ^0)
      , Object (Array (Pair Text ^0))
      }
    ; decode : Function[q,r :> q](t : (Array U8)@q)(json@r) = ...
    }

Here, region `r` contains region `q`, so it may reference anything in it.
That is, anything in `q` can be upcast so that it looks like it is in `r`.
Either of these regions may reference the global heap as well, since every
region contains the global heap. We can use decode in several ways:

    // A. Useful for arbitrary analysis and transformation of json
    // (like making a clone of jq) 
    decode[Global,Global]
    // B. Useful if most of the string components are aliased in whatever
    // object the syntax tree is later decoded to.
    decode[Global,myRegion]
    // C. Useful if most of the string components are aliased in the
    // final decoded object, and that object itself lives in a region
    // with all the string payloads.
    decode[myRegionA,myRegionB]
    // D. Useful when the decoded syntax tree should live alongside the
    // bytes that were parsed. They will all be deallocated at the same time
    decode[myRegion,myRegion]

And a single lowering of the function should handle all of these cases.
The way these work makes me realize that we will end up needing a magical
primitive that is either a no-op or a copy:

    intoRegion[q,r](a@q)(a@r)

We need this because most or all of the json strings have payloads that
physically referrence the first region, but from the result type, we cannot
know this.

As I'm thinking about this more, I'm realizing that D is nearly always
preferable to C. The region containing the serialized json must live at least
as long as the second region. The only reason to have them in separate regions
is if there is some plan to do additional work on the serialized json
after we are done decoding it. But there is almost no use for it at that
point in time.

If we are using B, then when we decode the syntax tree to something an object
on the global heap, we will have code like this:

    p5 = case val of
      String -> write(p4,name,intoRegion[myRegion,Global](val.string))
      _ -> ...

When a json string contains an escape sequence, we must allocate a new string
for the unescaped value. We can actually do this in either the first region
or the second one. Using the first one probably makes more sense because then
we are certain about what region all the string payloads live in.
