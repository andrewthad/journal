# Notes on Linear Regions Are All You Need

I read the paper Linear Regions Are All You Need again, and this time around,
some things finally clicked for me.

In the high-level language FRGN, universal quantification prevents
stack-variable escape from `rgn` computations (which have a monadic
interface). This works almost just like `ST` in GHC. The biggest difference
is that `letRgn` includes a subtype proof that makes it possible to embed
outer-region computations in the inner region. Computations of type `rgn`
are indexed by both a stack variable (of type `STACK`, no concrete inhabitants)
and a type variable.

In the direct-style language λrgnUL, `rgn` computations are replaced by functions
that consume the stack variable linearly:

    T*[rgn s τ] = S*[s] ⊸ (S*[s] ⊗ T*[τ])

But what are these stack variables actually instantiated with? They have
kind ★ instead of kind STACK (there is no kind STACK in λrgnUL). In the
lowering of `runRgn`, we see that a linear value of the unit type is provided
as a stack value. But the lowering of `letRgn` uses `newrgn` to make a fresh
capability, one with no specified concrete type but that can presumably be
erased. Figure 4 presents the lowerings of `hnd` and ref`. They are similar,
so I'll only describe the lowering of `hnd`. An FRGN handle indexed by `s`
(of kind STACK) is lowered to pair that references an existentially
quantified `Q` (of kind RGN). The pair has:

1. A λrgnUL handle indexed by `Q`
2. A proof that a λrgnUL capability (of type `cap Q`) is present somewhere
   in `S* [s]`, which is a giant nested tuple.

Intuitively, this means that as we lower an expression with nested `letRgn`,
we are building a nested tuple of capabilities, and we can use any handle
or any reference whose corresponding capability shows up in that nested tuple.

Unlike FRGN, λrgnUL supports region programming without LIFO region lifetimes.
Section 5 of the paper discusses this and some cool features of the Cyclone
programming language that rely on this. For example, Cyclone has dynamic
regions, which existentially pack a region with the possibility of opening
it later. While this is powerful, I'm not convinced that it is a feature that
I want in a language.

# Less Expressive Variants

There are just some of my own thoughts about how I could adapt this system.
The presentation of λrgnUL in the paper makes it possible to have non-LIFO
regions, but it's possible to tighten it so that this restriction reappears.
If this is done, the `RGN` kind could be a list of stack indices, and we
might have expressions like this:

    foo[regList : List Region](c : Cap regList, value : Foo @regList)(s64):
      let [r : Region], c1 : Cap (Cons c regList), hnd : Hnd (Cons c regList) = newRegion[regList](c)
      ... // do some stuff with our new capability and handle
      c = release[r,regList](c1,hnd) // destroy the capability, freeing the region
      ... // return something

What's strange about this is that we need the individual `r` regions, but
we really only use region lists in kinds. A different idea is to represent
region lists as natural numbers. Here's the same example rewritten to use
natural numbers:

    // Note: I realized near the end of writing this entry that this variant
    // has a fatal flaw. If we allocate a another region after release, the
    // new capability allows us to access invalid objects.
    foo[n : Nat](c : Cap n, value : Foo @n)(s64):
      let c1 : Cap (n+1), hnd : Hnd (n+1) = newRegion[n](c)
      ... // do some stuff with our new capability and handle
      c = release[n](c1,hnd) // destroy the capability, freeing the region
      ... // return something

This eliminates the need for existential quantification, which is really nice.
Since LIFO regions have a total order, this suggests that every function
should accept at most a single region (nat) argument. All boxed
arguments must live in that region. In this system, it's not even possible
to pass two capabilities since the only way to create a usable capability
is to consume an existing one. (The construct I'm using to create regions
here differs from the one in the paper.) For immutable data `Foo`, we have
the following relation:

    Foo @n <: Foo @(n+k)

This means that objects in newer regions can reference objects in older
regions, which makes sense. But doing the same thing with mutable data
is unsound.

# Example With Arrays

Let's say that we are dealing with a format like JSON that uses an
indefinite-length representation of arrays. This means that we cannot know
in advance how many elements will appear, so we need to populate some kind
of "builder" and then copy from it as the final step. There are lots of
options for what the builder might be. It could be a giant fixed-length
array that crashes after you write too much. It could be cons list of small
chunks. It just needs this interface:

    push : [r,s] Hnd s -> Cap s ⊸ Builder@s a@r ⊸ a@r -> (Builder@s a@r, Cap s)
    copy : [r,s] Hnd r -> Cap s ⊸ Builder@s a@r ⊸        (Array@r   a@r, Cap s)

This doesn't quite work. The `copy` function needs some kind of proof that `r`
is smaller than `s`. We could redo it like this:

    copy : [r] Hnd r -> Cap r+1 ⊸ Builder@r+1 a@r ⊸ (Array@r a@r, Cap r+1)

Anyway, that's one issue, but there's another issue. Our builder elements need
to be able to refer to a region that's different from the one that the builder
itself is in. This is crucial, and there's no way to work around this. This
isn't the only situation where this requirement shows up either. We need to
be able to do the same thing when we deserialized ordered maps, and that's
true even for definite-length encodings.

# Relationship Between Regions and Types

I've been trying to avoid having regions be a part of types in this way, but
it seems inevitable. The probably with being able to instantiate abstract types
with anything that references regions is that it makes building a module system
on top difficult. But primitive types have already put some strain on what I've
been hoping for. That is, types like `s64` really don't live in a region, and
I should probably be more explicit about this. Let's try (without refinements):

    Boxedness : Kind
    Nat       : Kind
    Star      : Kind
    Boxed     : Kind
    Unboxed   : Kind
    S64       : Unboxed
    @         : Boxed -> Nat -> Star (infix operator)
    Lift      : Unboxed -> Star
    Lift S64  : Star
    Foo       : Boxed
    Foo@n     : Star

We may write:

    identity : forall (a : Star). a -> a

And this works with everything, as we would expect. We can do this as well:

    // Note: Cap should be used linearly
    cons : [r]{a} -> Hnd r -> Cap r -> a -> List@r a -> (List@r a, Cap r)

With higher-order functions, we start getting into trouble:

    map : [r,s]{a,b} -> Hnd s -> Cap s -> (a -> b) -> List@r a -> (List@s b, Cap s)

Here, we see a problem we ran into earlier, which is that `s` needs to be
greater than `r` (so that we can use the capability to read from the argument
list). But there's another problem. Since the callback doesn't accept any
capabilities, we can only instantiate `a` and `b` with unboxed types. We can
thread the capability through to remedy this:
 
    map : [r,s]{a,b} -> Hnd s -> Cap s -> (Cap s -> a -> (b, Cap s)) -> List@r a -> (List@s b, Cap s)

This variant is more general.

# Variants on Capability Consumption

I still hate having to thread the capability through the program. In a language
that supports mutability everywhere, it makes more sense. But in a language
where almost everything is immutable, it's terrible. I've considered having
reads count as "zero consumption" of a capability, but that doesn't work.
But what if a capability had a weight associated with it and it had to give
part of that weight away to anything it allocated. And to release a region,
you had to account for 100% of the weight. This could also be used to prevent
leaking allocations in a region, but it doesn't have to do that. Let's see
what this might look like:

    // Note: Capability and handle are merged into a single Cap type
    makeFoo : (r : Region) -> Cap[n] r {5} -> Foo@r[5]

The number `n` indicates the capabilities total expected use count.
This says that the function consumes the capability 5 times (a number that
I made up). The number 5 written by `Foo` does not indicate consumption. It
describes what the weight of `Foo` is. We can combine these:

    makeFooTwice : (r : Region) -> Cap[n] r {10} -> (Foo@r[5], Foo@r[5])
    makeFooTwice(r,c):
      a = makeFoo(r,c)
      b = makeFoo(r,c)
      return (a,b)

This is a common extension of linear types. All we do is add up the consumption
counts. The annotation on `Foo`, to my knowledge, is more novel. We can only
free a region by proving that we can account for everything allocated into it:

    release : (r : Region) -> Cap[n] r {0} -> a@r[n] -> Unit

But this does not work. It doesn't prevent us from reading from the value of
type `a` after the region is released.

# Different Variant on Capability Consumption

What if we keep a monad-like thing to track the region? So our functions have
have types like this:

    indexArray : forall (r : Nat). (Array@r a, S64) {r}=>{r} a

Typically, we sequence operations with a monadic bind:

    bind :
         (a {r}=>{r} b)
      -> (b {r}=>{r} c)
      -> (a {r}=>{r} c)

And we allocate a new region with a special bind:

    newRegion : Unit {r}=>{r+1} (Cap r+1)

And we can destroy regions:

    releaseRegion : Cap (r+1) {r+1}=>{r} Unit

This nearly works, but it doesn't actually work. The problem is that we can
release a region and then allocate a new one, and then they will share a
region number. This means that we could reference data from the old region
even though the data is invalidated. Maybe the numbers should never be reused.
This requires a number nesting scheme:

    newRegion     : Unit  {s}=>{s+1} (Cap (s+1))
    releaseRegion : Cap s {s}=>{s} Unit

If we did a sequence of new-then-release operations over and over, our sequence
numbers would be: 1, 2, 3, 4. These are consider adjacent, and an object in
region 2 cannot be used in context 3. Subtyping is now based on prefixes, meaning
that an object in region 2 could be used in context 2.N or 2.N.M.

To get these nested contexts, we need an operation to "indent" one level:

    indent : Unit {r}=>{r.0} Unit
    dedent : Unit {r.z}=>{r} Unit

Those types don't work. We need to make sure we cannot create the same
region id twice:

    indent : Unit {r}=>{r.0} Unit
    dedent : Unit {r.z}=>{r+1} Unit

This is sound, but it's too restrictive.

Existential unpacking fixes the reuse problem, but I'd love to fix it some
other way. Using natural numbers as region numbers is nice because it makes
all the subtyping obvious.

I don't want to do anything too weird like rewriting the context every time
region release happens. What about this:

    newRegion     : Unit  {s[n]}=>{s.n[0]} (Cap (s.n))
    releaseRegion : Cap s {s.n[0]}=>{s[n+1]} Unit

The bracketed number beside `s` is the next child region that will be used.
So if we run new and release over and over starting with 5[2], we get this:

    5.2
    5.3
    5.4

What about running new three times in a row, then freeing them all:

    5.2[0]
    5.2.0[0]
    5.2.0.0[0]
    5.2.0[1]
    5.2[1]
    5[3]

This actually works. It has some strange properties though. For example,
functions need more variables to describe region arguments, and the leak
information about how many nonoverlapping temporary regions they allocate:

    foo : X {r[n]}=>{r[n+42]} Y

The caller shouldn't care about this information, but there is it anyway.
Because of this, recursion is weird. We generally do not know in advance
how many times we will call a recursive function. This will end up requiring
existential types, which is the very thing I wanted to avoid.

Maybe I should just embrace existentials instead. Then I could have:

    newRegion     : Unit      {r}  =>{r+e} (Hnd (r+e)) // e is existentially quantified
    releaseRegion : Hnd (r+e) {r+e}=>{r}   Unit

And this works fine. It's also easy to do stuff like this:

    map : [r,s]{a,b}. (Cap s, a {r+s}=>{r+s} b, List@r a) {r+s}=>{r+s} List@s b

Notice that the closure has to operate in the same context as the larger
function. This should not cause any problems. Let's try foldling:

    fold : [r]{a,b}. ((b,a) {r}=>{r} b, b, List@r a) {r}=>{r} b

This looks restrictive, but it's not. We might want the fold to allocate into
a region separate from the List. To do this, we just have to allocate the
new region (as a child of `r`) first. Then we can upcast the list into the
new region. Or we could go in the other order. Maybe the result of the fold
needs to outlive the list. That's fine too. The closure can close over the
allocation capability.

I think the type signature of `map` could be simplified for the same reason.
We can pretend that the source and destination regions are the same.

Let's look at the example from earlier:

    foo : {r : Region}. Foo@r {r}=>{r} s64
    foo{r}(value):
      // Notice that we do not need a handle to region r.
      {e}, hnd = newRegion{r}()
      ... // do some stuff in context r+e with a handle to e
      releaseRegion{e}(hnd)
      // Now we are back in context r
      return 1

In this system, a user could allocate another temporary region named `e`
after releasing the first one. Or they could even do this *before* releasing
the first one. Dealing with shaddowing will be annoying. Before typechecking,
I could do a context-resolution pass. This would come up with unique numeric
identifiers for every bound name and then replace each reference with the
identifier pointing to the correct one. This would eliminate shaddowing.
I could also do a debruijn-style solution, but this requires rewriting the
context a bunch.

Also, it should be possible to have a function create a fresh region. I do not
want to support this because I cannot see any use for it. But the possibility
is open.
