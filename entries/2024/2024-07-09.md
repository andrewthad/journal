# Experience with Parser Generator for LALR Grammar

I used the program Happy recently to build a parser from a grammar. It went
really well. The grammar was not LL(1). It wasn't even close to being LL(1),
so I don't think that I could have written this parser by hand. I had used
Happy years ago and was disappointed by how confusing it is to deal with
the precedence of infix operators. (This problem is not unique to Happy.)
But this time, I didn't have any infix operators with different precedences.
Now I can write stuff that looks like this:

    cubeAbsoluteValue : Func(s64)(s64) = (n : s64)
      cube = join(a : s64)
        b = multiply(a,a)
        c = multiply(a,b)
        return(c)
      if a < s64(0)
        z = negate(a)
        jump cube(z)
      else
        jump cube(a)

I always thought it would be way harder to parse a whitespace-sensitive
language, but that's not actually true. In the language that I defined,
all of the leading spaces have to be tabs. Before parsing, I perform
a pass that converts tabs to Indent and Unindent tokens. This interacts
with newlines in a sort of weird ways. You have to decide what happens
to the newline adjacent to an indentation shift. You can remove the newline
entirely, or you can preserve it. I actually do something worse than
either of these sensible option. I flank the indent/unindent character
with newlines on each side. This makes it easy to print out the result
of this pass. The cost is that the tokenizer becomes a little more
complicated. In the tokenizer, I remove all newlines that are adjacent
to indentation shifts. I also collapse sequences of multiple newlines
into a single newline token. (Although I refer to the token in code as
the "break" token). To give an example, the `cube` join point above
ends up looking sort of like this:

    join(a : s64)
    INDENT
    b = multiply(a,a)
    BREAK
    c = multiply(a,b)
    BREAK
    return(c)
    UNINDENT

All the parens and symbols all get turned into tokens, but I've omitted
that to illustrate what makes this unusual.

# Names for Top-Level Things

In the example I gave before, the name of the function was `cubeAbsoluteValue`.
This follows the tradition of having identifiers consist of letters, numbers,
and the underscore character. But this language is intended as a low-level
target. A high-level language that targets it would have, at the least,
namespaces. Consider:

    text = namespace
      utf8 = namespace
        validate : Func(array u8)(bool) = ...
        length : Func(array u8)(s64) = ...
    ...
    foo : Func(...)(...) = func(arg)
      isUtf8 = call text.utf8.validate(arg)
      ...

When we lower all of this, what do we want to end up with? Probably something
like this:

    text.utf8.validate : Func(array u8)(bool) = ...
    text.utf8.length : Func(array u8)(s64) = ...
    foo : Func(...)(...) = func(arg)
      isUtf8 = call text.utf8.validate(arg)
      ...

We simply flatten out the namespace, but we are able to preserve the names.
If we disallow the underscore character in identifier names, then we can
use it as the separator when lowering to C.

# Syntax for Refinements

It would be nice, for the common case of something not being refined, to not
have to write anything. But if something is refined, what then? Let's
consider the common case of non-negative integers:

    x : s64[0..] // nonnegative integers

Square brackets are nice. I'm not using these for arrays, so this might work.
It's nice to be able to omit the upper bound when the upper bound exactly
matches range that the type is able to represent. Otherwise:

    x : s64[0..100] // numbers between 0 and 100, inclusive

# Types and Refinements as Arguments to Primitives

Consider two functions: `addThrow` and `add`. The second one does not accept
arguments that might cause an overflow. The first one does accept them, and
it crash if the result would overflow. What should the type signatures look
like:

    addThrow :
         (loA, loB, hiA, hiB : Integer)
      -> (s : Signedness)
      -> (w : Width)
      -> Bounded s w loA hiA
      -> Bounded s w loB hiB
      -> Bounded s w
           (clipBelow (minInteger s w) (loA + loB))
           (clipAbove (maxInteger s w) (hiA + hiB))

This signature is pretty complicated, but the types are completely driven
by the arguments. What about the safe `add` that cannot crash:

    add :
         (loA, loB, hiA, hiB : Integer)
      -> (s : Signedness)
      -> (w : Width)
      -> Constraint (loA + loB >= minInteger s w)
      -> Constraint (hiA + hiB >= maxInteger s w)
      -> Bounded s w loA hiA
      -> Bounded s w loB hiB
      -> Bounded s w (loA + loB) (hiA + hiB)

This requires constraints. These constraints should always be solved by the
compiler since there are no type variables or predicate variables.

# Namespace vs Module and Nominally Defined Types

If I add namespaces as a level above modules, and namespaces cannot be created
by functors, then I can recover nominally defined data types. This works
somewhat, but it still has some problems. Consider trying to define a JSON
type in such a way that the map type used for objects can be changed out.
I need to think about this more.

# Some Built-in Type Constructors

One problem with my low-level language is monomorphism. The makes it difficult
to play with type constructors. For example, the type constructor list (for
cons lists) is actually a family of an infinite number of types. Normally,
with a nominally defined data type, if we want to project from it, we have
to look up the type in some kind of environment. Then, we learn what fields
it has and what the types of those fields are. Then, we can typecheck
projection. Same thing goes for scrutinizing with a case statement. But
with a type constructor, what we really  

