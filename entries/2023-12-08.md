# Programming Language Ease of Use

## Using Python for a Simple Task

I don't use python often. I used it recently to do a simple JSON-to-YAML
transformation where a single JSON document was split into many YAML documents.
This wasn't a very difficult task (maybe 30 or 40 lines of code), but there were
several things about the python specifically that made it easier than it would
be in GHC Haskell:

* Python has a large standard library. A python script can work with
  filesystems, JSON, YAML, and CSV without pulling in dependencies. Which
  means that there is no need for a build tool that performs dependency
  management. This saves a small amount of time, but it's enough time
  that it's noticeable when the programming task only takes 30 minutes.
  With GHC Haskell, it would be necessary to write a cabal file.
* Python has syntactic sugar for working with arrays and dictionaries. This
  made it easier to write the script.
* Python has a single string type. This is normal, and GHC Haskell is in
  the uncommon position of having `String` and `Text` compete with one another.
* Python has fstrings, a form of syntactic sugar for string interpolation.
* Python uses throwable exceptions to deal with error cases. Technically,
  GHC Haskell's `base` library operates this way, but other libaries
  (like `aeson`) use `Maybe` or `Either` to indicate the possibility
  of failure. If the programmer's plan is to just have the program die when
  anything goes wrong, then a standard library full of "hard fail" functions
  makes it easier to write the program.

Why have I been thinking about this? I don't like using dynamically typed languages
for anything larger than a few hundred lines of code. I lean heavily on static
type systems, and I have never been able to maintain large bodies of code
without them. But this task was striking to me because I could see how much
longer it would've taken in GHC Haskell. It might've jumped from being a
20-minute task to a 40-minute task or something like that.

But even the doing this task in Python was not frictionless. For one of the
tasks, I ended up repeatedly opening files in append mode, writing a YAML
document, and then closing the file. The same files were opened over and over,
so this was an inefficient solution. It happens to work correctly because each
file contained a YAML array at the top level, the concatenation of YAML arrays
is still well formed. This solution was unintuitive and brittle (in addition
to having poor performance). It would have been nice if there was a function
that could serialize a recursive map to a directory tree. Then the entire
result could have been build in memory and written to disk at the very end.

## Regions

Region types are very cool, but I think they result in a bad experience for
end users. Libraries the constrain operations based on the region in which
an object lives are more difficult to use. A sufficiently smart compiler
ought to be able to figure out most of this for the user. There is prior
work on this by Tofte and Talpin, but they show mixed results for a full
region-based automatic memory management system. I believe that there should
exist other solutions that use a traditional tracing collector and only
resort to regions when they are certain to improve performance. I am still
thinking through this, and I need to summarize my thinking soon.

## Builders

The concept of a "builder" shows in both function and non-functional programming
languages. The ideas is that the programmer wants to create an array (often
an array of textual data), but they do it piece by piece, gluing the pieces
together with some kind of cheap concatenation. The interface looks like this:

    type Builder
    type Element
    singleton : Element -> Builder
    append : Builder -> Builder -> Builder
    run : Builder -> Array Element

There are a lot of ways to implement this, but the implementation will be
ignored for now. What's particularly important about this is that it is
abstract. This makes it possible for the compiler to perform aggresive
optimizations. Consider if the interface included:

    append3 : Builder -> Builder -> Builder -> Builder

Then the compiler could perform this rewrite:

    append a (append b c) ==> append3 a b c

And it is important that the compiler be able to do this. This kind of rewrite
suggests that staged compilation, where certain stages apply module functors
to make types concrete, would be useful. I do not think that it is good to
expose this staging to end users though.

## Functions as Staging Optimization Boundaries

One common problem with staged compilation arises when there is no user-facing
two-level type system. The problem is that functions end up acting as
boundaries that seriously block optimization. For example, we might encode
an ip address and a port like this:

    encodeIp     : IPv4 -> Builder // dot decimal ASCII encoding
    encodeU16    : U16  -> Builder
    encodeIpPort : (IPv4, U16) -> Builder
    encodeIpPort(ip,port) = encodeIp(ip) <> encodeU16(port)

IP addresses and 16-bit words have upper bounds on the amount of bytes that
it takes to encode them. For IP addresses, the bound is 15, and for 16-bit
words, the bound is 5, so the total bound is 20. When `encodeIpPort` is
compiled, we want the implementation to allocate 20 bytes, encode the
components, and shrink/freeze the buffer down to the correct size (which is
discovered while encoding).

If we tried to use GHC Haskell's rewrite rules to accomplish this, then
we would need for both `encodeIp` and `encodeU16` to inline. This makes
the whole thing finicky and requires users (not just library authors!) to
add pragmas controlling inlining.

Another way of approaching this is to think of `encodeIp` as a family
of functions:

    // Chunks with O(1) append, delegates to encodeIpToBufferAt.
    encodeIpToCatenable : IPv4 -> CatenableBuilder
    // Encodings with bound on size known at compile time
    struct EncodeIpBounded {
      // Maximum number of bytes needed for encoding
      encodeIpSize        : Int
      // Takes buffer and position, returns same buffer and next position to write to.
      // Crashes if enough space is not available.
      encodeIpToBufferAt  : (IPv4, MutableArray U8, Int) -> (MutableArray U8, Int)
    }

This system does not require the user to think about inlining. In the above
example, the compiler would see that the body of `encodeIpPort` is the
composition of bounded builders, so `encodeIpPort` would be lowered
to a bounded builder that just calls the other bounded builders (and it
would get a wrapper for a catenable builder that just allocates the
right amount of space and then delegates to the bounded builder).

There are other ways to approach builders. GHC Haskell's `bytestring` and
`text` libraries use a mutable "head" buffer and a tail of immutable (frozen)
chunks. This means that it is only possible to efficiently add bytes to
one side of the builder. It also makes it possible to opportunistically
copy small byte arrays into the head buffer rather than using sharing.
The strategy that I propose differs from this strategy in two ways:

* It is possible to cheaply add bytes to either end of a builder
* There is no head buffer and no opportunistic copying. Sharing is maximized
  and the system only allocates a buffer when it knows an upper bound.

## Other Opportunities for Staging

GHC Haskell exposes rewrite rules directly to end users. I personally
(and this is certainly not everyone's opinion) see this as an experiment whose
outcome showed that it is difficult for end users to use rewrite rules. The
libraries `base`, `text`, `vector`, and `bytestring` all use rewrite rules
(and staging) to varying degrees. When `text` was rewritten to use UTF-8
instead of UTF-16 (in the year 2022 I think), the maintainers abandoned the
streaming framework that relied heavily on rewrite rules. It might be the
case that the problem is just stream fusion, not user-facing rewrite rules.
But I'm not sure. The whole thing is brittle, and the issue trackers of all
of these are little with "this term doesn't get rewritten when I expect it to".
I think it's hard enough to use that it might be better to just keep it as
a compiler internal.

Returning to the actual topic, I think that staging is useful for handling:

* Builders (immutable-style builders but also rewriting terms to directly
  fill a buffer when possible)
* Regex (both vanilla matching and capture groups)
* Fusing Traversals (not full stream fusion like `vector` attempts)
* Ordered Maps. Data constructors are never directly needed for these.
  Naive `lookup+insert` operations can be improved to an `upsert`.
  Sequences of operations for building small maps can be improved,
  especially when the keys are known at compile time. I'm not sure
  this is a good idea in general. If a user provides a bad ordering
  for the key, optimizations could lead to observable differences.
  Maybe just for a specialized map with integral keys, we could do this. 
* Various immutable data structures: persistent arrays, finger trees.

## Making Failure Easy 

In GHC Haskell, we have several ways to indicate failure:

    IO A
    IO (Either MyError A)
    Either MyError A
    IO (Maybe A)
    Maybe A

This effectfulness distinctions are useful, but some of this makes writing
small, "hard fail on any error" programs difficult. One simplification is
to have `Maybe = Either ()`. Now things are a little easier. A syntactic
shortcut I have been thinking of is a suffix bang. Ruby uses this for a
different purpose (in-place function variant). But I think it would be nice
to be able to use it for this conversion:

    bang : IO (Either MyError A) -> IO A

So that with POSIX functions like `read : Fd ->{IO} Either Errno [U8]`
and `write`, we could do this:

    echoStdinToStdout : () ->{IO} ()
    echoStdinToStdout = do
      bytes <- read!(0)
      write!(1,bytes)

But what about

    fromRight : Either MyError A -> A

Maybe the bang suffix could handle this as well:

    bang : (a ->{M} (Either MyError b)) -> a ->{M} b

Now we can set M to the IO effect or to the "uneffectful" effect, and
it works for both cases (and for any other algebraic effect).
