# Region-like Garbage Collection for Functions

Consider functions like this:

    data Foo:
      name: Text
      age: Int64
    decodeFoo(bytes b) returning Foo:
      obj = decodeJson(b)
      fooFromObject(obj)

Some components of the intermediate json value are guaranteed to not
escape the function. Which ones?

    data Val
      = True -- not allocated
      | False -- not allocated
      | Null -- not allocated
      | Number Integer -- no escape
      | String Text -- the text inside can escape
      | Array [Val] -- no escape
      | Object [Member] -- no escape
    data Member:
      key: Text
      value: Val

Is there a way to take advantage of this? It depends on how the allocator
works. If each data type is allocated into a separate area, then there
is a simple trick. At the beginning of `decodeFoo`, mark the start
of the areas for `String`, `Array`, and `Object`. At the end of `decodeFoo`,
check to see if GC happened. If it did, abandon the optimization. If it
didn't, roll back the position into those three regions.

The glaring flaw is we have to give up when a GC happens in the middle.
There are two reasons for this:

1. Other pre-existing values of these types might need to stay live.
   It might be possible to work around this by coming up with a GC
   scheme that supports disabling the scanning of certain types. I'm
   don't think this can be done though. Although in this particular
   case, it should be acceptable to defer GC entirely. Why? Because
   `decodeJson` doesn't produce "waste" allocations and `fooFromObject`
   performs a bounded number of allocations.
2. We might need to reclaim memory allocated in `decodeFoo`. Although,
   as discussed above, because of the allocation patterns of the functions
   in `decodeFoo`, we actually do not have to reclaim memory.

This has made two function properties more clear in my mind. They are:

1. Wastefulness: Does a function produce waste? If a function allocates
   something that is not referenced by the object that it returns, then
   it produces waste. In a GC scheme that keeps different types of objects
   in different parts of memory, one neat trick about a non-wasteful
   function is that it is always beneficial (I think) to wait to do GC
   until the function returns. This is because everything allocated
   by the function so far would be deemed "live". The only benefit that
   to running GC in the middle of the function would be that memory used
   by earlier operations could be recovered, but this should not be a
   big deal except in unusual situations. For example, before the
   function call, if over half of the available heap was released, and
   then the function itself wanted to use over half of the heap, then
   this optimization would cause memory to become exhausted when
   the program could have otherwise reclaimed memory and continued
   running. Again, I think this is a rare situation. We have the this ordering:
   non-allocating < non-wasteful < wasteful
2. Boundedness: Does a function allocate a finite amount of memory?
   If a function doesn't have any loops, and if it doesn't allocate
   any arrays, then we can put an upper bound on its allocations.
   We have this ordering:
   non-allocating < bounded

In the original example, `decodeJson` is non-wasteful (at least,
a non-wasteful implementation is possible) but has unbounded allocations.
And `fooFromObject` is non-wasteful and has bounded allocations.
However, when we stick these together, we end up with a `decodeFoo`
that is both wasteful and has unbounded allocations. But there
is a "crossover point", that is, a place where the function switches
from non-wasteful to bounded. The second part is
free to waste the allocations from the first part since we know
that we the waste will not last long. I'm using a strange definition
of "last long", where I'm measure not in time but in the number
of allocations.

At the end of `decodeJson`, we can roll back all of the `String`,
`Array`, `Object`, and `Number` allocations, but we cannot roll back
any of the `Text` allocations. Even the json keys, which cannot possibly
appear in the output object, cannot be rolled back.

Alternatively (with a GC that didn't segregate objects by type),
we could roll back everything and deep copy the output `Foo`.
I don't think the deep copy is a good idea since a large
piece of text could turn it into a pessimization.

I dislike that this optimization requires identifying a "crossover point".
It makes it less likely that the optimization would survive if either
part were inlined. Maybe there's a better way to formulate this idea,
but this at least lays some groundwork.

Discussed earlier was an different optimization where, rather than
preventing garbage collection, we just let it happen and possibly
abandon the rollback. This optimization is easier because it does
not have the same non-wastefulness constraint. All you have to do
is figure out what types can show up in the output, and you get to
roll back everything except those types. However, the rotten part
of the deal is that you might go to all the effort to store
current positions, and the function might always do a large amount
of allocation. So, it could just be wasted effort, a pessimization.

I think, though, that both are useful. Rolling back is very
powerful if you can get away with it. For example, when handling
an HTTP request, it might be possible to just roll back in many
cases (as long as handling the request didn't cause you to
allocate too much).
